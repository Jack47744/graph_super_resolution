{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from MatrixVectorizer import *\n",
    "import torch\n",
    "import random\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA not available. Using CPU.\n"
     ]
    }
   ],
   "source": [
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "# Check for CUDA (GPU support) and set device accordingly\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)  # For multi-GPU setups\n",
    "    # Additional settings for ensuring reproducibility on CUDA\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_LR_train = pd.read_csv(\"../data/lr_train.csv\")\n",
    "A_HR_train = pd.read_csv(\"../data/hr_train.csv\")\n",
    "A_LR_test = pd.read_csv(\"../data/lr_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_size = 160\n",
    "HR_size = 268"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(167, 161)\n",
      "[[ 0 13]\n",
      " [ 1 41]\n",
      " [ 2 14]\n",
      " [ 3 48]\n",
      " [ 4 51]]\n",
      "Train size: 133\n",
      "Val size: 34\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "A_HR_train = pd.read_csv(\"../data/hr_train.csv\")\n",
    "\n",
    "pca = PCA(n_components=0.99, whiten=False)\n",
    "A_HR_train_pca = pca.fit_transform(A_HR_train)\n",
    "print(A_HR_train_pca.shape)\n",
    "\n",
    "gm = GaussianMixture(n_components=5, random_state=random_seed)\n",
    "A_HR_train_label = gm.fit_predict(A_HR_train_pca)\n",
    "unique, counts = np.unique(A_HR_train_label, return_counts=True)\n",
    "print(np.asarray((unique, counts)).T)\n",
    "\n",
    "X = np.load('A_LR_train_matrix.npy')\n",
    "y = np.load('A_HR_train_matrix.npy')\n",
    "\n",
    "n_sample = X.shape[0]\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X.reshape(n_sample, -1), \n",
    "    y.reshape(n_sample, -1), \n",
    "    test_size=0.20, \n",
    "    random_state=random_seed,\n",
    "    stratify=A_HR_train_label\n",
    ")\n",
    "\n",
    "X_train = X_train.reshape(-1, LR_size, LR_size)\n",
    "X_val = X_val.reshape(-1, LR_size, LR_size)\n",
    "y_train = y_train.reshape(-1, HR_size, HR_size)\n",
    "y_val = y_val.reshape(-1, HR_size, HR_size)\n",
    "\n",
    "print(\"Train size:\", len(X_train))\n",
    "print(\"Val size:\", len(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wutikorn/Desktop/Imperial/Deep Graph-based Learning/Project/graph_super_resolution/gsr_net_gat/MatrixVectorizer.py:88: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  matrix[row, col] = vector[vector_idx]\n",
      "/Users/wutikorn/Desktop/Imperial/Deep Graph-based Learning/Project/graph_super_resolution/gsr_net_gat/MatrixVectorizer.py:89: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  matrix[col, row] = vector[vector_idx]\n"
     ]
    }
   ],
   "source": [
    "def antivectorize_df(adj_mtx_df, size):\n",
    "    \n",
    "    num_subject = adj_mtx_df.shape[0]\n",
    "    adj_mtx = np.zeros((num_subject, size, size)) #torch.zeros((num_subject, LR_size, LR_size))\n",
    "    for i in range(num_subject):\n",
    "        adj_mtx[i] = MatrixVectorizer.anti_vectorize(adj_mtx_df.iloc[i], size) # torch.from_numpy(MatrixVectorizer.anti_vectorize(A_LR_train.iloc[i], LR_size))\n",
    "    return adj_mtx\n",
    "\n",
    "np.save('A_LR_train_matrix.npy', antivectorize_df(A_LR_train, LR_size))\n",
    "np.save('A_HR_train_matrix.npy', antivectorize_df(A_HR_train, HR_size))\n",
    "np.save('A_LR_test_matrix.npy', antivectorize_df(A_LR_test, LR_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(167, 160, 160)\n",
      "(167, 268, 268)\n",
      "(112, 160, 160)\n"
     ]
    }
   ],
   "source": [
    "A_LR_train_matrix = np.load('A_LR_train_matrix.npy')\n",
    "A_HR_train_matrix = np.load('A_HR_train_matrix.npy')\n",
    "A_LR_test_matrix = np.load(\"A_LR_test_matrix.npy\")\n",
    "\n",
    "print(A_LR_train_matrix.shape)\n",
    "print(A_HR_train_matrix.shape)\n",
    "print(A_LR_test_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(epochs=200, lr=0.0001, splits=3, lmbda=16, lr_dim=160, hr_dim=268, hidden_dim=280, padding=26, embedding_size=32, early_stop_patient=3)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Main function of Graph Super-Resolution Network (GSR-Net) framework \n",
    "   for predicting high-resolution brain connectomes from low-resolution connectomes. \n",
    "    \n",
    "    ---------------------------------------------------------------------\n",
    "    \n",
    "    This file contains the implementation of the training and testing process of our GSR-Net model.\n",
    "        train(model, optimizer, subjects_adj, subjects_ground_truth, args)\n",
    "\n",
    "                Inputs:\n",
    "                        model:        constructor of our GSR-Net model:  model = GSRNet(ks,args)\n",
    "                                      ks:   array that stores reduction rates of nodes in Graph U-Net pooling layers\n",
    "                                      args: parsed command line arguments\n",
    "\n",
    "                        optimizer:    constructor of our model's optimizer (borrowed from PyTorch)  \n",
    "\n",
    "                        subjects_adj: (n × l x l) tensor stacking LR connectivity matrices of all training subjects\n",
    "                                       n: the total number of subjects\n",
    "                                       l: the dimensions of the LR connectivity matrices\n",
    "\n",
    "                        subjects_ground_truth: (n × h x h) tensor stacking LR connectivity matrices of all training subjects\n",
    "                                                n: the total number of subjects\n",
    "                                                h: the dimensions of the LR connectivity matrices\n",
    "\n",
    "                        args:          parsed command line arguments, to learn more about the arguments run: \n",
    "                                       python demo.py --help\n",
    "                Output:\n",
    "                        for each epoch, prints out the mean training MSE error\n",
    "\n",
    "\n",
    "            \n",
    "        test(model, test_adj,test_ground_truth,args)\n",
    "\n",
    "                Inputs:\n",
    "                        test_adj:      (n × l x l) tensor stacking LR connectivity matrices of all testing subjects\n",
    "                                        n: the total number of subjects\n",
    "                                        l: the dimensions of the LR connectivity matrices\n",
    "\n",
    "                        test_ground_truth:      (n × h x h) tensor stacking LR connectivity matrices of all testing subjects\n",
    "                                                 n: the total number of subjects\n",
    "                                                 h: the dimensions of the LR connectivity matrices\n",
    "\n",
    "                        see train method above for model and args.\n",
    "\n",
    "                Outputs:\n",
    "                        for each epoch, prints out the mean testing MSE error\n",
    "\n",
    "\n",
    "    To evaluate our framework we used 5-fold cross-validation strategy.\n",
    "\n",
    "    ---------------------------------------------------------------------\n",
    "    Copyright 2020 Megi Isallari, Istanbul Technical University.\n",
    "    All rights reserved.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import KFold\n",
    "from preprocessing import *\n",
    "from model import *\n",
    "from train import *\n",
    "import argparse\n",
    "\n",
    "\n",
    "\n",
    "epochs = 200\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='GSR-Net')\n",
    "parser.add_argument('--epochs', type=int, default=epochs, metavar='no_epochs',\n",
    "                help='number of episode to train ')\n",
    "parser.add_argument('--lr', type=float, default=0.0001, metavar='lr',\n",
    "                help='learning rate (default: 0.0001 using Adam Optimizer)')\n",
    "parser.add_argument('--splits', type=int, default=3, metavar='n_splits',\n",
    "                help='no of cross validation folds')\n",
    "parser.add_argument('--lmbda', type=int, default=16, metavar='L',\n",
    "                help='self-reconstruction error hyperparameter')\n",
    "parser.add_argument('--lr_dim', type=int, default=LR_size, metavar='N',\n",
    "                help='adjacency matrix input dimensions')\n",
    "parser.add_argument('--hr_dim', type=int, default=HR_size, metavar='N',\n",
    "                help='super-resolved adjacency matrix output dimensions')\n",
    "parser.add_argument('--hidden_dim', type=int, default=280, metavar='N',\n",
    "                help='hidden GraphConvolutional layer dimensions')\n",
    "parser.add_argument('--padding', type=int, default=26, metavar='padding',\n",
    "                help='dimensions of padding')\n",
    "parser.add_argument('--embedding_size', type=int, default=32, metavar='embedding_size',\n",
    "                help='node embedding size')\n",
    "parser.add_argument('--early_stop_patient', type=int, default=3, metavar='early_stop_patient',\n",
    "                help='early_stop_patience')\n",
    "\n",
    "\n",
    "\n",
    "# Create an empty Namespace to hold the default arguments\n",
    "args = parser.parse_args([]) \n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(167, 160, 160)\n",
      "(167, 268, 268)\n"
     ]
    }
   ],
   "source": [
    "# SIMULATING THE DATA: EDIT TO ENTER YOUR OWN DATA\n",
    "X = A_LR_train_matrix #np.random.normal(0, 0.5, (167, 160, 160))\n",
    "Y = A_HR_train_matrix #np.random.normal(0, 0.5, (167, 288, 288))\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = get_device()\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(167, 160, 160)\n"
     ]
    }
   ],
   "source": [
    "def compute_degree_matrix_normalization_batch_numpy(adjacency_batch):\n",
    "    \"\"\"\n",
    "    Optimizes the degree matrix normalization for a batch of adjacency matrices using NumPy.\n",
    "    Computes the normalized adjacency matrix D^-1 * A for each graph in the batch.\n",
    "    \n",
    "    Parameters:\n",
    "    - adjacency_batch: A NumPy array of shape (batch_size, num_nodes, num_nodes) representing\n",
    "                       a batch of adjacency matrices.\n",
    "\n",
    "    Returns:\n",
    "    - A NumPy array of normalized adjacency matrices.\n",
    "    \"\"\"\n",
    "    epsilon = 1e-6  # Small constant to avoid division by zero\n",
    "    # Calculate the degree for each node in the batch\n",
    "    d = adjacency_batch.sum(axis=2) + epsilon\n",
    "    \n",
    "    # Compute the inverse degree matrix D^-1 for the batch\n",
    "    D_inv = np.reciprocal(d)[:, :, np.newaxis] * np.eye(adjacency_batch.shape[1])[np.newaxis, :, :]\n",
    "    \n",
    "    # Normalize the adjacency matrix using batch matrix multiplication\n",
    "    normalized_adjacency_batch = np.matmul(D_inv, adjacency_batch)\n",
    "    \n",
    "    return normalized_adjacency_batch\n",
    "X = compute_degree_matrix_normalization_batch_numpy(X)\n",
    "A_LR_test_matrix = compute_degree_matrix_normalization_batch_numpy(A_LR_test_matrix)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Fold 1 -----\n",
      "Epoch: 1, Train Loss: 0.303294, Train Error: 0.249331, Test Error: 0.210879\n",
      "Epoch: 2, Train Loss: 0.238523, Train Error: 0.189838, Test Error: 0.168311\n",
      "Epoch: 3, Train Loss: 0.222542, Train Error: 0.177075, Test Error: 0.166930\n",
      "Epoch: 4, Train Loss: 0.217828, Train Error: 0.176241, Test Error: 0.166609\n",
      "Epoch: 5, Train Loss: 0.214499, Train Error: 0.175635, Test Error: 0.165968\n",
      "Epoch: 6, Train Loss: 0.212144, Train Error: 0.174963, Test Error: 0.165703\n",
      "Epoch: 7, Train Loss: 0.209974, Train Error: 0.174521, Test Error: 0.165028\n",
      "Epoch: 8, Train Loss: 0.208880, Train Error: 0.174110, Test Error: 0.164358\n",
      "Epoch: 9, Train Loss: 0.206291, Train Error: 0.173275, Test Error: 0.163597\n",
      "Epoch: 10, Train Loss: 0.205541, Train Error: 0.172695, Test Error: 0.162765\n",
      "Epoch: 11, Train Loss: 0.203720, Train Error: 0.171675, Test Error: 0.161782\n",
      "Epoch: 12, Train Loss: 0.202105, Train Error: 0.170777, Test Error: 0.160633\n",
      "Epoch: 13, Train Loss: 0.200255, Train Error: 0.169452, Test Error: 0.159467\n",
      "Epoch: 14, Train Loss: 0.198238, Train Error: 0.168056, Test Error: 0.157991\n",
      "Epoch: 15, Train Loss: 0.195972, Train Error: 0.166453, Test Error: 0.156348\n",
      "Epoch: 16, Train Loss: 0.194597, Train Error: 0.165277, Test Error: 0.155052\n",
      "Epoch: 17, Train Loss: 0.193530, Train Error: 0.164014, Test Error: 0.153710\n",
      "Epoch: 18, Train Loss: 0.190883, Train Error: 0.162287, Test Error: 0.152291\n",
      "Epoch: 19, Train Loss: 0.190237, Train Error: 0.161292, Test Error: 0.151306\n",
      "Epoch: 20, Train Loss: 0.188207, Train Error: 0.159855, Test Error: 0.149899\n",
      "Epoch: 21, Train Loss: 0.186358, Train Error: 0.158630, Test Error: 0.148927\n",
      "Epoch: 22, Train Loss: 0.185602, Train Error: 0.157416, Test Error: 0.148071\n",
      "Epoch: 23, Train Loss: 0.183582, Train Error: 0.156403, Test Error: 0.147241\n",
      "Epoch: 24, Train Loss: 0.181738, Train Error: 0.155057, Test Error: 0.146407\n",
      "Epoch: 25, Train Loss: 0.181747, Train Error: 0.154474, Test Error: 0.145441\n",
      "Epoch: 26, Train Loss: 0.181007, Train Error: 0.153303, Test Error: 0.144540\n",
      "Epoch: 27, Train Loss: 0.180084, Train Error: 0.152655, Test Error: 0.143707\n",
      "Epoch: 28, Train Loss: 0.178663, Train Error: 0.151861, Test Error: 0.143020\n",
      "Epoch: 29, Train Loss: 0.177906, Train Error: 0.150922, Test Error: 0.142310\n",
      "Epoch: 30, Train Loss: 0.177724, Train Error: 0.150397, Test Error: 0.141667\n",
      "Epoch: 31, Train Loss: 0.176751, Train Error: 0.149811, Test Error: 0.140810\n",
      "Epoch: 32, Train Loss: 0.175158, Train Error: 0.148932, Test Error: 0.140342\n",
      "Epoch: 33, Train Loss: 0.174925, Train Error: 0.148332, Test Error: 0.139778\n",
      "Epoch: 34, Train Loss: 0.174083, Train Error: 0.147510, Test Error: 0.139337\n",
      "Epoch: 35, Train Loss: 0.173621, Train Error: 0.147263, Test Error: 0.139030\n",
      "Epoch: 36, Train Loss: 0.172835, Train Error: 0.146595, Test Error: 0.138667\n",
      "Epoch: 37, Train Loss: 0.172555, Train Error: 0.146267, Test Error: 0.138359\n",
      "Epoch: 38, Train Loss: 0.171895, Train Error: 0.145626, Test Error: 0.137747\n",
      "Epoch: 39, Train Loss: 0.171931, Train Error: 0.145549, Test Error: 0.137463\n",
      "Epoch: 40, Train Loss: 0.171145, Train Error: 0.145068, Test Error: 0.137253\n",
      "Epoch: 41, Train Loss: 0.170918, Train Error: 0.144838, Test Error: 0.137057\n",
      "Epoch: 42, Train Loss: 0.170808, Train Error: 0.144491, Test Error: 0.136809\n",
      "Epoch: 43, Train Loss: 0.170049, Train Error: 0.143880, Test Error: 0.136706\n",
      "Epoch: 44, Train Loss: 0.169497, Train Error: 0.143700, Test Error: 0.136333\n",
      "Epoch: 45, Train Loss: 0.169419, Train Error: 0.143700, Test Error: 0.135982\n",
      "Epoch: 46, Train Loss: 0.169031, Train Error: 0.143064, Test Error: 0.135908\n",
      "Epoch: 47, Train Loss: 0.169088, Train Error: 0.143019, Test Error: 0.135651\n",
      "Epoch: 48, Train Loss: 0.168585, Train Error: 0.142739, Test Error: 0.135543\n",
      "Epoch: 49, Train Loss: 0.168346, Train Error: 0.142325, Test Error: 0.135294\n",
      "Epoch: 50, Train Loss: 0.167986, Train Error: 0.142156, Test Error: 0.135194\n",
      "Epoch: 51, Train Loss: 0.167790, Train Error: 0.141865, Test Error: 0.135249\n",
      "Epoch: 52, Train Loss: 0.168139, Train Error: 0.141922, Test Error: 0.135144\n",
      "Epoch: 53, Train Loss: 0.167605, Train Error: 0.141548, Test Error: 0.134700\n",
      "Epoch: 54, Train Loss: 0.167316, Train Error: 0.141456, Test Error: 0.135140\n",
      "Epoch: 55, Train Loss: 0.166632, Train Error: 0.141121, Test Error: 0.134449\n",
      "Epoch: 56, Train Loss: 0.166810, Train Error: 0.140884, Test Error: 0.134624\n",
      "Epoch: 57, Train Loss: 0.166716, Train Error: 0.140967, Test Error: 0.134339\n",
      "Epoch: 58, Train Loss: 0.167020, Train Error: 0.140985, Test Error: 0.134231\n",
      "Epoch: 59, Train Loss: 0.166280, Train Error: 0.140699, Test Error: 0.134392\n",
      "Epoch: 60, Train Loss: 0.166075, Train Error: 0.140173, Test Error: 0.134286\n",
      "Epoch: 61, Train Loss: 0.166122, Train Error: 0.140251, Test Error: 0.133872\n",
      "Epoch: 62, Train Loss: 0.165926, Train Error: 0.140102, Test Error: 0.134122\n",
      "Epoch: 63, Train Loss: 0.166349, Train Error: 0.140215, Test Error: 0.134285\n",
      "Epoch: 64, Train Loss: 0.166155, Train Error: 0.140116, Test Error: 0.133840\n",
      "Epoch: 65, Train Loss: 0.165538, Train Error: 0.139488, Test Error: 0.133875\n",
      "Epoch: 66, Train Loss: 0.164617, Train Error: 0.139216, Test Error: 0.133597\n",
      "Epoch: 67, Train Loss: 0.164820, Train Error: 0.139188, Test Error: 0.133693\n",
      "Epoch: 68, Train Loss: 0.165288, Train Error: 0.139518, Test Error: 0.133468\n",
      "Epoch: 69, Train Loss: 0.165275, Train Error: 0.139124, Test Error: 0.134116\n",
      "Epoch: 70, Train Loss: 0.164696, Train Error: 0.138928, Test Error: 0.133636\n",
      "Epoch: 71, Train Loss: 0.164691, Train Error: 0.138938, Test Error: 0.133375\n",
      "Epoch: 72, Train Loss: 0.164827, Train Error: 0.139037, Test Error: 0.133517\n",
      "Epoch: 73, Train Loss: 0.164470, Train Error: 0.138973, Test Error: 0.133056\n",
      "Epoch: 74, Train Loss: 0.164249, Train Error: 0.138525, Test Error: 0.132741\n",
      "Epoch: 75, Train Loss: 0.164081, Train Error: 0.138483, Test Error: 0.132744\n",
      "Epoch: 76, Train Loss: 0.163531, Train Error: 0.137781, Test Error: 0.133106\n",
      "Epoch: 77, Train Loss: 0.163888, Train Error: 0.138366, Test Error: 0.132889\n",
      "Epoch: 78, Train Loss: 0.163789, Train Error: 0.138137, Test Error: 0.132733\n",
      "Epoch: 79, Train Loss: 0.163699, Train Error: 0.137940, Test Error: 0.132936\n",
      "Epoch: 80, Train Loss: 0.163982, Train Error: 0.138061, Test Error: 0.132893\n",
      "Epoch: 81, Train Loss: 0.163503, Train Error: 0.137860, Test Error: 0.132631\n",
      "Epoch: 82, Train Loss: 0.162649, Train Error: 0.137365, Test Error: 0.133105\n",
      "Epoch: 83, Train Loss: 0.163564, Train Error: 0.137815, Test Error: 0.132509\n",
      "Epoch: 84, Train Loss: 0.162734, Train Error: 0.137327, Test Error: 0.132870\n",
      "Epoch: 85, Train Loss: 0.162432, Train Error: 0.137030, Test Error: 0.132364\n",
      "Epoch: 86, Train Loss: 0.162562, Train Error: 0.137068, Test Error: 0.132256\n",
      "Epoch: 87, Train Loss: 0.161941, Train Error: 0.136550, Test Error: 0.132425\n",
      "Epoch: 88, Train Loss: 0.162826, Train Error: 0.137106, Test Error: 0.132843\n",
      "Epoch: 89, Train Loss: 0.162961, Train Error: 0.136848, Test Error: 0.132263\n",
      "Epoch: 90, Train Loss: 0.161842, Train Error: 0.136454, Test Error: 0.131857\n",
      "Epoch: 91, Train Loss: 0.162105, Train Error: 0.136727, Test Error: 0.131826\n",
      "Epoch: 92, Train Loss: 0.162300, Train Error: 0.136550, Test Error: 0.132307\n",
      "Epoch: 93, Train Loss: 0.161828, Train Error: 0.136243, Test Error: 0.131681\n",
      "Epoch: 94, Train Loss: 0.162146, Train Error: 0.136329, Test Error: 0.131765\n",
      "Epoch: 95, Train Loss: 0.162020, Train Error: 0.136263, Test Error: 0.131617\n",
      "Epoch: 96, Train Loss: 0.161796, Train Error: 0.136032, Test Error: 0.131621\n",
      "Epoch: 97, Train Loss: 0.161624, Train Error: 0.136027, Test Error: 0.131713\n",
      "Epoch: 98, Train Loss: 0.161724, Train Error: 0.135708, Test Error: 0.131509\n",
      "Epoch: 99, Train Loss: 0.161530, Train Error: 0.135931, Test Error: 0.131490\n",
      "Epoch: 100, Train Loss: 0.161136, Train Error: 0.135653, Test Error: 0.131644\n",
      "Epoch: 101, Train Loss: 0.161173, Train Error: 0.135627, Test Error: 0.131725\n",
      "Epoch: 102, Train Loss: 0.161274, Train Error: 0.135538, Test Error: 0.131607\n",
      "Val Error: 0.131490\n",
      "Val MAE: 0.13149016935910499\n",
      "----- Fold 2 -----\n",
      "Epoch: 1, Train Loss: 0.300122, Train Error: 0.245501, Test Error: 0.214099\n",
      "Epoch: 2, Train Loss: 0.234173, Train Error: 0.185116, Test Error: 0.176869\n",
      "Epoch: 3, Train Loss: 0.217488, Train Error: 0.173186, Test Error: 0.175157\n",
      "Epoch: 4, Train Loss: 0.213934, Train Error: 0.172143, Test Error: 0.174658\n",
      "Epoch: 5, Train Loss: 0.211421, Train Error: 0.171509, Test Error: 0.174255\n",
      "Epoch: 6, Train Loss: 0.208674, Train Error: 0.171125, Test Error: 0.173817\n",
      "Epoch: 7, Train Loss: 0.205999, Train Error: 0.170622, Test Error: 0.173335\n",
      "Epoch: 8, Train Loss: 0.204069, Train Error: 0.170136, Test Error: 0.172792\n",
      "Epoch: 9, Train Loss: 0.202508, Train Error: 0.169548, Test Error: 0.172071\n",
      "Epoch: 10, Train Loss: 0.201512, Train Error: 0.169025, Test Error: 0.171553\n",
      "Epoch: 11, Train Loss: 0.199405, Train Error: 0.168108, Test Error: 0.170681\n",
      "Epoch: 12, Train Loss: 0.197876, Train Error: 0.167084, Test Error: 0.169802\n",
      "Epoch: 13, Train Loss: 0.196168, Train Error: 0.165977, Test Error: 0.168591\n",
      "Epoch: 14, Train Loss: 0.194397, Train Error: 0.164618, Test Error: 0.167085\n",
      "Epoch: 15, Train Loss: 0.192575, Train Error: 0.163192, Test Error: 0.165694\n",
      "Epoch: 16, Train Loss: 0.190884, Train Error: 0.161600, Test Error: 0.163832\n",
      "Epoch: 17, Train Loss: 0.188764, Train Error: 0.159871, Test Error: 0.162939\n",
      "Epoch: 18, Train Loss: 0.186373, Train Error: 0.157985, Test Error: 0.161045\n",
      "Epoch: 19, Train Loss: 0.184817, Train Error: 0.156498, Test Error: 0.159862\n",
      "Epoch: 20, Train Loss: 0.184046, Train Error: 0.154996, Test Error: 0.158742\n",
      "Epoch: 21, Train Loss: 0.181501, Train Error: 0.153450, Test Error: 0.157650\n",
      "Epoch: 22, Train Loss: 0.180431, Train Error: 0.152568, Test Error: 0.156720\n",
      "Epoch: 23, Train Loss: 0.178920, Train Error: 0.151314, Test Error: 0.155727\n",
      "Epoch: 24, Train Loss: 0.178120, Train Error: 0.150151, Test Error: 0.154924\n",
      "Epoch: 25, Train Loss: 0.176040, Train Error: 0.148685, Test Error: 0.154031\n",
      "Epoch: 26, Train Loss: 0.175398, Train Error: 0.148167, Test Error: 0.153394\n",
      "Epoch: 27, Train Loss: 0.174077, Train Error: 0.147169, Test Error: 0.152533\n",
      "Epoch: 28, Train Loss: 0.173129, Train Error: 0.145994, Test Error: 0.151937\n",
      "Epoch: 29, Train Loss: 0.171935, Train Error: 0.145260, Test Error: 0.151466\n",
      "Epoch: 30, Train Loss: 0.172112, Train Error: 0.145160, Test Error: 0.150818\n",
      "Epoch: 31, Train Loss: 0.170837, Train Error: 0.144097, Test Error: 0.150296\n",
      "Epoch: 32, Train Loss: 0.170156, Train Error: 0.143492, Test Error: 0.149758\n",
      "Epoch: 33, Train Loss: 0.169539, Train Error: 0.142820, Test Error: 0.149179\n",
      "Epoch: 34, Train Loss: 0.168713, Train Error: 0.142161, Test Error: 0.148901\n",
      "Epoch: 35, Train Loss: 0.167970, Train Error: 0.141726, Test Error: 0.148351\n",
      "Epoch: 36, Train Loss: 0.167415, Train Error: 0.141245, Test Error: 0.148078\n",
      "Epoch: 37, Train Loss: 0.166623, Train Error: 0.140789, Test Error: 0.148050\n",
      "Epoch: 38, Train Loss: 0.166397, Train Error: 0.140396, Test Error: 0.147732\n",
      "Epoch: 39, Train Loss: 0.166412, Train Error: 0.140262, Test Error: 0.147185\n",
      "Epoch: 40, Train Loss: 0.165655, Train Error: 0.139535, Test Error: 0.146846\n",
      "Epoch: 41, Train Loss: 0.165267, Train Error: 0.139088, Test Error: 0.146641\n",
      "Epoch: 42, Train Loss: 0.164797, Train Error: 0.138786, Test Error: 0.146658\n",
      "Epoch: 43, Train Loss: 0.164930, Train Error: 0.138697, Test Error: 0.146111\n",
      "Epoch: 44, Train Loss: 0.164494, Train Error: 0.138298, Test Error: 0.145861\n",
      "Epoch: 45, Train Loss: 0.164099, Train Error: 0.138002, Test Error: 0.145944\n",
      "Epoch: 46, Train Loss: 0.163370, Train Error: 0.137459, Test Error: 0.145509\n",
      "Epoch: 47, Train Loss: 0.163417, Train Error: 0.137227, Test Error: 0.145493\n",
      "Epoch: 48, Train Loss: 0.163356, Train Error: 0.137547, Test Error: 0.145344\n",
      "Epoch: 49, Train Loss: 0.163391, Train Error: 0.137273, Test Error: 0.145288\n",
      "Epoch: 50, Train Loss: 0.163064, Train Error: 0.137020, Test Error: 0.145441\n",
      "Epoch: 51, Train Loss: 0.162488, Train Error: 0.136973, Test Error: 0.145193\n",
      "Epoch: 52, Train Loss: 0.162157, Train Error: 0.136096, Test Error: 0.144974\n",
      "Epoch: 53, Train Loss: 0.161829, Train Error: 0.136033, Test Error: 0.145028\n",
      "Epoch: 54, Train Loss: 0.161275, Train Error: 0.135661, Test Error: 0.144694\n",
      "Epoch: 55, Train Loss: 0.162001, Train Error: 0.135855, Test Error: 0.144592\n",
      "Epoch: 56, Train Loss: 0.161555, Train Error: 0.135732, Test Error: 0.144271\n",
      "Epoch: 57, Train Loss: 0.160743, Train Error: 0.135092, Test Error: 0.144118\n",
      "Epoch: 58, Train Loss: 0.160988, Train Error: 0.135081, Test Error: 0.143983\n",
      "Epoch: 59, Train Loss: 0.161269, Train Error: 0.135440, Test Error: 0.144458\n",
      "Epoch: 60, Train Loss: 0.160743, Train Error: 0.134710, Test Error: 0.144210\n",
      "Epoch: 61, Train Loss: 0.160664, Train Error: 0.134903, Test Error: 0.144081\n",
      "Epoch: 62, Train Loss: 0.159944, Train Error: 0.134207, Test Error: 0.143902\n",
      "Epoch: 63, Train Loss: 0.160670, Train Error: 0.134655, Test Error: 0.143962\n",
      "Epoch: 64, Train Loss: 0.160015, Train Error: 0.134590, Test Error: 0.144013\n",
      "Epoch: 65, Train Loss: 0.159211, Train Error: 0.133663, Test Error: 0.143452\n",
      "Epoch: 66, Train Loss: 0.159716, Train Error: 0.133698, Test Error: 0.143687\n",
      "Epoch: 67, Train Loss: 0.159354, Train Error: 0.133492, Test Error: 0.143873\n",
      "Epoch: 68, Train Loss: 0.159276, Train Error: 0.133438, Test Error: 0.143348\n",
      "Epoch: 69, Train Loss: 0.159258, Train Error: 0.133650, Test Error: 0.142989\n",
      "Epoch: 70, Train Loss: 0.158965, Train Error: 0.133128, Test Error: 0.143243\n",
      "Epoch: 71, Train Loss: 0.158651, Train Error: 0.132758, Test Error: 0.142728\n",
      "Epoch: 72, Train Loss: 0.158931, Train Error: 0.132992, Test Error: 0.142618\n",
      "Epoch: 73, Train Loss: 0.158835, Train Error: 0.133101, Test Error: 0.143217\n",
      "Epoch: 74, Train Loss: 0.158779, Train Error: 0.132882, Test Error: 0.143071\n",
      "Epoch: 75, Train Loss: 0.158346, Train Error: 0.132761, Test Error: 0.142992\n",
      "Val Error: 0.142618\n",
      "Val MAE: 0.1426182095227497\n",
      "----- Fold 3 -----\n",
      "Epoch: 1, Train Loss: 0.301163, Train Error: 0.247976, Test Error: 0.219163\n",
      "Epoch: 2, Train Loss: 0.237019, Train Error: 0.188371, Test Error: 0.172290\n",
      "Epoch: 3, Train Loss: 0.219778, Train Error: 0.175078, Test Error: 0.171111\n",
      "Epoch: 4, Train Loss: 0.215521, Train Error: 0.173977, Test Error: 0.170114\n",
      "Epoch: 5, Train Loss: 0.211959, Train Error: 0.173425, Test Error: 0.169790\n",
      "Epoch: 6, Train Loss: 0.208874, Train Error: 0.172727, Test Error: 0.169373\n",
      "Epoch: 7, Train Loss: 0.207001, Train Error: 0.172433, Test Error: 0.169011\n",
      "Epoch: 8, Train Loss: 0.205329, Train Error: 0.171901, Test Error: 0.168323\n",
      "Epoch: 9, Train Loss: 0.204002, Train Error: 0.171181, Test Error: 0.167696\n",
      "Epoch: 10, Train Loss: 0.202516, Train Error: 0.170523, Test Error: 0.166882\n",
      "Epoch: 11, Train Loss: 0.200794, Train Error: 0.169534, Test Error: 0.165592\n",
      "Epoch: 12, Train Loss: 0.198800, Train Error: 0.168489, Test Error: 0.164220\n",
      "Epoch: 13, Train Loss: 0.197685, Train Error: 0.167295, Test Error: 0.162905\n",
      "Epoch: 14, Train Loss: 0.195934, Train Error: 0.165918, Test Error: 0.161248\n",
      "Epoch: 15, Train Loss: 0.193412, Train Error: 0.164134, Test Error: 0.159967\n",
      "Epoch: 16, Train Loss: 0.192099, Train Error: 0.162896, Test Error: 0.158613\n",
      "Epoch: 17, Train Loss: 0.189670, Train Error: 0.161242, Test Error: 0.157203\n",
      "Epoch: 18, Train Loss: 0.189358, Train Error: 0.160557, Test Error: 0.155810\n",
      "Epoch: 19, Train Loss: 0.186892, Train Error: 0.158791, Test Error: 0.154804\n",
      "Epoch: 20, Train Loss: 0.186668, Train Error: 0.158037, Test Error: 0.153667\n",
      "Epoch: 21, Train Loss: 0.184451, Train Error: 0.156372, Test Error: 0.152494\n",
      "Epoch: 22, Train Loss: 0.183782, Train Error: 0.155541, Test Error: 0.151505\n",
      "Epoch: 23, Train Loss: 0.181565, Train Error: 0.154020, Test Error: 0.150305\n",
      "Epoch: 24, Train Loss: 0.180529, Train Error: 0.152968, Test Error: 0.149495\n",
      "Epoch: 25, Train Loss: 0.179271, Train Error: 0.151807, Test Error: 0.148406\n",
      "Epoch: 26, Train Loss: 0.178270, Train Error: 0.151153, Test Error: 0.147602\n",
      "Epoch: 27, Train Loss: 0.176492, Train Error: 0.149870, Test Error: 0.146766\n",
      "Epoch: 28, Train Loss: 0.176145, Train Error: 0.149166, Test Error: 0.146243\n",
      "Epoch: 29, Train Loss: 0.175055, Train Error: 0.148267, Test Error: 0.145328\n",
      "Epoch: 30, Train Loss: 0.174644, Train Error: 0.147845, Test Error: 0.144808\n",
      "Epoch: 31, Train Loss: 0.174096, Train Error: 0.147316, Test Error: 0.144348\n",
      "Epoch: 32, Train Loss: 0.173023, Train Error: 0.146409, Test Error: 0.143934\n",
      "Epoch: 33, Train Loss: 0.171863, Train Error: 0.145466, Test Error: 0.143162\n",
      "Epoch: 34, Train Loss: 0.172179, Train Error: 0.145386, Test Error: 0.142892\n",
      "Epoch: 35, Train Loss: 0.170989, Train Error: 0.144437, Test Error: 0.142608\n",
      "Epoch: 36, Train Loss: 0.170393, Train Error: 0.144109, Test Error: 0.142054\n",
      "Epoch: 37, Train Loss: 0.169757, Train Error: 0.143246, Test Error: 0.141907\n",
      "Epoch: 38, Train Loss: 0.169047, Train Error: 0.142699, Test Error: 0.141359\n",
      "Epoch: 39, Train Loss: 0.169239, Train Error: 0.142734, Test Error: 0.141223\n",
      "Epoch: 40, Train Loss: 0.168611, Train Error: 0.142304, Test Error: 0.140963\n",
      "Epoch: 41, Train Loss: 0.167944, Train Error: 0.141888, Test Error: 0.140803\n",
      "Epoch: 42, Train Loss: 0.167399, Train Error: 0.141368, Test Error: 0.140302\n",
      "Epoch: 43, Train Loss: 0.167609, Train Error: 0.141304, Test Error: 0.140044\n",
      "Epoch: 44, Train Loss: 0.166717, Train Error: 0.140559, Test Error: 0.139914\n",
      "Epoch: 45, Train Loss: 0.167012, Train Error: 0.140835, Test Error: 0.139908\n",
      "Epoch: 46, Train Loss: 0.166530, Train Error: 0.140250, Test Error: 0.139735\n",
      "Epoch: 47, Train Loss: 0.166560, Train Error: 0.140341, Test Error: 0.139568\n",
      "Epoch: 48, Train Loss: 0.165775, Train Error: 0.139711, Test Error: 0.139432\n",
      "Epoch: 49, Train Loss: 0.165524, Train Error: 0.139422, Test Error: 0.139155\n",
      "Epoch: 50, Train Loss: 0.164682, Train Error: 0.138878, Test Error: 0.139094\n",
      "Epoch: 51, Train Loss: 0.165741, Train Error: 0.139275, Test Error: 0.139154\n",
      "Epoch: 52, Train Loss: 0.164568, Train Error: 0.138749, Test Error: 0.139140\n",
      "Epoch: 53, Train Loss: 0.165082, Train Error: 0.138929, Test Error: 0.138856\n",
      "Epoch: 54, Train Loss: 0.164423, Train Error: 0.138630, Test Error: 0.138889\n",
      "Epoch: 55, Train Loss: 0.163440, Train Error: 0.137751, Test Error: 0.138868\n",
      "Epoch: 56, Train Loss: 0.164308, Train Error: 0.138339, Test Error: 0.138817\n",
      "Epoch: 57, Train Loss: 0.164413, Train Error: 0.138360, Test Error: 0.138804\n",
      "Epoch: 58, Train Loss: 0.163530, Train Error: 0.137693, Test Error: 0.138463\n",
      "Epoch: 59, Train Loss: 0.163340, Train Error: 0.137438, Test Error: 0.139098\n",
      "Epoch: 60, Train Loss: 0.162961, Train Error: 0.137218, Test Error: 0.138849\n",
      "Epoch: 61, Train Loss: 0.163169, Train Error: 0.137182, Test Error: 0.138850\n",
      "Val Error: 0.138463\n",
      "Val MAE: 0.13846265402707186\n"
     ]
    }
   ],
   "source": [
    "cv = KFold(n_splits=args.splits, random_state=random_seed, shuffle=True)\n",
    "\n",
    "ks = [0.9, 0.7, 0.6, 0.5]\n",
    "\n",
    "best_model_fold_list = []\n",
    "data_fold_list = []\n",
    "i = 1\n",
    "for train_index, test_index in cv.split(X):\n",
    "\n",
    "    print(f\"----- Fold {i} -----\")\n",
    "\n",
    "\n",
    "    netG = GSRNet(ks, args).to(device)\n",
    "    optimizerG = optim.Adam(netG.parameters(), lr=args.lr)\n",
    "\n",
    "    netD = Discriminator(input_dim=args.embedding_size, hidden_sizes=[16, 8]).to(device)\n",
    "    optimizerD = optim.Adam(netD.parameters(), lr=args.lr)\n",
    "\n",
    "    subjects_adj, test_adj, subjects_ground_truth, test_ground_truth = X[\n",
    "        train_index], X[test_index], Y[train_index], Y[test_index]\n",
    "    data_fold_list.append((subjects_adj, test_adj, subjects_ground_truth, test_ground_truth))\n",
    "\n",
    "\n",
    "    ##################\n",
    "    # subjects_adj = subjects_adj[:1]\n",
    "    # subjects_ground_truth = subjects_ground_truth[:1]\n",
    "    ##################\n",
    "\n",
    "    # return_model = train_gan(\n",
    "    #     netG, \n",
    "    #     optimizerG, \n",
    "    #     netD,\n",
    "    #     optimizerD,\n",
    "    #     subjects_adj, \n",
    "    #     subjects_ground_truth, \n",
    "    #     args, \n",
    "    #     test_adj=test_adj, \n",
    "    #     test_ground_truth=test_ground_truth\n",
    "    # )\n",
    "\n",
    "    return_model = train(netG, optimizerG, subjects_adj, subjects_ground_truth, args, test_adj, test_ground_truth)\n",
    "    test_mae = test(return_model, test_adj, test_ground_truth, args)\n",
    "    print(f\"Val MAE: {test_mae}\")\n",
    "    best_model_fold_list.append(return_model)\n",
    "\n",
    "    i += 1\n",
    "\n",
    "    # break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MatrixVectorizer import MatrixVectorizer\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "import torch\n",
    "import networkx as nx\n",
    "\n",
    "def evaluate(pred_matrices, gt_matrices):\n",
    "\n",
    "    num_test_samples = gt_matrices.shape[0]\n",
    "\n",
    "    # Initialize lists to store MAEs for each centrality measure\n",
    "    mae_bc = []\n",
    "    mae_ec = []\n",
    "    mae_pc = []\n",
    "\n",
    "    # # Iterate over each test sample\n",
    "    # for i in range(num_test_samples):\n",
    "    #     # Convert adjacency matrices to NetworkX graphs\n",
    "    #     pred_graph = nx.from_numpy_array(pred_matrices[i], edge_attr=\"weight\")\n",
    "    #     gt_graph = nx.from_numpy_array(gt_matrices[i], edge_attr=\"weight\")\n",
    "\n",
    "    #     # Compute centrality measures\n",
    "    #     pred_bc = nx.betweenness_centrality(pred_graph, weight=\"weight\")\n",
    "    #     pred_ec = nx.eigenvector_centrality(pred_graph, weight=\"weight\")\n",
    "    #     pred_pc = nx.pagerank(pred_graph, weight=\"weight\")\n",
    "\n",
    "    #     gt_bc = nx.betweenness_centrality(gt_graph, weight=\"weight\")\n",
    "    #     gt_ec = nx.eigenvector_centrality(gt_graph, weight=\"weight\")\n",
    "    #     gt_pc = nx.pagerank(gt_graph, weight=\"weight\")\n",
    "\n",
    "    #     # Convert centrality dictionaries to lists\n",
    "    #     pred_bc_values = list(pred_bc.values())\n",
    "    #     pred_ec_values = list(pred_ec.values())\n",
    "    #     pred_pc_values = list(pred_pc.values())\n",
    "\n",
    "    #     gt_bc_values = list(gt_bc.values())\n",
    "    #     gt_ec_values = list(gt_ec.values())\n",
    "    #     gt_pc_values = list(gt_pc.values())\n",
    "\n",
    "    #     # Compute MAEs\n",
    "    #     mae_bc.append(mean_absolute_error(pred_bc_values, gt_bc_values))\n",
    "    #     mae_ec.append(mean_absolute_error(pred_ec_values, gt_ec_values))\n",
    "    #     mae_pc.append(mean_absolute_error(pred_pc_values, gt_pc_values))\n",
    "\n",
    "    # # Compute average MAEs\n",
    "    # avg_mae_bc = sum(mae_bc) / len(mae_bc)\n",
    "    # avg_mae_ec = sum(mae_ec) / len(mae_ec)\n",
    "    # avg_mae_pc = sum(mae_pc) / len(mae_pc)\n",
    "\n",
    "    # vectorize and flatten\n",
    "    pred_1d = MatrixVectorizer.vectorize(pred_matrices).flatten()\n",
    "    gt_1d = MatrixVectorizer.vectorize(gt_matrices).flatten()\n",
    "\n",
    "    mae = mean_absolute_error(pred_1d, gt_1d)\n",
    "    pcc = pearsonr(pred_1d, gt_1d)[0]\n",
    "    js_dis = jensenshannon(pred_1d, gt_1d)\n",
    "\n",
    "    print(\"MAE: \", mae)\n",
    "    print(\"PCC: \", pcc)\n",
    "    print(\"Jensen-Shannon Distance: \", js_dis)\n",
    "    # print(\"Average MAE betweenness centrality:\", avg_mae_bc)\n",
    "    # print(\"Average MAE eigenvector centrality:\", avg_mae_ec)\n",
    "    # print(\"Average MAE PageRank centrality:\", avg_mae_pc)\n",
    "    # return mae, pcc, js_dis, avg_mae_bc, avg_mae_ec, avg_mae_pc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  0.13154196853614147\n",
      "PCC:  0.6802722030634454\n",
      "Jensen-Shannon Distance:  0.27837621012900604\n",
      "MAE:  0.14431458607218078\n",
      "PCC:  0.6317420781368348\n",
      "Jensen-Shannon Distance:  0.2964802439885349\n",
      "MAE:  0.14255834417849683\n",
      "PCC:  0.6370953568804921\n",
      "Jensen-Shannon Distance:  0.28816710510793964\n"
     ]
    }
   ],
   "source": [
    "for i in range(args.splits):\n",
    "    _, test_adjs, _, gt_matrices = data_fold_list[i]\n",
    "    model = best_model_fold_list[i]\n",
    "    model.eval()\n",
    "    pred_matrices = np.zeros(gt_matrices.shape)\n",
    "    with torch.no_grad():\n",
    "        for j, test_adj in enumerate(test_adjs):\n",
    "            pred_matrices[j], _, _, _ = model(torch.from_numpy(test_adj))\n",
    "    evaluate(pred_matrices, gt_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(167, 161)\n",
      "[[ 0 13]\n",
      " [ 1 41]\n",
      " [ 2 14]\n",
      " [ 3 48]\n",
      " [ 4 51]]\n",
      "Train size: 133\n",
      "Val size: 34\n",
      "Epoch: 1, Train Loss: 0.294423, Train Error: 0.240760, Test Error: 0.200704\n",
      "Epoch: 2, Train Loss: 0.227672, Train Error: 0.180523, Test Error: 0.173503\n",
      "Epoch: 3, Train Loss: 0.217120, Train Error: 0.174384, Test Error: 0.172363\n",
      "Epoch: 4, Train Loss: 0.212870, Train Error: 0.173416, Test Error: 0.171529\n",
      "Epoch: 5, Train Loss: 0.210128, Train Error: 0.172934, Test Error: 0.171213\n",
      "Epoch: 6, Train Loss: 0.207268, Train Error: 0.172366, Test Error: 0.170932\n",
      "Epoch: 7, Train Loss: 0.205172, Train Error: 0.171777, Test Error: 0.170072\n",
      "Epoch: 8, Train Loss: 0.203600, Train Error: 0.171226, Test Error: 0.169293\n",
      "Epoch: 9, Train Loss: 0.201901, Train Error: 0.170313, Test Error: 0.168599\n",
      "Epoch: 10, Train Loss: 0.199754, Train Error: 0.169206, Test Error: 0.167197\n",
      "Epoch: 11, Train Loss: 0.197757, Train Error: 0.167953, Test Error: 0.165638\n",
      "Epoch: 12, Train Loss: 0.195808, Train Error: 0.166246, Test Error: 0.163807\n",
      "Epoch: 13, Train Loss: 0.193434, Train Error: 0.164329, Test Error: 0.161795\n",
      "Epoch: 14, Train Loss: 0.190894, Train Error: 0.162243, Test Error: 0.159658\n",
      "Epoch: 15, Train Loss: 0.189076, Train Error: 0.160448, Test Error: 0.158151\n",
      "Epoch: 16, Train Loss: 0.187114, Train Error: 0.158514, Test Error: 0.156497\n",
      "Epoch: 17, Train Loss: 0.184757, Train Error: 0.156915, Test Error: 0.155099\n",
      "Epoch: 18, Train Loss: 0.183365, Train Error: 0.155484, Test Error: 0.153872\n",
      "Epoch: 19, Train Loss: 0.181554, Train Error: 0.154041, Test Error: 0.152359\n",
      "Epoch: 20, Train Loss: 0.180581, Train Error: 0.152759, Test Error: 0.151689\n",
      "Epoch: 21, Train Loss: 0.178659, Train Error: 0.151438, Test Error: 0.150328\n",
      "Epoch: 22, Train Loss: 0.177623, Train Error: 0.150459, Test Error: 0.149249\n",
      "Epoch: 23, Train Loss: 0.176801, Train Error: 0.149521, Test Error: 0.148317\n",
      "Epoch: 24, Train Loss: 0.174953, Train Error: 0.148320, Test Error: 0.147609\n",
      "Epoch: 25, Train Loss: 0.174179, Train Error: 0.147457, Test Error: 0.146965\n",
      "Epoch: 26, Train Loss: 0.173707, Train Error: 0.146807, Test Error: 0.146204\n",
      "Epoch: 27, Train Loss: 0.173075, Train Error: 0.146031, Test Error: 0.145556\n",
      "Epoch: 28, Train Loss: 0.171650, Train Error: 0.145343, Test Error: 0.145443\n",
      "Epoch: 29, Train Loss: 0.171318, Train Error: 0.144966, Test Error: 0.144606\n",
      "Epoch: 30, Train Loss: 0.170511, Train Error: 0.144344, Test Error: 0.144042\n",
      "Epoch: 31, Train Loss: 0.170245, Train Error: 0.143901, Test Error: 0.143866\n",
      "Epoch: 32, Train Loss: 0.169394, Train Error: 0.143330, Test Error: 0.143311\n",
      "Epoch: 33, Train Loss: 0.169104, Train Error: 0.143011, Test Error: 0.143239\n",
      "Epoch: 34, Train Loss: 0.168456, Train Error: 0.142378, Test Error: 0.142848\n",
      "Epoch: 35, Train Loss: 0.167921, Train Error: 0.142062, Test Error: 0.142797\n",
      "Epoch: 36, Train Loss: 0.167917, Train Error: 0.141812, Test Error: 0.142379\n",
      "Epoch: 37, Train Loss: 0.167519, Train Error: 0.141445, Test Error: 0.142701\n",
      "Epoch: 38, Train Loss: 0.167120, Train Error: 0.141111, Test Error: 0.142007\n",
      "Epoch: 39, Train Loss: 0.166630, Train Error: 0.140767, Test Error: 0.141911\n",
      "Epoch: 40, Train Loss: 0.166777, Train Error: 0.140520, Test Error: 0.141857\n",
      "Epoch: 41, Train Loss: 0.166233, Train Error: 0.140504, Test Error: 0.141537\n",
      "Epoch: 42, Train Loss: 0.165829, Train Error: 0.140093, Test Error: 0.141541\n",
      "Epoch: 43, Train Loss: 0.165293, Train Error: 0.139838, Test Error: 0.141165\n",
      "Epoch: 44, Train Loss: 0.165095, Train Error: 0.139436, Test Error: 0.141405\n",
      "Epoch: 45, Train Loss: 0.165039, Train Error: 0.139373, Test Error: 0.140713\n",
      "Epoch: 46, Train Loss: 0.164562, Train Error: 0.138987, Test Error: 0.140325\n",
      "Epoch: 47, Train Loss: 0.164254, Train Error: 0.138698, Test Error: 0.140495\n",
      "Epoch: 48, Train Loss: 0.164201, Train Error: 0.138631, Test Error: 0.140509\n",
      "Epoch: 49, Train Loss: 0.163943, Train Error: 0.138279, Test Error: 0.140653\n",
      "Val Error: 0.140325\n"
     ]
    }
   ],
   "source": [
    "A_HR_train = pd.read_csv(\"../data/hr_train.csv\")\n",
    "\n",
    "pca = PCA(n_components=0.99, whiten=False)\n",
    "A_HR_train_pca = pca.fit_transform(A_HR_train)\n",
    "print(A_HR_train_pca.shape)\n",
    "\n",
    "gm = GaussianMixture(n_components=5, random_state=random_seed)\n",
    "A_HR_train_label = gm.fit_predict(A_HR_train_pca)\n",
    "unique, counts = np.unique(A_HR_train_label, return_counts=True)\n",
    "print(np.asarray((unique, counts)).T)\n",
    "\n",
    "X = np.load('A_LR_train_matrix.npy')\n",
    "y = np.load('A_HR_train_matrix.npy')\n",
    "\n",
    "n_sample = X.shape[0]\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X.reshape(n_sample, -1), \n",
    "    y.reshape(n_sample, -1), \n",
    "    test_size=0.20, \n",
    "    random_state=random_seed,\n",
    "    stratify=A_HR_train_label\n",
    ")\n",
    "\n",
    "X_train = X_train.reshape(-1, LR_size, LR_size)\n",
    "X_val = X_val.reshape(-1, LR_size, LR_size)\n",
    "y_train = y_train.reshape(-1, HR_size, HR_size)\n",
    "y_val = y_val.reshape(-1, HR_size, HR_size)\n",
    "\n",
    "print(\"Train size:\", len(X_train))\n",
    "print(\"Val size:\", len(X_val))\n",
    "\n",
    "netG = GSRNet(ks, args).to(device)\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=args.lr)\n",
    "final_model = train(netG, optimizerG, X_train, y_train, args, X_val, y_val)\n",
    "\n",
    "# final_model = best_model_fold_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_pred_list = []\n",
    "final_model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(A_LR_test_matrix.shape[0]):\n",
    "        output_pred, _, _, _ = final_model(torch.Tensor(A_LR_test_matrix[i]))\n",
    "        output_pred = MatrixVectorizer.vectorize(output_pred).tolist()\n",
    "        output_pred_list.append(output_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_pred_stack = np.stack(output_pred_list, axis=0)\n",
    "output_pred_1d = output_pred_stack.flatten()\n",
    "assert output_pred_1d.shape == (4007136, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.661695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.617287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.745749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.656484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.671040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4007131</th>\n",
       "      <td>4007132</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4007132</th>\n",
       "      <td>4007133</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4007133</th>\n",
       "      <td>4007134</td>\n",
       "      <td>0.268915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4007134</th>\n",
       "      <td>4007135</td>\n",
       "      <td>0.242048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4007135</th>\n",
       "      <td>4007136</td>\n",
       "      <td>0.318101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4007136 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID  Predicted\n",
       "0              1   0.661695\n",
       "1              2   0.617287\n",
       "2              3   0.745749\n",
       "3              4   0.656484\n",
       "4              5   0.671040\n",
       "...          ...        ...\n",
       "4007131  4007132   0.000000\n",
       "4007132  4007133   0.000000\n",
       "4007133  4007134   0.268915\n",
       "4007134  4007135   0.242048\n",
       "4007135  4007136   0.318101\n",
       "\n",
       "[4007136 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"ID\": [i+1 for i in range(len(output_pred_1d))],\n",
    "    \"Predicted\": output_pred_1d.tolist()\n",
    "})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"gsr_gat.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
