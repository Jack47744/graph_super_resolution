{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from MatrixVectorizer import *\n",
    "import torch\n",
    "import random\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA not available. Using CPU.\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "# Check for CUDA (GPU support) and set device accordingly\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)  # For multi-GPU setups\n",
    "    # Additional settings for ensuring reproducibility on CUDA\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "else:\n",
    "    # device = torch.device(\"mps\")\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA not available. Using CPU.\")\n",
    "    print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_size = 160\n",
    "HR_size = 268"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def antivectorize_df(adj_mtx_df, size):\n",
    "    \n",
    "    num_subject = adj_mtx_df.shape[0]\n",
    "    adj_mtx = np.zeros((num_subject, size, size)) #torch.zeros((num_subject, LR_size, LR_size))\n",
    "    for i in range(num_subject):\n",
    "        adj_mtx[i] = MatrixVectorizer.anti_vectorize(adj_mtx_df.iloc[i], size) # torch.from_numpy(MatrixVectorizer.anti_vectorize(A_LR_train.iloc[i], LR_size))\n",
    "    return adj_mtx\n",
    "\n",
    "\n",
    "# A_LR_train = pd.read_csv(\"../data/lr_train.csv\")\n",
    "# A_HR_train = pd.read_csv(\"../data/hr_train.csv\")\n",
    "# A_LR_test = pd.read_csv(\"../data/lr_test.csv\")\n",
    "\n",
    "# np.save('A_LR_train_matrix.npy', antivectorize_df(A_LR_train, LR_size))\n",
    "# np.save('A_HR_train_matrix.npy', antivectorize_df(A_HR_train, HR_size))\n",
    "# np.save('A_LR_test_matrix.npy', antivectorize_df(A_LR_test, LR_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(167, 160, 160)\n",
      "(167, 268, 268)\n",
      "(112, 160, 160)\n"
     ]
    }
   ],
   "source": [
    "A_LR_train_matrix = np.load('A_LR_train_matrix.npy')\n",
    "A_HR_train_matrix = np.load('A_HR_train_matrix.npy')\n",
    "A_LR_test_matrix = np.load(\"A_LR_test_matrix.npy\")\n",
    "\n",
    "print(A_LR_train_matrix.shape)\n",
    "print(A_HR_train_matrix.shape)\n",
    "print(A_LR_test_matrix.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on CPU\n",
      "Namespace(epochs=100, lrD=0.025, lrG=0.025, lrA=0.025, splits=3, lr_dim=160, hr_dim=268, padding=26, embedding_size=32, early_stop_patient=3, mean_dense=0.0, std_dense=0.01, mean_gaussian=0.0, std_gaussian=0.1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import KFold\n",
    "from preprocessing import *\n",
    "from model import *\n",
    "from train import *\n",
    "import argparse\n",
    "\n",
    "\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='GSR-Net')\n",
    "parser.add_argument('--epochs', type=int, default=epochs, metavar='no_epochs',\n",
    "                help='number of episode to train ')\n",
    "parser.add_argument('--lrD', type=float, default=0.025, metavar='lr',\n",
    "                help='learning rate D (default: 0.025 using Adam Optimizer)')\n",
    "parser.add_argument('--lrG', type=float, default=0.025, metavar='lr',\n",
    "                help='learning rate G (default: 0.025 using Adam Optimizer)')\n",
    "parser.add_argument('--lrA', type=float, default=0.025, metavar='lr',\n",
    "                help='learning rate A (default: 0.025 using Adam Optimizer)')\n",
    "parser.add_argument('--splits', type=int, default=3, metavar='n_splits',\n",
    "                help='no of cross validation folds')\n",
    "# parser.add_argument('--lmbda', type=int, default=16, metavar='L',\n",
    "#                 help='self-reconstruction error hyperparameter')\n",
    "parser.add_argument('--lr_dim', type=int, default=LR_size, metavar='N',\n",
    "                help='adjacency matrix input dimensions')\n",
    "parser.add_argument('--hr_dim', type=int, default=HR_size, metavar='N',\n",
    "                help='super-resolved adjacency matrix output dimensions')\n",
    "# parser.add_argument('--hidden_dim', type=int, default=280, metavar='N',\n",
    "#                 help='hidden GraphConvolutional layer dimensions')\n",
    "parser.add_argument('--padding', type=int, default=26, metavar='padding',\n",
    "                help='dimensions of padding')\n",
    "# parser.add_argument('--padding', type=int, default=26, metavar='padding',\n",
    "#                 help='dimensions of padding')\n",
    "parser.add_argument('--embedding_size', type=int, default=32, metavar='embedding_size',\n",
    "                help='node embedding size')\n",
    "parser.add_argument('--early_stop_patient', type=int, default=3, metavar='early_stop_patient',\n",
    "                help='early_stop_patience')\n",
    "\n",
    "parser.add_argument('--mean_dense', type=float, default=0., metavar='mean',\n",
    "                        help='mean of the normal distribution in Dense Layer')\n",
    "parser.add_argument('--std_dense', type=float, default=0.01, metavar='std',\n",
    "                    help='standard deviation of the normal distribution in Dense Layer')\n",
    "parser.add_argument('--mean_gaussian', type=float, default=0., metavar='mean',\n",
    "                    help='mean of the normal distribution in Gaussian Noise Layer')\n",
    "parser.add_argument('--std_gaussian', type=float, default=0.1, metavar='std',\n",
    "                    help='standard deviation of the normal distribution in Gaussian Noise Layer')\n",
    "\n",
    "\n",
    "\n",
    "# Create an empty Namespace to hold the default arguments\n",
    "args = parser.parse_args([]) \n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(167, 160, 160)\n",
      "(167, 268, 268)\n"
     ]
    }
   ],
   "source": [
    "# SIMULATING THE DATA: EDIT TO ENTER YOUR OWN DATA\n",
    "X = A_LR_train_matrix #np.random.normal(0, 0.5, (167, 160, 160))\n",
    "Y = A_HR_train_matrix #np.random.normal(0, 0.5, (167, 288, 288))\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = get_device()\n",
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_degree_matrix_normalization_batch_numpy(adjacency_batch):\n",
    "    \"\"\"\n",
    "    Optimizes the degree matrix normalization for a batch of adjacency matrices using NumPy.\n",
    "    Computes the normalized adjacency matrix D^-1 * A for each graph in the batch.\n",
    "    \n",
    "    Parameters:\n",
    "    - adjacency_batch: A NumPy array of shape (batch_size, num_nodes, num_nodes) representing\n",
    "                       a batch of adjacency matrices.\n",
    "\n",
    "    Returns:\n",
    "    - A NumPy array of normalized adjacency matrices.\n",
    "    \"\"\"\n",
    "    epsilon = 1e-6  # Small constant to avoid division by zero\n",
    "    # Calculate the degree for each node in the batch\n",
    "    d = adjacency_batch.sum(axis=2) + epsilon\n",
    "    \n",
    "    # Compute the inverse degree matrix D^-1 for the batch\n",
    "    D_inv = np.reciprocal(d)[:, :, np.newaxis] * np.eye(adjacency_batch.shape[1])[np.newaxis, :, :]\n",
    "    \n",
    "    # Normalize the adjacency matrix using batch matrix multiplication\n",
    "    normalized_adjacency_batch = np.matmul(D_inv, adjacency_batch)\n",
    "    \n",
    "    return normalized_adjacency_batch\n",
    "# X = compute_degree_matrix_normalization_batch_numpy(X)\n",
    "# A_LR_test_matrix = compute_degree_matrix_normalization_batch_numpy(A_LR_test_matrix)\n",
    "# print(X.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Fold 1 -----\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'x'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m netD \u001b[38;5;241m=\u001b[39m Discriminator(args)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     23\u001b[0m optimizerD \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(netD\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mlrD)\n\u001b[0;32m---> 25\u001b[0m return_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_iman\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnetA\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnetA\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizerA\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizerA\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnetG\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnetG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizerG\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizerG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnetD\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnetD\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizerD\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizerD\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubjects_adj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubjects_adj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubjects_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubjects_ground_truth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_adj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_adj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_ground_truth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_ground_truth\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# return_model = train(netG, optimizerG, subjects_adj, subjects_ground_truth, args, test_adj, test_ground_truth)\u001b[39;00m\n\u001b[1;32m     40\u001b[0m test_mae \u001b[38;5;241m=\u001b[39m test(return_model, test_adj, test_ground_truth, args)\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Imperial/Term2/Deep Graph-based Learning/CW2/graph_super_resolution/iman_graph_net/train.py:102\u001b[0m, in \u001b[0;36mtrain_iman\u001b[0;34m(netA, optimizerA, netG, optimizerG, netD, optimizerD, subjects_adj, subjects_labels, args, test_adj, test_ground_truth)\u001b[0m\n\u001b[1;32m     99\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data_source, data_target \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(subjects_adj, subjects_labels):\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;66;03m# ************    Domain alignment    ************\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m     A_output \u001b[38;5;241m=\u001b[39m \u001b[43mnetA\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_source\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m     target \u001b[38;5;241m=\u001b[39m data_target\u001b[38;5;241m.\u001b[39medge_attr\u001b[38;5;241m.\u001b[39mview(args\u001b[38;5;241m.\u001b[39mhr_dim, args\u001b[38;5;241m.\u001b[39mhr_dim)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    105\u001b[0m     target_mean \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(target)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/dgl/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/dgl/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Imperial/Term2/Deep Graph-based Learning/CW2/graph_super_resolution/iman_graph_net/model.py:46\u001b[0m, in \u001b[0;36mAligner.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[0;32m---> 46\u001b[0m     x, edge_index, edge_attr \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m, data\u001b[38;5;241m.\u001b[39mpos_edge_index, data\u001b[38;5;241m.\u001b[39medge_attr\n\u001b[1;32m     48\u001b[0m     x1 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv11(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x, edge_index, edge_attr)))\n\u001b[1;32m     49\u001b[0m     x1 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(x1, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'x'"
     ]
    }
   ],
   "source": [
    "cv = KFold(n_splits=args.splits, random_state=random_seed, shuffle=True)\n",
    "\n",
    "ks = [0.9, 0.7, 0.6, 0.5]\n",
    "\n",
    "best_model_fold_list = []\n",
    "data_fold_list = []\n",
    "i = 1\n",
    "for train_index, test_index in cv.split(X):\n",
    "\n",
    "    print(f\"----- Fold {i} -----\")\n",
    "\n",
    "    subjects_adj, test_adj, subjects_ground_truth, test_ground_truth = X[\n",
    "        train_index], X[test_index], Y[train_index], Y[test_index]\n",
    "    data_fold_list.append((subjects_adj, test_adj, subjects_ground_truth, test_ground_truth))\n",
    "\n",
    "    netA = Aligner(args).to(device)\n",
    "    optimizerA = optim.Adam(netA.parameters(), lr=args.lrA)\n",
    "\n",
    "    netG = Generator(args).to(device)\n",
    "    optimizerG = optim.Adam(netG.parameters(), lr=args.lrG)\n",
    "\n",
    "    netD = Discriminator(args).to(device)\n",
    "    optimizerD = optim.Adam(netD.parameters(), lr=args.lrD)\n",
    "\n",
    "    return_model = train_iman(\n",
    "        netA=netA,\n",
    "        optimizerA=optimizerA,\n",
    "        netG=netG, \n",
    "        optimizerG=optimizerG, \n",
    "        netD=netD,\n",
    "        optimizerD=optimizerD,\n",
    "        subjects_adj=subjects_adj, \n",
    "        subjects_labels=subjects_ground_truth, \n",
    "        args=args, \n",
    "        test_adj=test_adj, \n",
    "        test_ground_truth=test_ground_truth\n",
    "    )\n",
    "\n",
    "    # return_model = train(netG, optimizerG, subjects_adj, subjects_ground_truth, args, test_adj, test_ground_truth)\n",
    "    test_mae = test(return_model, test_adj, test_ground_truth, args)\n",
    "    print(f\"Val MAE: {test_mae}\")\n",
    "    best_model_fold_list.append(return_model)\n",
    "\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MatrixVectorizer import MatrixVectorizer\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "import torch\n",
    "import networkx as nx\n",
    "\n",
    "def evaluate(pred_matrices, gt_matrices):\n",
    "\n",
    "    num_test_samples = gt_matrices.shape[0]\n",
    "\n",
    "    # Initialize lists to store MAEs for each centrality measure\n",
    "    mae_bc = []\n",
    "    mae_ec = []\n",
    "    mae_pc = []\n",
    "\n",
    "    # # Iterate over each test sample\n",
    "    # for i in range(num_test_samples):\n",
    "    #     # Convert adjacency matrices to NetworkX graphs\n",
    "    #     pred_graph = nx.from_numpy_array(pred_matrices[i], edge_attr=\"weight\")\n",
    "    #     gt_graph = nx.from_numpy_array(gt_matrices[i], edge_attr=\"weight\")\n",
    "\n",
    "    #     # Compute centrality measures\n",
    "    #     pred_bc = nx.betweenness_centrality(pred_graph, weight=\"weight\")\n",
    "    #     pred_ec = nx.eigenvector_centrality(pred_graph, weight=\"weight\")\n",
    "    #     pred_pc = nx.pagerank(pred_graph, weight=\"weight\")\n",
    "\n",
    "    #     gt_bc = nx.betweenness_centrality(gt_graph, weight=\"weight\")\n",
    "    #     gt_ec = nx.eigenvector_centrality(gt_graph, weight=\"weight\")\n",
    "    #     gt_pc = nx.pagerank(gt_graph, weight=\"weight\")\n",
    "\n",
    "    #     # Convert centrality dictionaries to lists\n",
    "    #     pred_bc_values = list(pred_bc.values())\n",
    "    #     pred_ec_values = list(pred_ec.values())\n",
    "    #     pred_pc_values = list(pred_pc.values())\n",
    "\n",
    "    #     gt_bc_values = list(gt_bc.values())\n",
    "    #     gt_ec_values = list(gt_ec.values())\n",
    "    #     gt_pc_values = list(gt_pc.values())\n",
    "\n",
    "    #     # Compute MAEs\n",
    "    #     mae_bc.append(mean_absolute_error(pred_bc_values, gt_bc_values))\n",
    "    #     mae_ec.append(mean_absolute_error(pred_ec_values, gt_ec_values))\n",
    "    #     mae_pc.append(mean_absolute_error(pred_pc_values, gt_pc_values))\n",
    "\n",
    "    # # Compute average MAEs\n",
    "    # avg_mae_bc = sum(mae_bc) / len(mae_bc)\n",
    "    # avg_mae_ec = sum(mae_ec) / len(mae_ec)\n",
    "    # avg_mae_pc = sum(mae_pc) / len(mae_pc)\n",
    "\n",
    "    # vectorize and flatten\n",
    "    pred_1d = MatrixVectorizer.vectorize(pred_matrices).flatten()\n",
    "    gt_1d = MatrixVectorizer.vectorize(gt_matrices).flatten()\n",
    "\n",
    "    mae = mean_absolute_error(pred_1d, gt_1d)\n",
    "    pcc = pearsonr(pred_1d, gt_1d)[0]\n",
    "    js_dis = jensenshannon(pred_1d, gt_1d)\n",
    "\n",
    "    print(\"MAE: \", mae)\n",
    "    print(\"PCC: \", pcc)\n",
    "    print(\"Jensen-Shannon Distance: \", js_dis)\n",
    "    # print(\"Average MAE betweenness centrality:\", avg_mae_bc)\n",
    "    # print(\"Average MAE eigenvector centrality:\", avg_mae_ec)\n",
    "    # print(\"Average MAE PageRank centrality:\", avg_mae_pc)\n",
    "    # return mae, pcc, js_dis, avg_mae_bc, avg_mae_ec, avg_mae_pc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  0.1337221793821569\n",
      "PCC:  0.6690317113475489\n",
      "Jensen-Shannon Distance:  0.28160411074519315\n",
      "MAE:  0.14377453968697707\n",
      "PCC:  0.6368867678033938\n",
      "Jensen-Shannon Distance:  0.2941316276366009\n",
      "MAE:  0.14358360996777259\n",
      "PCC:  0.6280176357506309\n",
      "Jensen-Shannon Distance:  0.28623036397916884\n"
     ]
    }
   ],
   "source": [
    "for i in range(args.splits):\n",
    "    _, test_adjs, _, gt_matrices = data_fold_list[i]\n",
    "    model = best_model_fold_list[i]\n",
    "    model.eval()\n",
    "    pred_matrices = np.zeros(gt_matrices.shape)\n",
    "    with torch.no_grad():\n",
    "        for j, test_adj in enumerate(test_adjs):\n",
    "            pred_matrices[j], _, _, _ = model(torch.from_numpy(test_adj))\n",
    "    evaluate(pred_matrices, gt_matrices)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(167, 161)\n",
      "[[ 0 14]\n",
      " [ 1 30]\n",
      " [ 2 30]\n",
      " [ 3 46]\n",
      " [ 4 47]]\n",
      "Train size: 133\n",
      "Val size: 34\n"
     ]
    }
   ],
   "source": [
    "A_HR_train = pd.read_csv(\"../data/hr_train.csv\")\n",
    "\n",
    "pca = PCA(n_components=0.99, whiten=False)\n",
    "A_HR_train_pca = pca.fit_transform(A_HR_train)\n",
    "print(A_HR_train_pca.shape)\n",
    "\n",
    "gm = GaussianMixture(n_components=5, random_state=random_seed)\n",
    "A_HR_train_label = gm.fit_predict(A_HR_train_pca)\n",
    "unique, counts = np.unique(A_HR_train_label, return_counts=True)\n",
    "print(np.asarray((unique, counts)).T)\n",
    "\n",
    "X = np.load('A_LR_train_matrix.npy')\n",
    "y = np.load('A_HR_train_matrix.npy')\n",
    "\n",
    "n_sample = X.shape[0]\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X.reshape(n_sample, -1), \n",
    "    y.reshape(n_sample, -1), \n",
    "    test_size=0.20, \n",
    "    random_state=random_seed,\n",
    "    stratify=A_HR_train_label\n",
    ")\n",
    "\n",
    "X_train = X_train.reshape(-1, LR_size, LR_size)\n",
    "X_val = X_val.reshape(-1, LR_size, LR_size)\n",
    "y_train = y_train.reshape(-1, HR_size, HR_size)\n",
    "y_val = y_val.reshape(-1, HR_size, HR_size)\n",
    "\n",
    "print(\"Train size:\", len(X_train))\n",
    "print(\"Val size:\", len(X_val))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress:   0%|          | 0/200 [00:00<?, ?epoch/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress:   0%|          | 1/200 [00:26<1:27:50, 26.49s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 2.023875, Train Error: 0.246109, Test Error: 0.214552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress:   1%|          | 2/200 [00:53<1:29:17, 27.06s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train Loss: 2.414312, Train Error: 0.184230, Test Error: 0.169388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress:   2%|▏         | 3/200 [01:22<1:31:47, 27.96s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Train Loss: 3.492870, Train Error: 0.174615, Test Error: 0.168158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress:   2%|▏         | 4/200 [01:50<1:30:35, 27.73s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Train Loss: 4.350230, Train Error: 0.173600, Test Error: 0.167503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress:   2%|▎         | 5/200 [02:17<1:29:52, 27.66s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Train Loss: 5.026290, Train Error: 0.172799, Test Error: 0.166740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress:   3%|▎         | 6/200 [02:43<1:27:41, 27.12s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Train Loss: 5.488374, Train Error: 0.171931, Test Error: 0.166027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress:   4%|▎         | 7/200 [03:10<1:26:15, 26.82s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Train Loss: 5.784516, Train Error: 0.170924, Test Error: 0.164946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress:   4%|▍         | 8/200 [03:37<1:26:08, 26.92s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Train Loss: 5.930690, Train Error: 0.169634, Test Error: 0.163561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress:   4%|▍         | 9/200 [04:03<1:24:52, 26.66s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Train Loss: 6.071169, Train Error: 0.167955, Test Error: 0.161598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress:   5%|▌         | 10/200 [04:29<1:24:04, 26.55s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Train Loss: 6.119112, Train Error: 0.165761, Test Error: 0.159212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress:   6%|▌         | 11/200 [05:00<1:27:22, 27.74s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Train Loss: 6.112508, Train Error: 0.163287, Test Error: 0.156630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress:   6%|▌         | 12/200 [05:26<1:25:39, 27.34s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Train Loss: 6.070328, Train Error: 0.160897, Test Error: 0.154057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress:   6%|▋         | 13/200 [05:53<1:24:26, 27.09s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Train Loss: 6.087673, Train Error: 0.158530, Test Error: 0.152158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress:   7%|▋         | 14/200 [06:19<1:23:49, 27.04s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Train Loss: 6.102086, Train Error: 0.156615, Test Error: 0.150223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress:   8%|▊         | 15/200 [06:47<1:23:30, 27.08s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Train Loss: 6.142952, Train Error: 0.154862, Test Error: 0.148491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress:   8%|▊         | 16/200 [07:13<1:22:29, 26.90s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, Train Loss: 6.245514, Train Error: 0.153333, Test Error: 0.147355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress:   8%|▊         | 17/200 [07:40<1:21:58, 26.88s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, Train Loss: 6.355291, Train Error: 0.151928, Test Error: 0.145985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress:   9%|▉         | 18/200 [08:06<1:21:08, 26.75s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18, Train Loss: 6.550700, Train Error: 0.150726, Test Error: 0.144965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress:  10%|▉         | 19/200 [08:33<1:20:39, 26.74s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, Train Loss: 6.713019, Train Error: 0.149540, Test Error: 0.143922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress:  10%|█         | 20/200 [09:00<1:19:59, 26.67s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, Train Loss: 6.929258, Train Error: 0.148407, Test Error: 0.143182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress:  10%|█         | 21/200 [09:27<1:19:44, 26.73s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21, Train Loss: 7.132182, Train Error: 0.147524, Test Error: 0.142391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress:  11%|█         | 22/200 [09:53<1:19:13, 26.70s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22, Train Loss: 7.422778, Train Error: 0.146649, Test Error: 0.141669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress:  12%|█▏        | 23/200 [10:20<1:18:59, 26.77s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23, Train Loss: 7.797724, Train Error: 0.145883, Test Error: 0.140802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress:  12%|█▏        | 24/200 [10:47<1:18:57, 26.92s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24, Train Loss: 8.253225, Train Error: 0.145115, Test Error: 0.140395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress:  12%|█▎        | 25/200 [11:14<1:18:12, 26.81s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25, Train Loss: 8.603925, Train Error: 0.144467, Test Error: 0.139784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress:  13%|█▎        | 26/200 [11:41<1:17:54, 26.87s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26, Train Loss: 9.058057, Train Error: 0.143782, Test Error: 0.139194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress:  14%|█▎        | 27/200 [12:09<1:18:46, 27.32s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27, Train Loss: 9.468748, Train Error: 0.143048, Test Error: 0.138739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress:  14%|█▍        | 28/200 [12:35<1:17:07, 26.90s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28, Train Loss: 9.884005, Train Error: 0.142446, Test Error: 0.138083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress:  14%|█▍        | 29/200 [13:01<1:15:42, 26.57s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29, Train Loss: 10.219618, Train Error: 0.141901, Test Error: 0.137680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress:  15%|█▌        | 30/200 [13:27<1:14:47, 26.40s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30, Train Loss: 10.524211, Train Error: 0.141393, Test Error: 0.137385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress:  16%|█▌        | 31/200 [13:54<1:15:13, 26.71s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31, Train Loss: 10.874853, Train Error: 0.140967, Test Error: 0.137020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress:  16%|█▌        | 32/200 [14:21<1:14:21, 26.56s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32, Train Loss: 11.208862, Train Error: 0.140628, Test Error: 0.136728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress:  16%|█▋        | 33/200 [14:47<1:13:26, 26.38s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33, Train Loss: 11.634868, Train Error: 0.140185, Test Error: 0.136325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress:  17%|█▋        | 34/200 [15:13<1:13:02, 26.40s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34, Train Loss: 11.954612, Train Error: 0.139801, Test Error: 0.136059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress:  18%|█▊        | 35/200 [15:40<1:12:49, 26.48s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35, Train Loss: 12.274712, Train Error: 0.139512, Test Error: 0.135896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress:  18%|█▊        | 36/200 [16:06<1:12:08, 26.39s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36, Train Loss: 12.462278, Train Error: 0.139195, Test Error: 0.136039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress:  18%|█▊        | 37/200 [16:33<1:12:27, 26.67s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37, Train Loss: 12.754637, Train Error: 0.138977, Test Error: 0.136175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress:  19%|█▉        | 38/200 [17:00<1:11:52, 26.62s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38, Train Loss: 13.112604, Train Error: 0.138541, Test Error: 0.135931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress:  20%|█▉        | 39/200 [17:26<1:11:28, 26.64s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39, Train Loss: 13.423132, Train Error: 0.138117, Test Error: 0.135578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress:  20%|██        | 40/200 [17:52<1:10:29, 26.43s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40, Train Loss: 13.715359, Train Error: 0.137827, Test Error: 0.135187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress:  20%|██        | 41/200 [18:18<1:09:45, 26.32s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41, Train Loss: 13.863079, Train Error: 0.137576, Test Error: 0.134791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress:  21%|██        | 42/200 [18:44<1:08:57, 26.18s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 42, Train Loss: 13.962252, Train Error: 0.137235, Test Error: 0.134544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress:  22%|██▏       | 43/200 [19:11<1:08:54, 26.33s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 43, Train Loss: 14.087437, Train Error: 0.137018, Test Error: 0.134329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress:  22%|██▏       | 44/200 [19:37<1:08:34, 26.37s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 44, Train Loss: 14.225527, Train Error: 0.136764, Test Error: 0.134430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress:  22%|██▎       | 45/200 [20:03<1:07:47, 26.24s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 45, Train Loss: 14.507247, Train Error: 0.136489, Test Error: 0.134499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress:  23%|██▎       | 46/200 [20:30<1:07:33, 26.32s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 46, Train Loss: 14.663796, Train Error: 0.136374, Test Error: 0.134747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress:  23%|██▎       | 46/200 [21:00<1:10:18, 27.39s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Error: 0.134329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "netG = GSRNet(ks, args).to(device)\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=args.lr)\n",
    "\n",
    "netD = Discriminator(args).to(device)\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=args.lr)\n",
    "\n",
    "final_model = train_gan(\n",
    "    netG, \n",
    "    optimizerG, \n",
    "    netD,\n",
    "    optimizerD,\n",
    "    X_train, \n",
    "    y_train, \n",
    "    args, \n",
    "    test_adj=X_val, \n",
    "    test_ground_truth=y_val\n",
    ")\n",
    "# final_model = train(netG, optimizerG, X_train, y_train, args, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "MAE:  0.13772890941784888\n",
      "PCC:  0.5828529207172568\n",
      "Jensen-Shannon Distance:  0.30501884605713386\n",
      "Val\n",
      "MAE:  0.13137430550865048\n",
      "PCC:  0.6699868739227568\n",
      "Jensen-Shannon Distance:  0.2891569937294208\n"
     ]
    }
   ],
   "source": [
    "final_model.eval()\n",
    "pred_train_matrices = np.zeros(y_train.shape)\n",
    "pred_val_matrices = np.zeros(y_val.shape)\n",
    "with torch.no_grad():\n",
    "    for j, test_adj in enumerate(X_train):\n",
    "        pred_train_matrices[j], _, _, _ = final_model(torch.from_numpy(test_adj))\n",
    "\n",
    "    print(\"Train\")\n",
    "    evaluate(pred_train_matrices, y_train)\n",
    "\n",
    "    for j, test_adj in enumerate(X_val):\n",
    "        pred_val_matrices[j], _, _, _ = final_model(torch.from_numpy(test_adj))\n",
    "\n",
    "    print(\"Val\")\n",
    "    evaluate(pred_val_matrices, y_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_pred_list = []\n",
    "final_model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(A_LR_test_matrix.shape[0]):\n",
    "        output_pred, _, _, _ = final_model(torch.Tensor(A_LR_test_matrix[i]))\n",
    "        output_pred = MatrixVectorizer.vectorize(output_pred).tolist()\n",
    "        output_pred_list.append(output_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_pred_stack = np.stack(output_pred_list, axis=0)\n",
    "output_pred_1d = output_pred_stack.flatten()\n",
    "assert output_pred_1d.shape == (4007136, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.506379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.544307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.615777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.468869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.431444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4007131</th>\n",
       "      <td>4007132</td>\n",
       "      <td>0.038365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4007132</th>\n",
       "      <td>4007133</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4007133</th>\n",
       "      <td>4007134</td>\n",
       "      <td>0.241379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4007134</th>\n",
       "      <td>4007135</td>\n",
       "      <td>0.177764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4007135</th>\n",
       "      <td>4007136</td>\n",
       "      <td>0.458423</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4007136 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID  Predicted\n",
       "0              1   0.506379\n",
       "1              2   0.544307\n",
       "2              3   0.615777\n",
       "3              4   0.468869\n",
       "4              5   0.431444\n",
       "...          ...        ...\n",
       "4007131  4007132   0.038365\n",
       "4007132  4007133   0.000000\n",
       "4007133  4007134   0.241379\n",
       "4007134  4007135   0.177764\n",
       "4007135  4007136   0.458423\n",
       "\n",
       "[4007136 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"ID\": [i+1 for i in range(len(output_pred_1d))],\n",
    "    \"Predicted\": output_pred_1d.tolist()\n",
    "})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"gsr_gan_gat_relu.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
