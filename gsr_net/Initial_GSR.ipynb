{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "\n",
    "import pandas as pd\n",
    "from MatrixVectorizer import *\n",
    "import torch\n",
    "import random\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "# Check for CUDA (GPU support) and set device accordingly\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)  # For multi-GPU setups\n",
    "    # Additional settings for ensuring reproducibility on CUDA\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_LR_train = pd.read_csv(\"../data/lr_train.csv\")\n",
    "A_HR_train = pd.read_csv(\"../data/hr_train.csv\")\n",
    "A_LR_test = pd.read_csv(\"../data/lr_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_size = 160\n",
    "HR_size = 268"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# A_HR_train = pd.read_csv(\"../data/hr_train.csv\")\n",
    "\n",
    "# pca = PCA(n_components=0.99, whiten=False)\n",
    "# A_HR_train_pca = pca.fit_transform(A_HR_train)\n",
    "# print(A_HR_train_pca.shape)\n",
    "\n",
    "# gm = GaussianMixture(n_components=5, random_state=random_seed)\n",
    "# A_HR_train_label = gm.fit_predict(A_HR_train_pca)\n",
    "# unique, counts = np.unique(A_HR_train_label, return_counts=True)\n",
    "# print(np.asarray((unique, counts)).T)\n",
    "\n",
    "# X = np.load('A_LR_train_matrix.npy')\n",
    "# y = np.load('A_HR_train_matrix.npy')\n",
    "\n",
    "# n_sample = X.shape[0]\n",
    "# X_train, X_val, y_train, y_val = train_test_split(\n",
    "#     X.reshape(n_sample, -1), \n",
    "#     y.reshape(n_sample, -1), \n",
    "#     test_size=0.20, \n",
    "#     random_state=random_seed,\n",
    "#     stratify=A_HR_train_label\n",
    "# )\n",
    "\n",
    "# X_train = X_train.reshape(-1, LR_size, LR_size)\n",
    "# X_val = X_val.reshape(-1, LR_size, LR_size)\n",
    "# y_train = y_train.reshape(-1, HR_size, HR_size)\n",
    "# y_val = y_val.reshape(-1, HR_size, HR_size)\n",
    "\n",
    "# print(\"Train size:\", len(X_train))\n",
    "# print(\"Val size:\", len(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def antivectorize_df(adj_mtx_df, size):\n",
    "    \n",
    "#     num_subject = adj_mtx_df.shape[0]\n",
    "#     adj_mtx = np.zeros((num_subject, size, size)) #torch.zeros((num_subject, LR_size, LR_size))\n",
    "#     for i in range(num_subject):\n",
    "#         adj_mtx[i] = MatrixVectorizer.anti_vectorize(adj_mtx_df.iloc[i], size) # torch.from_numpy(MatrixVectorizer.anti_vectorize(A_LR_train.iloc[i], LR_size))\n",
    "#     return adj_mtx\n",
    "\n",
    "# np.save('A_LR_train_matrix.npy', antivectorize_df(A_LR_train, LR_size))\n",
    "# np.save('A_HR_train_matrix.npy', antivectorize_df(A_HR_train, HR_size))\n",
    "# np.save('A_LR_test_matrix.npy', antivectorize_df(A_LR_test, LR_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(167, 160, 160)\n",
      "(167, 268, 268)\n",
      "(112, 160, 160)\n"
     ]
    }
   ],
   "source": [
    "A_LR_train_matrix = np.load('A_LR_train_matrix.npy')\n",
    "A_HR_train_matrix = np.load('A_HR_train_matrix.npy')\n",
    "A_LR_test_matrix = np.load(\"A_LR_test_matrix.npy\")\n",
    "\n",
    "print(A_LR_train_matrix.shape)\n",
    "print(A_HR_train_matrix.shape)\n",
    "print(A_LR_test_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(epochs=200, lr=0.0001, splits=3, lmbda=16, lr_dim=160, hr_dim=268, hidden_dim=268, padding=26, embedding_size=32, early_stop_patient=10, p_perturbe=0.5, p_drop_node=0.03, p_drop_edges=0.1, mean_dense=0.0, std_dense=0.01, mean_gaussian=0.0, std_gaussian=0.1)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Main function of Graph Super-Resolution Network (GSR-Net) framework \n",
    "   for predicting high-resolution brain connectomes from low-resolution connectomes. \n",
    "    \n",
    "    ---------------------------------------------------------------------\n",
    "    \n",
    "    This file contains the implementation of the training and testing process of our GSR-Net model.\n",
    "        train(model, optimizer, subjects_adj, subjects_ground_truth, args)\n",
    "\n",
    "                Inputs:\n",
    "                        model:        constructor of our GSR-Net model:  model = GSRNet(ks,args)\n",
    "                                      ks:   array that stores reduction rates of nodes in Graph U-Net pooling layers\n",
    "                                      args: parsed command line arguments\n",
    "\n",
    "                        optimizer:    constructor of our model's optimizer (borrowed from PyTorch)  \n",
    "\n",
    "                        subjects_adj: (n × l x l) tensor stacking LR connectivity matrices of all training subjects\n",
    "                                       n: the total number of subjects\n",
    "                                       l: the dimensions of the LR connectivity matrices\n",
    "\n",
    "                        subjects_ground_truth: (n × h x h) tensor stacking LR connectivity matrices of all training subjects\n",
    "                                                n: the total number of subjects\n",
    "                                                h: the dimensions of the LR connectivity matrices\n",
    "\n",
    "                        args:          parsed command line arguments, to learn more about the arguments run: \n",
    "                                       python demo.py --help\n",
    "                Output:\n",
    "                        for each epoch, prints out the mean training MSE error\n",
    "\n",
    "\n",
    "            \n",
    "        test(model, test_adj,test_ground_truth,args)\n",
    "\n",
    "                Inputs:\n",
    "                        test_adj:      (n × l x l) tensor stacking LR connectivity matrices of all testing subjects\n",
    "                                        n: the total number of subjects\n",
    "                                        l: the dimensions of the LR connectivity matrices\n",
    "\n",
    "                        test_ground_truth:      (n × h x h) tensor stacking LR connectivity matrices of all testing subjects\n",
    "                                                 n: the total number of subjects\n",
    "                                                 h: the dimensions of the LR connectivity matrices\n",
    "\n",
    "                        see train method above for model and args.\n",
    "\n",
    "                Outputs:\n",
    "                        for each epoch, prints out the mean testing MSE error\n",
    "\n",
    "\n",
    "    To evaluate our framework we used 5-fold cross-validation strategy.\n",
    "\n",
    "    ---------------------------------------------------------------------\n",
    "    Copyright 2020 Megi Isallari, Istanbul Technical University.\n",
    "    All rights reserved.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import KFold\n",
    "from preprocessing import *\n",
    "from model import *\n",
    "from train import *\n",
    "import argparse\n",
    "\n",
    "\n",
    "\n",
    "epochs = 200\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='GSR-Net')\n",
    "parser.add_argument('--epochs', type=int, default=epochs, metavar='no_epochs',\n",
    "                help='number of episode to train ')\n",
    "parser.add_argument('--lr', type=float, default=0.0001, metavar='lr',\n",
    "                help='learning rate (default: 0.0001 using Adam Optimizer)')\n",
    "parser.add_argument('--splits', type=int, default=3, metavar='n_splits',\n",
    "                help='no of cross validation folds')\n",
    "parser.add_argument('--lmbda', type=int, default=16, metavar='L',\n",
    "                help='self-reconstruction error hyperparameter')\n",
    "parser.add_argument('--lr_dim', type=int, default=LR_size, metavar='N',\n",
    "                help='adjacency matrix input dimensions')\n",
    "parser.add_argument('--hr_dim', type=int, default=HR_size, metavar='N',\n",
    "                help='super-resolved adjacency matrix output dimensions')\n",
    "parser.add_argument('--hidden_dim', type=int, default=268, metavar='N',\n",
    "                help='hidden GraphConvolutional layer dimensions')\n",
    "parser.add_argument('--padding', type=int, default=26, metavar='padding',\n",
    "                help='dimensions of padding')\n",
    "parser.add_argument('--embedding_size', type=int, default=32, metavar='embedding_size',\n",
    "                help='node embedding size')\n",
    "parser.add_argument('--early_stop_patient', type=int, default=10, metavar='early_stop_patient',\n",
    "                help='early_stop_patience')\n",
    "\n",
    "parser.add_argument('--p_perturbe', type=float, default=0.5, metavar='p_perturbe',\n",
    "                help='p_perturbe')\n",
    "parser.add_argument('--p_drop_node', type=float, default=0.03, metavar='p_drop_node',\n",
    "                help='p_drop_node')\n",
    "parser.add_argument('--p_drop_edges', type=float, default=0.1, metavar='p_drop_edges',\n",
    "                help='p_drop_edges')\n",
    "parser.add_argument('--mean_dense', type=float, default=0., metavar='mean',\n",
    "                        help='mean of the normal distribution in Dense Layer')\n",
    "parser.add_argument('--std_dense', type=float, default=0.01, metavar='std',\n",
    "                    help='standard deviation of the normal distribution in Dense Layer')\n",
    "parser.add_argument('--mean_gaussian', type=float, default=0., metavar='mean',\n",
    "                    help='mean of the normal distribution in Gaussian Noise Layer')\n",
    "parser.add_argument('--std_gaussian', type=float, default=0.1, metavar='std',\n",
    "                    help='standard deviation of the normal distribution in Gaussian Noise Layer')\n",
    "\n",
    "\n",
    "# Create an empty Namespace to hold the default arguments\n",
    "args = parser.parse_args([]) \n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(167, 160, 160)\n",
      "(167, 268, 268)\n"
     ]
    }
   ],
   "source": [
    "# SIMULATING THE DATA: EDIT TO ENTER YOUR OWN DATA\n",
    "X = A_LR_train_matrix #np.random.normal(0, 0.5, (167, 160, 160))\n",
    "Y = A_HR_train_matrix #np.random.normal(0, 0.5, (167, 288, 288))\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "device = get_device()\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(167, 160, 160)\n"
     ]
    }
   ],
   "source": [
    "def compute_degree_matrix_normalization_batch_numpy(adjacency_batch):\n",
    "    \"\"\"\n",
    "    Optimizes the degree matrix normalization for a batch of adjacency matrices using NumPy.\n",
    "    Computes the normalized adjacency matrix D^-1 * A for each graph in the batch.\n",
    "    \n",
    "    Parameters:\n",
    "    - adjacency_batch: A NumPy array of shape (batch_size, num_nodes, num_nodes) representing\n",
    "                       a batch of adjacency matrices.\n",
    "\n",
    "    Returns:\n",
    "    - A NumPy array of normalized adjacency matrices.\n",
    "    \"\"\"\n",
    "    epsilon = 1e-6  # Small constant to avoid division by zero\n",
    "    # Calculate the degree for each node in the batch\n",
    "    d = adjacency_batch.sum(axis=2) + epsilon\n",
    "    \n",
    "    # Compute the inverse degree matrix D^-1 for the batch\n",
    "    D_inv = np.reciprocal(d)[:, :, np.newaxis] * np.eye(adjacency_batch.shape[1])[np.newaxis, :, :]\n",
    "    \n",
    "    # Normalize the adjacency matrix using batch matrix multiplication\n",
    "    normalized_adjacency_batch = np.matmul(D_inv, adjacency_batch)\n",
    "    \n",
    "    return normalized_adjacency_batch\n",
    "X = compute_degree_matrix_normalization_batch_numpy(X)\n",
    "A_LR_test_matrix = compute_degree_matrix_normalization_batch_numpy(A_LR_test_matrix)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Fold 1 -----\n",
      "0.5 0.03 0.1\n",
      "Epoch: 1, Train Loss: 0.309430, Train Error: 0.250861, Test Error: 0.222269\n",
      "Epoch: 2, Train Loss: 0.248137, Train Error: 0.195264, Test Error: 0.169498\n",
      "Epoch: 3, Train Loss: 0.225787, Train Error: 0.177524, Test Error: 0.167621\n",
      "Epoch: 4, Train Loss: 0.219591, Train Error: 0.176320, Test Error: 0.167074\n",
      "Epoch: 5, Train Loss: 0.216312, Train Error: 0.175575, Test Error: 0.166250\n",
      "Epoch: 6, Train Loss: 0.213685, Train Error: 0.174963, Test Error: 0.166066\n",
      "Epoch: 7, Train Loss: 0.210200, Train Error: 0.174332, Test Error: 0.165294\n",
      "Epoch: 8, Train Loss: 0.208762, Train Error: 0.173719, Test Error: 0.164444\n",
      "Epoch: 9, Train Loss: 0.206056, Train Error: 0.173002, Test Error: 0.163748\n",
      "Epoch: 10, Train Loss: 0.205180, Train Error: 0.172084, Test Error: 0.162574\n",
      "Epoch: 11, Train Loss: 0.203807, Train Error: 0.171190, Test Error: 0.161436\n",
      "Epoch: 12, Train Loss: 0.201610, Train Error: 0.169927, Test Error: 0.159855\n",
      "Epoch: 13, Train Loss: 0.199169, Train Error: 0.168408, Test Error: 0.158348\n",
      "Epoch: 14, Train Loss: 0.198222, Train Error: 0.167181, Test Error: 0.156708\n",
      "Epoch: 15, Train Loss: 0.195916, Train Error: 0.165418, Test Error: 0.155167\n",
      "Epoch: 16, Train Loss: 0.192650, Train Error: 0.163137, Test Error: 0.153350\n",
      "Epoch: 17, Train Loss: 0.191806, Train Error: 0.161408, Test Error: 0.151602\n",
      "Epoch: 18, Train Loss: 0.189185, Train Error: 0.159770, Test Error: 0.151097\n",
      "Epoch: 19, Train Loss: 0.188166, Train Error: 0.158515, Test Error: 0.149322\n",
      "Epoch: 20, Train Loss: 0.186584, Train Error: 0.157270, Test Error: 0.148389\n",
      "Epoch: 21, Train Loss: 0.185038, Train Error: 0.156189, Test Error: 0.147138\n",
      "Epoch: 22, Train Loss: 0.183662, Train Error: 0.154849, Test Error: 0.146444\n",
      "Epoch: 23, Train Loss: 0.182531, Train Error: 0.153998, Test Error: 0.145323\n",
      "Epoch: 24, Train Loss: 0.181253, Train Error: 0.152564, Test Error: 0.144594\n",
      "Epoch: 25, Train Loss: 0.180417, Train Error: 0.151626, Test Error: 0.143666\n",
      "Epoch: 26, Train Loss: 0.179512, Train Error: 0.151420, Test Error: 0.143229\n",
      "Epoch: 27, Train Loss: 0.178247, Train Error: 0.150440, Test Error: 0.142400\n",
      "Epoch: 28, Train Loss: 0.177569, Train Error: 0.149462, Test Error: 0.142038\n",
      "Epoch: 29, Train Loss: 0.177375, Train Error: 0.149447, Test Error: 0.141492\n",
      "Epoch: 30, Train Loss: 0.176295, Train Error: 0.148533, Test Error: 0.141090\n",
      "Epoch: 31, Train Loss: 0.175140, Train Error: 0.147703, Test Error: 0.140386\n",
      "Epoch: 32, Train Loss: 0.174542, Train Error: 0.147194, Test Error: 0.140164\n",
      "Epoch: 33, Train Loss: 0.174736, Train Error: 0.147087, Test Error: 0.139748\n",
      "Epoch: 34, Train Loss: 0.174373, Train Error: 0.146868, Test Error: 0.139484\n",
      "Epoch: 35, Train Loss: 0.173005, Train Error: 0.145585, Test Error: 0.138940\n",
      "Epoch: 36, Train Loss: 0.172803, Train Error: 0.145605, Test Error: 0.138879\n",
      "Epoch: 37, Train Loss: 0.172925, Train Error: 0.145632, Test Error: 0.138246\n",
      "Epoch: 38, Train Loss: 0.172433, Train Error: 0.145055, Test Error: 0.138243\n",
      "Epoch: 39, Train Loss: 0.171466, Train Error: 0.144773, Test Error: 0.137841\n",
      "Epoch: 40, Train Loss: 0.171010, Train Error: 0.144020, Test Error: 0.137448\n",
      "Epoch: 41, Train Loss: 0.170754, Train Error: 0.143771, Test Error: 0.137095\n",
      "Epoch: 42, Train Loss: 0.170950, Train Error: 0.144049, Test Error: 0.137048\n",
      "Epoch: 43, Train Loss: 0.170573, Train Error: 0.143673, Test Error: 0.136886\n",
      "Epoch: 44, Train Loss: 0.169756, Train Error: 0.142934, Test Error: 0.136668\n",
      "Epoch: 45, Train Loss: 0.170104, Train Error: 0.143060, Test Error: 0.136706\n",
      "Epoch: 46, Train Loss: 0.169818, Train Error: 0.142923, Test Error: 0.136621\n",
      "Epoch: 47, Train Loss: 0.169499, Train Error: 0.142807, Test Error: 0.136659\n",
      "Epoch: 48, Train Loss: 0.169552, Train Error: 0.142625, Test Error: 0.136450\n",
      "Epoch: 49, Train Loss: 0.168544, Train Error: 0.142314, Test Error: 0.136204\n",
      "Epoch: 50, Train Loss: 0.168685, Train Error: 0.141910, Test Error: 0.135908\n",
      "Epoch: 51, Train Loss: 0.168327, Train Error: 0.141738, Test Error: 0.135592\n",
      "Epoch: 52, Train Loss: 0.168428, Train Error: 0.141787, Test Error: 0.135788\n",
      "Epoch: 53, Train Loss: 0.168614, Train Error: 0.141583, Test Error: 0.135351\n",
      "Epoch: 54, Train Loss: 0.167944, Train Error: 0.141257, Test Error: 0.135265\n",
      "Epoch: 55, Train Loss: 0.167307, Train Error: 0.140744, Test Error: 0.135396\n",
      "Epoch: 56, Train Loss: 0.167737, Train Error: 0.141183, Test Error: 0.135124\n",
      "Epoch: 57, Train Loss: 0.167206, Train Error: 0.140376, Test Error: 0.135225\n",
      "Epoch: 58, Train Loss: 0.167512, Train Error: 0.141034, Test Error: 0.134921\n",
      "Epoch: 59, Train Loss: 0.167249, Train Error: 0.140545, Test Error: 0.134971\n",
      "Epoch: 60, Train Loss: 0.167077, Train Error: 0.140446, Test Error: 0.134780\n",
      "Epoch: 61, Train Loss: 0.167060, Train Error: 0.140451, Test Error: 0.135540\n",
      "Epoch: 62, Train Loss: 0.166418, Train Error: 0.140147, Test Error: 0.134557\n",
      "Epoch: 63, Train Loss: 0.166164, Train Error: 0.139832, Test Error: 0.134362\n",
      "Epoch: 64, Train Loss: 0.165793, Train Error: 0.139268, Test Error: 0.134366\n",
      "Epoch: 65, Train Loss: 0.166354, Train Error: 0.140058, Test Error: 0.134862\n",
      "Epoch: 66, Train Loss: 0.165841, Train Error: 0.139508, Test Error: 0.134257\n",
      "Epoch: 67, Train Loss: 0.165228, Train Error: 0.138909, Test Error: 0.134163\n",
      "Epoch: 68, Train Loss: 0.166209, Train Error: 0.139557, Test Error: 0.134894\n",
      "Epoch: 69, Train Loss: 0.165342, Train Error: 0.138917, Test Error: 0.134421\n",
      "Epoch: 70, Train Loss: 0.165162, Train Error: 0.138682, Test Error: 0.134216\n",
      "Epoch: 71, Train Loss: 0.165219, Train Error: 0.138873, Test Error: 0.133914\n",
      "Epoch: 72, Train Loss: 0.165466, Train Error: 0.138752, Test Error: 0.133793\n",
      "Epoch: 73, Train Loss: 0.164586, Train Error: 0.138390, Test Error: 0.133735\n",
      "Epoch: 74, Train Loss: 0.165563, Train Error: 0.139142, Test Error: 0.134035\n",
      "Epoch: 75, Train Loss: 0.164773, Train Error: 0.138528, Test Error: 0.133657\n",
      "Epoch: 76, Train Loss: 0.163950, Train Error: 0.137920, Test Error: 0.133750\n",
      "Epoch: 77, Train Loss: 0.164430, Train Error: 0.138140, Test Error: 0.133584\n",
      "Epoch: 78, Train Loss: 0.163787, Train Error: 0.137647, Test Error: 0.133573\n",
      "Epoch: 79, Train Loss: 0.163942, Train Error: 0.137644, Test Error: 0.133452\n",
      "Epoch: 80, Train Loss: 0.163338, Train Error: 0.137027, Test Error: 0.133329\n",
      "Epoch: 81, Train Loss: 0.164096, Train Error: 0.137571, Test Error: 0.133943\n",
      "Epoch: 82, Train Loss: 0.163505, Train Error: 0.137270, Test Error: 0.133282\n",
      "Epoch: 83, Train Loss: 0.163571, Train Error: 0.136941, Test Error: 0.133600\n",
      "Epoch: 84, Train Loss: 0.163397, Train Error: 0.137096, Test Error: 0.133601\n",
      "Epoch: 85, Train Loss: 0.163421, Train Error: 0.137010, Test Error: 0.133600\n",
      "Epoch: 86, Train Loss: 0.163286, Train Error: 0.136988, Test Error: 0.133659\n",
      "Epoch: 87, Train Loss: 0.162844, Train Error: 0.136645, Test Error: 0.133575\n",
      "Epoch: 88, Train Loss: 0.162723, Train Error: 0.136618, Test Error: 0.133353\n",
      "Epoch: 89, Train Loss: 0.163003, Train Error: 0.136760, Test Error: 0.133484\n",
      "Epoch: 90, Train Loss: 0.163129, Train Error: 0.136875, Test Error: 0.133257\n",
      "Epoch: 91, Train Loss: 0.162439, Train Error: 0.136268, Test Error: 0.132828\n",
      "Epoch: 92, Train Loss: 0.162642, Train Error: 0.136200, Test Error: 0.132882\n",
      "Epoch: 93, Train Loss: 0.162478, Train Error: 0.136296, Test Error: 0.133450\n",
      "Epoch: 94, Train Loss: 0.162525, Train Error: 0.136091, Test Error: 0.133673\n",
      "Epoch: 95, Train Loss: 0.162726, Train Error: 0.136296, Test Error: 0.133428\n",
      "Epoch: 96, Train Loss: 0.162565, Train Error: 0.136186, Test Error: 0.133370\n",
      "Epoch: 97, Train Loss: 0.162998, Train Error: 0.136480, Test Error: 0.132960\n",
      "Epoch: 98, Train Loss: 0.161452, Train Error: 0.135351, Test Error: 0.132897\n",
      "Epoch: 99, Train Loss: 0.162605, Train Error: 0.136156, Test Error: 0.132805\n",
      "Epoch: 100, Train Loss: 0.161842, Train Error: 0.135646, Test Error: 0.132860\n",
      "Epoch: 101, Train Loss: 0.162166, Train Error: 0.135930, Test Error: 0.133580\n",
      "Epoch: 102, Train Loss: 0.162533, Train Error: 0.136157, Test Error: 0.133554\n",
      "Epoch: 103, Train Loss: 0.162232, Train Error: 0.135948, Test Error: 0.133197\n",
      "Epoch: 104, Train Loss: 0.162558, Train Error: 0.135954, Test Error: 0.133439\n",
      "Epoch: 105, Train Loss: 0.162047, Train Error: 0.135738, Test Error: 0.133446\n",
      "Epoch: 106, Train Loss: 0.162411, Train Error: 0.136075, Test Error: 0.133216\n",
      "Epoch: 107, Train Loss: 0.161820, Train Error: 0.135793, Test Error: 0.133028\n",
      "Epoch: 108, Train Loss: 0.161236, Train Error: 0.135369, Test Error: 0.132968\n",
      "Epoch: 109, Train Loss: 0.162112, Train Error: 0.135789, Test Error: 0.133433\n",
      "Val Error: 0.132805\n",
      "Val MAE: 0.1328050544751542\n",
      "----- Fold 2 -----\n",
      "0.5 0.03 0.1\n",
      "Epoch: 1, Train Loss: 0.305101, Train Error: 0.247789, Test Error: 0.229133\n",
      "Epoch: 2, Train Loss: 0.244798, Train Error: 0.191850, Test Error: 0.178331\n",
      "Epoch: 3, Train Loss: 0.221225, Train Error: 0.173545, Test Error: 0.176318\n",
      "Epoch: 4, Train Loss: 0.216023, Train Error: 0.172046, Test Error: 0.174960\n",
      "Epoch: 5, Train Loss: 0.212062, Train Error: 0.171132, Test Error: 0.174197\n",
      "Epoch: 6, Train Loss: 0.209345, Train Error: 0.170723, Test Error: 0.173375\n",
      "Epoch: 7, Train Loss: 0.205851, Train Error: 0.169587, Test Error: 0.172819\n",
      "Epoch: 8, Train Loss: 0.203573, Train Error: 0.168658, Test Error: 0.171751\n",
      "Epoch: 9, Train Loss: 0.201640, Train Error: 0.167724, Test Error: 0.170665\n",
      "Epoch: 10, Train Loss: 0.198846, Train Error: 0.166356, Test Error: 0.169420\n",
      "Epoch: 11, Train Loss: 0.196907, Train Error: 0.164895, Test Error: 0.168024\n",
      "Epoch: 12, Train Loss: 0.195021, Train Error: 0.163356, Test Error: 0.166211\n",
      "Epoch: 13, Train Loss: 0.192564, Train Error: 0.161198, Test Error: 0.164714\n",
      "Epoch: 14, Train Loss: 0.190507, Train Error: 0.159432, Test Error: 0.163443\n",
      "Epoch: 15, Train Loss: 0.188937, Train Error: 0.158339, Test Error: 0.161891\n",
      "Epoch: 16, Train Loss: 0.186092, Train Error: 0.156074, Test Error: 0.160724\n",
      "Epoch: 17, Train Loss: 0.185124, Train Error: 0.155006, Test Error: 0.159639\n",
      "Epoch: 18, Train Loss: 0.182910, Train Error: 0.153721, Test Error: 0.158403\n",
      "Epoch: 19, Train Loss: 0.181540, Train Error: 0.152467, Test Error: 0.157474\n",
      "Epoch: 20, Train Loss: 0.180196, Train Error: 0.150964, Test Error: 0.156454\n",
      "Epoch: 21, Train Loss: 0.178825, Train Error: 0.150249, Test Error: 0.155748\n",
      "Epoch: 22, Train Loss: 0.178289, Train Error: 0.149286, Test Error: 0.154718\n",
      "Epoch: 23, Train Loss: 0.176471, Train Error: 0.148090, Test Error: 0.153832\n",
      "Epoch: 24, Train Loss: 0.175700, Train Error: 0.147217, Test Error: 0.152915\n",
      "Epoch: 25, Train Loss: 0.173138, Train Error: 0.145543, Test Error: 0.152449\n",
      "Epoch: 26, Train Loss: 0.172756, Train Error: 0.144689, Test Error: 0.151893\n",
      "Epoch: 27, Train Loss: 0.172439, Train Error: 0.144320, Test Error: 0.151541\n",
      "Epoch: 28, Train Loss: 0.171508, Train Error: 0.143654, Test Error: 0.150499\n",
      "Epoch: 29, Train Loss: 0.170306, Train Error: 0.143087, Test Error: 0.149883\n",
      "Epoch: 30, Train Loss: 0.170286, Train Error: 0.142507, Test Error: 0.149489\n",
      "Epoch: 31, Train Loss: 0.169221, Train Error: 0.141818, Test Error: 0.149195\n",
      "Epoch: 32, Train Loss: 0.169084, Train Error: 0.141607, Test Error: 0.148594\n",
      "Epoch: 33, Train Loss: 0.168501, Train Error: 0.141073, Test Error: 0.148383\n",
      "Epoch: 34, Train Loss: 0.167486, Train Error: 0.140358, Test Error: 0.148149\n",
      "Epoch: 35, Train Loss: 0.167649, Train Error: 0.140126, Test Error: 0.147794\n",
      "Epoch: 36, Train Loss: 0.167077, Train Error: 0.139684, Test Error: 0.147668\n",
      "Epoch: 37, Train Loss: 0.166645, Train Error: 0.139372, Test Error: 0.147286\n",
      "Epoch: 38, Train Loss: 0.166493, Train Error: 0.139371, Test Error: 0.147068\n",
      "Epoch: 39, Train Loss: 0.166154, Train Error: 0.138819, Test Error: 0.146912\n",
      "Epoch: 40, Train Loss: 0.165182, Train Error: 0.138463, Test Error: 0.146591\n",
      "Epoch: 41, Train Loss: 0.165098, Train Error: 0.138456, Test Error: 0.146677\n",
      "Epoch: 42, Train Loss: 0.164859, Train Error: 0.138173, Test Error: 0.146443\n",
      "Epoch: 43, Train Loss: 0.164459, Train Error: 0.137746, Test Error: 0.145953\n",
      "Epoch: 44, Train Loss: 0.164058, Train Error: 0.137605, Test Error: 0.146134\n",
      "Epoch: 45, Train Loss: 0.164020, Train Error: 0.137315, Test Error: 0.145814\n",
      "Epoch: 46, Train Loss: 0.163627, Train Error: 0.136931, Test Error: 0.146062\n"
     ]
    }
   ],
   "source": [
    "cv = KFold(n_splits=args.splits, random_state=random_seed, shuffle=True)\n",
    "\n",
    "ks = [0.9, 0.7, 0.6, 0.5]\n",
    "\n",
    "best_model_fold_list = []\n",
    "data_fold_list = []\n",
    "i = 1\n",
    "for train_index, test_index in cv.split(X):\n",
    "\n",
    "    print(f\"----- Fold {i} -----\")\n",
    "\n",
    "\n",
    "    netG = GSRNet(ks, args).to(device)\n",
    "    optimizerG = optim.Adam(netG.parameters(), lr=args.lr)\n",
    "\n",
    "    netD = Discriminator(args).to(device)\n",
    "    optimizerD = optim.Adam(netD.parameters(), lr=args.lr)\n",
    "\n",
    "    subjects_adj, test_adj, subjects_ground_truth, test_ground_truth = X[\n",
    "        train_index], X[test_index], Y[train_index], Y[test_index]\n",
    "    data_fold_list.append((subjects_adj, test_adj, subjects_ground_truth, test_ground_truth))\n",
    "\n",
    "\n",
    "#GAN model\n",
    "    # return_model = train_gan(\n",
    "    #     netG, \n",
    "    #     optimizerG, \n",
    "    #     netD,\n",
    "    #     optimizerD,\n",
    "    #     subjects_adj, \n",
    "    #     subjects_ground_truth, \n",
    "    #     args, \n",
    "    #     test_adj=test_adj, \n",
    "    #     test_ground_truth=test_ground_truth\n",
    "    # )\n",
    "\n",
    "#Non-GAN model\n",
    "    return_model = train(netG, optimizerG, subjects_adj, subjects_ground_truth, args, test_adj, test_ground_truth)\n",
    "\n",
    "\n",
    "    test_mae = test(return_model, test_adj, test_ground_truth, args)\n",
    "    print(f\"Val MAE: {test_mae}\")\n",
    "    best_model_fold_list.append(return_model)\n",
    "\n",
    "    i += 1\n",
    "\n",
    "    # break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MatrixVectorizer import MatrixVectorizer\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "import torch\n",
    "import networkx as nx\n",
    "\n",
    "def evaluate(pred_matrices, gt_matrices):\n",
    "\n",
    "    num_test_samples = gt_matrices.shape[0]\n",
    "\n",
    "    # Initialize lists to store MAEs for each centrality measure\n",
    "    mae_bc = []\n",
    "    mae_ec = []\n",
    "    mae_pc = []\n",
    "\n",
    "    pred_1d = []\n",
    "    gt_1d = []\n",
    "    for i in range(num_test_samples):\n",
    "        pred_1d.append(MatrixVectorizer.vectorize(pred_matrices[i]))\n",
    "        gt_1d.append(MatrixVectorizer.vectorize(gt_1d[i]))\n",
    "\n",
    "\n",
    "    # # Iterate over each test sample\n",
    "    # for i in range(num_test_samples):\n",
    "    #     # Convert adjacency matrices to NetworkX graphs\n",
    "    #     pred_graph = nx.from_numpy_array(pred_matrices[i], edge_attr=\"weight\")\n",
    "    #     gt_graph = nx.from_numpy_array(gt_matrices[i], edge_attr=\"weight\")\n",
    "\n",
    "    #     # Compute centrality measures\n",
    "    #     pred_bc = nx.betweenness_centrality(pred_graph, weight=\"weight\")\n",
    "    #     pred_ec = nx.eigenvector_centrality(pred_graph, weight=\"weight\")\n",
    "    #     pred_pc = nx.pagerank(pred_graph, weight=\"weight\")\n",
    "\n",
    "    #     gt_bc = nx.betweenness_centrality(gt_graph, weight=\"weight\")\n",
    "    #     gt_ec = nx.eigenvector_centrality(gt_graph, weight=\"weight\")\n",
    "    #     gt_pc = nx.pagerank(gt_graph, weight=\"weight\")\n",
    "\n",
    "    #     # Convert centrality dictionaries to lists\n",
    "    #     pred_bc_values = list(pred_bc.values())\n",
    "    #     pred_ec_values = list(pred_ec.values())\n",
    "    #     pred_pc_values = list(pred_pc.values())\n",
    "\n",
    "    #     gt_bc_values = list(gt_bc.values())\n",
    "    #     gt_ec_values = list(gt_ec.values())\n",
    "    #     gt_pc_values = list(gt_pc.values())\n",
    "\n",
    "    #     # Compute MAEs\n",
    "    #     mae_bc.append(mean_absolute_error(pred_bc_values, gt_bc_values))\n",
    "    #     mae_ec.append(mean_absolute_error(pred_ec_values, gt_ec_values))\n",
    "    #     mae_pc.append(mean_absolute_error(pred_pc_values, gt_pc_values))\n",
    "\n",
    "    # # Compute average MAEs\n",
    "    # avg_mae_bc = sum(mae_bc) / len(mae_bc)\n",
    "    # avg_mae_ec = sum(mae_ec) / len(mae_ec)\n",
    "    # avg_mae_pc = sum(mae_pc) / len(mae_pc)\n",
    "\n",
    "    # vectorize and flatten\n",
    "    pred_1d = np.concatenate(pred_1d, axis=0).flatten()\n",
    "    gt_1d = np.concatenate(gt_1d, axis=0).flatten()\n",
    "\n",
    "    mae = mean_absolute_error(pred_1d, gt_1d)\n",
    "    pcc = pearsonr(pred_1d, gt_1d)[0]\n",
    "    js_dis = jensenshannon(pred_1d, gt_1d)\n",
    "\n",
    "    print(\"MAE: \", mae)\n",
    "    print(\"PCC: \", pcc)\n",
    "    print(\"Jensen-Shannon Distance: \", js_dis)\n",
    "    # print(\"Average MAE betweenness centrality:\", avg_mae_bc)\n",
    "    # print(\"Average MAE eigenvector centrality:\", avg_mae_ec)\n",
    "    # print(\"Average MAE PageRank centrality:\", avg_mae_pc)\n",
    "    # return mae, pcc, js_dis, avg_mae_bc, avg_mae_ec, avg_mae_pc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(args.splits):\n",
    "#     _, test_adjs, _, gt_matrices = data_fold_list[i]\n",
    "#     model = best_model_fold_list[i]\n",
    "#     model.eval()\n",
    "#     pred_matrices = np.zeros(gt_matrices.shape)\n",
    "#     with torch.no_grad():\n",
    "#         for j, test_adj in enumerate(test_adjs):\n",
    "#             pred_matrices[j] = model(torch.from_numpy(test_adj))[0].cpu()\n",
    "#     evaluate(pred_matrices, gt_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_HR_train = pd.read_csv(\"../data/hr_train.csv\")\n",
    "\n",
    "pca = PCA(n_components=0.99, whiten=False)\n",
    "A_HR_train_pca = pca.fit_transform(A_HR_train)\n",
    "print(A_HR_train_pca.shape)\n",
    "\n",
    "gm = GaussianMixture(n_components=5, random_state=random_seed)\n",
    "A_HR_train_label = gm.fit_predict(A_HR_train_pca)\n",
    "unique, counts = np.unique(A_HR_train_label, return_counts=True)\n",
    "print(np.asarray((unique, counts)).T)\n",
    "\n",
    "X = np.load('A_LR_train_matrix.npy')\n",
    "y = np.load('A_HR_train_matrix.npy')\n",
    "\n",
    "n_sample = X.shape[0]\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X.reshape(n_sample, -1), \n",
    "    y.reshape(n_sample, -1), \n",
    "    test_size=0.20, \n",
    "    random_state=random_seed,\n",
    "    stratify=A_HR_train_label\n",
    ")\n",
    "\n",
    "X_train = X_train.reshape(-1, LR_size, LR_size)\n",
    "X_val = X_val.reshape(-1, LR_size, LR_size)\n",
    "y_train = y_train.reshape(-1, HR_size, HR_size)\n",
    "y_val = y_val.reshape(-1, HR_size, HR_size)\n",
    "\n",
    "print(\"Train size:\", len(X_train))\n",
    "print(\"Val size:\", len(X_val))\n",
    "\n",
    "netG = GSRNet(ks, args).to(device)\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=args.lr)\n",
    "args.early_stop_patient = 20\n",
    "# print(args.early_stop_patient)\n",
    "final_model = train(netG, optimizerG, X_train, y_train, args, X_val, y_val)\n",
    "\n",
    "# final_model = best_model_fold_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_model = best_model_fold_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(epochs=200, lr=0.0001, splits=3, lmbda=16, lr_dim=160, hr_dim=268, hidden_dim=268, padding=26, embedding_size=32, early_stop_patient=20)\n"
     ]
    }
   ],
   "source": [
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "MAE:  0.13436881332422806\n",
      "PCC:  0.6034752685336133\n",
      "Jensen-Shannon Distance:  0.29682179478612897\n",
      "Val\n",
      "MAE:  0.13112623280920516\n",
      "PCC:  0.6398439488024216\n",
      "Jensen-Shannon Distance:  0.27799914711990203\n"
     ]
    }
   ],
   "source": [
    "final_model.eval()\n",
    "pred_train_matrices = np.zeros(y_train.shape)\n",
    "pred_val_matrices = np.zeros(y_val.shape)\n",
    "with torch.no_grad():\n",
    "    for j, test_adj in enumerate(X_train):\n",
    "        pred_train_matrices[j] = final_model(torch.from_numpy(test_adj))[0].cpu()\n",
    "\n",
    "    print(\"Train\")\n",
    "    evaluate(pred_train_matrices, y_train)\n",
    "\n",
    "    for j, test_adj in enumerate(X_val):\n",
    "        pred_val_matrices[j] = final_model(torch.from_numpy(test_adj))[0].cpu()\n",
    "\n",
    "    print(\"Val\")\n",
    "    evaluate(pred_val_matrices, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_pred_list = []\n",
    "final_model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(A_LR_test_matrix.shape[0]):\n",
    "        output_pred = final_model(torch.Tensor(A_LR_test_matrix[i]))[0].cpu()\n",
    "        output_pred = MatrixVectorizer.vectorize(output_pred).tolist()\n",
    "        output_pred_list.append(output_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_pred_stack = np.stack(output_pred_list, axis=0)\n",
    "output_pred_1d = output_pred_stack.flatten()\n",
    "assert output_pred_1d.shape == (4007136, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.596928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.625193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.690432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.682919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.664565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4007131</th>\n",
       "      <td>4007132</td>\n",
       "      <td>0.051685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4007132</th>\n",
       "      <td>4007133</td>\n",
       "      <td>0.010242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4007133</th>\n",
       "      <td>4007134</td>\n",
       "      <td>0.309375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4007134</th>\n",
       "      <td>4007135</td>\n",
       "      <td>0.285448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4007135</th>\n",
       "      <td>4007136</td>\n",
       "      <td>0.314852</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4007136 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID  Predicted\n",
       "0              1   0.596928\n",
       "1              2   0.625193\n",
       "2              3   0.690432\n",
       "3              4   0.682919\n",
       "4              5   0.664565\n",
       "...          ...        ...\n",
       "4007131  4007132   0.051685\n",
       "4007132  4007133   0.010242\n",
       "4007133  4007134   0.309375\n",
       "4007134  4007135   0.285448\n",
       "4007135  4007136   0.314852\n",
       "\n",
       "[4007136 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"ID\": [i+1 for i in range(len(output_pred_1d))],\n",
    "    \"Predicted\": output_pred_1d.tolist()\n",
    "})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"gsr_gat_residual_node_edge_drop_50perecentdrop_fold0.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
