{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "\n",
    "import pandas as pd\n",
    "from MatrixVectorizer import *\n",
    "import torch\n",
    "import random\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import seaborn as sns\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Using GPU.\n"
     ]
    }
   ],
   "source": [
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "# Check for CUDA (GPU support) and set device accordingly\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)  # For multi-GPU setups\n",
    "    # Additional settings for ensuring reproducibility on CUDA\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_size = 160\n",
    "HR_size = 268"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def antivectorize_df(adj_mtx_df, size):\n",
    "    \n",
    "    num_subject = adj_mtx_df.shape[0]\n",
    "    adj_mtx = np.zeros((num_subject, size, size)) #torch.zeros((num_subject, LR_size, LR_size))\n",
    "    for i in range(num_subject):\n",
    "        adj_mtx[i] = MatrixVectorizer.anti_vectorize(adj_mtx_df.iloc[i], size) # torch.from_numpy(MatrixVectorizer.anti_vectorize(A_LR_train.iloc[i], LR_size))\n",
    "    return adj_mtx\n",
    "\n",
    "\n",
    "# A_LR_train = pd.read_csv(\"../data/lr_train.csv\")\n",
    "# A_HR_train = pd.read_csv(\"../data/hr_train.csv\")\n",
    "# A_LR_test = pd.read_csv(\"../data/lr_test.csv\")\n",
    "\n",
    "# np.save('A_LR_train_matrix.npy', antivectorize_df(A_LR_train, LR_size))\n",
    "# np.save('A_HR_train_matrix.npy', antivectorize_df(A_HR_train, HR_size))\n",
    "# np.save('A_LR_test_matrix.npy', antivectorize_df(A_LR_test, LR_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(167, 160, 160)\n",
      "(167, 268, 268)\n",
      "(112, 160, 160)\n"
     ]
    }
   ],
   "source": [
    "A_LR_train_matrix = np.load('A_LR_train_matrix.npy')\n",
    "A_HR_train_matrix = np.load('A_HR_train_matrix.npy')\n",
    "A_LR_test_matrix = np.load(\"A_LR_test_matrix.npy\")\n",
    "\n",
    "print(A_LR_train_matrix.shape)\n",
    "print(A_HR_train_matrix.shape)\n",
    "print(A_LR_test_matrix.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(epochs=200, lr=0.0001, splits=3, lmbda=20, lr_dim=160, hr_dim=268, hidden_dim=280, hidden_gat_dim=268, padding=26, embedding_size=32, early_stop_patient=5, dropout_rate=0.2, p_perturbe=0.5, p_drop_node=0.03, p_drop_edges=0.1, mean_dense=0.0, std_dense=0.01, mean_gaussian=0.0, std_gaussian=0.1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import KFold\n",
    "from preprocessing import *\n",
    "from model import *\n",
    "from train import *\n",
    "import argparse\n",
    "\n",
    "\n",
    "\n",
    "epochs = 200\n",
    "lmbda = 20 # 16\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='GSR-Net')\n",
    "parser.add_argument('--epochs', type=int, default=epochs, metavar='no_epochs',\n",
    "                help='number of episode to train ')\n",
    "parser.add_argument('--lr', type=float, default=0.0001, metavar='lr',\n",
    "                help='learning rate (default: 0.0001 using Adam Optimizer)')\n",
    "parser.add_argument('--splits', type=int, default=3, metavar='n_splits',\n",
    "                help='no of cross validation folds')\n",
    "parser.add_argument('--lmbda', type=int, default=lmbda, metavar='L',\n",
    "                help='self-reconstruction error hyperparameter')\n",
    "parser.add_argument('--lr_dim', type=int, default=LR_size, metavar='N',\n",
    "                help='adjacency matrix input dimensions')\n",
    "parser.add_argument('--hr_dim', type=int, default=HR_size, metavar='N',\n",
    "                help='super-resolved adjacency matrix output dimensions')\n",
    "parser.add_argument('--hidden_dim', type=int, default=280, metavar='N',\n",
    "                help='hidden GraphConvolutional layer dimensions')\n",
    "parser.add_argument('--hidden_gat_dim', type=int, default=268, metavar='hidden_gat_dim',\n",
    "                help='hidden GAT layer dimensions')\n",
    "parser.add_argument('--padding', type=int, default=26, metavar='padding',\n",
    "                help='dimensions of padding')\n",
    "# parser.add_argument('--padding', type=int, default=26, metavar='padding',\n",
    "#                 help='dimensions of padding')\n",
    "parser.add_argument('--embedding_size', type=int, default=32, metavar='embedding_size',\n",
    "                help='node embedding size')\n",
    "parser.add_argument('--early_stop_patient', type=int, default=5, metavar='early_stop_patient',\n",
    "                help='early_stop_patience')\n",
    "parser.add_argument('--dropout_rate', type=float, default=0.2, metavar='dropout_rate',\n",
    "                help='dropout_rate')\n",
    "parser.add_argument('--p_perturbe', type=float, default=0.5, metavar='p_perturbe',\n",
    "                help='p_perturbe')\n",
    "parser.add_argument('--p_drop_node', type=float, default=0.03, metavar='p_drop_node',\n",
    "                help='p_drop_node')\n",
    "parser.add_argument('--p_drop_edges', type=float, default=0.1, metavar='p_drop_edges',\n",
    "                help='p_drop_edges')\n",
    "\n",
    "parser.add_argument('--mean_dense', type=float, default=0., metavar='mean',\n",
    "                        help='mean of the normal distribution in Dense Layer')\n",
    "parser.add_argument('--std_dense', type=float, default=0.01, metavar='std',\n",
    "                    help='standard deviation of the normal distribution in Dense Layer')\n",
    "parser.add_argument('--mean_gaussian', type=float, default=0., metavar='mean',\n",
    "                    help='mean of the normal distribution in Gaussian Noise Layer')\n",
    "parser.add_argument('--std_gaussian', type=float, default=0.1, metavar='std',\n",
    "                    help='standard deviation of the normal distribution in Gaussian Noise Layer')\n",
    "\n",
    "# Create an empty Namespace to hold the default arguments\n",
    "args = parser.parse_args([]) \n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(167, 160, 160)\n",
      "(167, 268, 268)\n"
     ]
    }
   ],
   "source": [
    "# SIMULATING THE DATA: EDIT TO ENTER YOUR OWN DATA\n",
    "X = A_LR_train_matrix #np.random.normal(0, 0.5, (167, 160, 160))\n",
    "Y = A_HR_train_matrix #np.random.normal(0, 0.5, (167, 288, 288))\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = get_device()\n",
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_degree_matrix_normalization_batch_numpy(adjacency_batch):\n",
    "    \"\"\"\n",
    "    Optimizes the degree matrix normalization for a batch of adjacency matrices using NumPy.\n",
    "    Computes the normalized adjacency matrix D^-1 * A for each graph in the batch.\n",
    "    \n",
    "    Parameters:\n",
    "    - adjacency_batch: A NumPy array of shape (batch_size, num_nodes, num_nodes) representing\n",
    "                       a batch of adjacency matrices.\n",
    "\n",
    "    Returns:\n",
    "    - A NumPy array of normalized adjacency matrices.\n",
    "    \"\"\"\n",
    "    epsilon = 1e-6  # Small constant to avoid division by zero\n",
    "    # Calculate the degree for each node in the batch\n",
    "    adjacency_batch = adjacency_batch + 2*np.eye(adjacency_batch.shape[1])\n",
    "    d = adjacency_batch.sum(axis=2) + epsilon\n",
    "    \n",
    "    # Compute the inverse degree matrix D^-1 for the batch\n",
    "    D_inv = np.reciprocal(d)[:, :, np.newaxis] * np.eye(adjacency_batch.shape[1])[np.newaxis, :, :]\n",
    "    \n",
    "    # Normalize the adjacency matrix using batch matrix multiplication\n",
    "    normalized_adjacency_batch = np.matmul(D_inv, adjacency_batch)\n",
    "    \n",
    "    return normalized_adjacency_batch\n",
    "\n",
    "def symmetric_norm(A):\n",
    "    num_samples = A.shape[0]  # Number of samples, i.e., slices in the 3D tensor\n",
    "\n",
    "    # Initialize an empty array for the normalized matrices\n",
    "    A_normalized = np.zeros_like(A)\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        # Extract the i-th adjacency matrix\n",
    "        Ai = A[i, :, :]\n",
    "        \n",
    "        # Compute the degree matrix D and its inverse square root for Ai\n",
    "        Di = np.diag(np.sum(Ai, axis=1))\n",
    "        D_inv_sqrt_i = np.linalg.inv(np.sqrt(Di))\n",
    "        \n",
    "        # Normalize the adjacency matrix\n",
    "        A_normalized[i, :, :] = D_inv_sqrt_i @ Ai @ D_inv_sqrt_i\n",
    "    return A_normalized\n",
    "\n",
    "def compute_pagerank_normalization(adjacency_matrices):\n",
    "    \"\"\"\n",
    "    Normalizes each adjacency matrix in a batch using PageRank centrality values.\n",
    "    \n",
    "    Parameters:\n",
    "    - adjacency_matrices: A NumPy array with shape (n_samples, n_dim, n_dim) representing the adjacency matrices.\n",
    "    \n",
    "    Returns:\n",
    "    - A NumPy array of the same shape, with each adjacency matrix normalized.\n",
    "    \"\"\"\n",
    "    n_samples, n_dim, _ = adjacency_matrices.shape\n",
    "    normalized_matrices = np.zeros_like(adjacency_matrices)\n",
    "    \n",
    "    for i in tqdm(range(n_samples)):\n",
    "        adjacency = adjacency_matrices[i]\n",
    "        \n",
    "        # Convert adjacency matrix to graph\n",
    "        G = nx.from_numpy_array(adjacency, create_using=nx.DiGraph)\n",
    "        \n",
    "        # Compute PageRank\n",
    "        pr_dict = nx.pagerank(G, alpha=0.85, max_iter=100, tol=1e-6)\n",
    "        \n",
    "        # Create a diagonal matrix with PageRank values\n",
    "        pr_values = np.array([pr_dict[node] for node in range(n_dim)])\n",
    "        pr_diag = np.diag(pr_values)\n",
    "        \n",
    "        # Normalize the adjacency matrix using the PageRank diagonal matrix\n",
    "        normalized_adjacency = np.dot(pr_diag, adjacency)\n",
    "        \n",
    "        normalized_matrices[i] = normalized_adjacency\n",
    "    \n",
    "    return normalized_matrices\n",
    "    \n",
    "\n",
    "# X = compute_pagerank_normalization(X)\n",
    "# A_LR_test_matrix = compute_pagerank_normalization(A_LR_test_matrix)\n",
    "\n",
    "X = compute_degree_matrix_normalization_batch_numpy(X)\n",
    "A_LR_test_matrix = compute_degree_matrix_normalization_batch_numpy(A_LR_test_matrix)\n",
    "\n",
    "# X = symmetric_norm(X)\n",
    "# A_LR_test_matrix = symmetric_norm(A_LR_test_matrix)\n",
    "\n",
    "# print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-06 18:07:21,068] A new study created in memory with name: GSR_hyperparameter_search\n",
      "Epoch Progress:   0%|                                                                                         | 0/200 [00:00<?, ?epoch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0: {'lr': 0.0001400864349906367, 'lmbda': 43, 'hidden_dim': 332, 'patience': 5, 'dropout_rate': 0.08709195440667815, 'p_perturbe': 0.7, 'p_drop_node': 0.0183937225954856, 'p_drop_edges': 0.09414930558623658}\n",
      "----- Fold 1 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress:   3%|▊                          | 6/200 [01:14<40:15, 12.45s/epoch, test_error=0.177, train_error=0.172, train_loss=4.46]\n",
      "[W 2024-03-06 18:08:35,846] Trial 0 failed with parameters: {'lr': 0.0001400864349906367, 'lmbda': 43, 'hidden_dim': 332, 'dropout_rate': 0.08709195440667815, 'p_perturbe': 0.7, 'p_drop_node': 0.0183937225954856, 'p_drop_edges': 0.09414930558623658} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/homes/ms922/.local/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_1891033/3573756406.py\", line 66, in objective\n",
      "    return_model = train_gan(modelG, optimizerG, modelD, optimizerD, subjects_adj, subjects_ground_truth, args, test_adj, test_ground_truth)\n",
      "  File \"/homes/ms922/graph_super_resolution/gsr_net/train.py\", line 280, in train_gan\n",
      "    _, U_hr = torch.linalg.eigh(padded_hr, UPLO='U')\n",
      "KeyboardInterrupt\n",
      "[W 2024-03-06 18:08:35,849] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 85\u001b[0m\n\u001b[1;32m     82\u001b[0m     study\u001b[38;5;241m.\u001b[39moptimize(objective, n_trials\u001b[38;5;241m=\u001b[39mn_trials)\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m study\u001b[38;5;241m.\u001b[39mbest_params\n\u001b[0;32m---> 85\u001b[0m best_params \u001b[38;5;241m=\u001b[39m \u001b[43mhyperparameter_search\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 82\u001b[0m, in \u001b[0;36mhyperparameter_search\u001b[0;34m(n_trials)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mean_mae\n\u001b[1;32m     81\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m, study_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGSR_hyperparameter_search\u001b[39m\u001b[38;5;124m'\u001b[39m, sampler\u001b[38;5;241m=\u001b[39moptuna\u001b[38;5;241m.\u001b[39msamplers\u001b[38;5;241m.\u001b[39mTPESampler())\n\u001b[0;32m---> 82\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m study\u001b[38;5;241m.\u001b[39mbest_params\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/optuna/study/study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/optuna/study/_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    250\u001b[0m ):\n\u001b[0;32m--> 251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/optuna/study/_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[13], line 66\u001b[0m, in \u001b[0;36mhyperparameter_search.<locals>.objective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     63\u001b[0m optimizerD \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(modelD\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mlr)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# return_model = train(modelG, optimizerG, subjects_adj, subjects_ground_truth, args, test_adj, test_ground_truth)\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m return_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_gan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodelG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizerG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodelD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizerD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubjects_adj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubjects_ground_truth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_adj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_ground_truth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m test_mae \u001b[38;5;241m=\u001b[39m test(return_model, test_adj, test_ground_truth, args)\n\u001b[1;32m     68\u001b[0m mae_list\u001b[38;5;241m.\u001b[39mappend(test_mae)\n",
      "File \u001b[0;32m~/graph_super_resolution/gsr_net/train.py:280\u001b[0m, in \u001b[0;36mtrain_gan\u001b[0;34m(netG, optimizerG, netD, optimizerD, subjects_adj, subjects_labels, args, test_adj, test_ground_truth)\u001b[0m\n\u001b[1;32m    278\u001b[0m padded_hr \u001b[38;5;241m=\u001b[39m pad_HR_adj(hr, args\u001b[38;5;241m.\u001b[39mpadding)\n\u001b[1;32m    279\u001b[0m padded_hr \u001b[38;5;241m=\u001b[39m padded_hr\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 280\u001b[0m _, U_hr \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meigh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpadded_hr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mUPLO\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mU\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[1;32m    281\u001b[0m U_hr \u001b[38;5;241m=\u001b[39m U_hr\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    284\u001b[0m mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones_like(model_outputs, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbool)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# X_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "best_mae = 1000\n",
    "best_params = None\n",
    "best_model = None\n",
    "\n",
    "import gc\n",
    "\n",
    "def hyperparameter_search(n_trials=40):\n",
    "    def objective(trial):\n",
    "        global best_mae\n",
    "        global best_params\n",
    "        global best_model\n",
    "        ks = [0.8, 0.5]\n",
    "        num_epochs = 200\n",
    "        lr = trial.suggest_float('lr', 1e-4, 5e-4, log=True)\n",
    "        lmbda = trial.suggest_int('lmbda', 8, 50)\n",
    "        # hidden_dim = trial.suggest_categorical('hidden_dim', [100, 200, 300, 400, 500])\n",
    "        hidden_dim = trial.suggest_int('hidden_dim', 268, 500, step=32)\n",
    "        # patience = trial.suggest_int('patience', 5, 20)\n",
    "        patience = 5\n",
    "        dropout_rate = trial.suggest_float('dropout_rate', 0.05, 0.3, log=True)\n",
    "        p_perturbe = trial.suggest_float('p_perturbe', 0.4, 0.7, step=0.1)\n",
    "        p_drop_node = trial.suggest_float('p_drop_node', 0.01, 0.15, log=True)\n",
    "        p_drop_edges = trial.suggest_float('p_drop_edges', 0.05, 0.1, log=True)\n",
    "\n",
    "\n",
    "        args.epochs = num_epochs\n",
    "        args.lr = lr\n",
    "        args.lmbda = lmbda\n",
    "        args.hidden_dim = hidden_dim\n",
    "        args.early_stop_patient = patience\n",
    "        args.dropout_rate = dropout_rate\n",
    "        args.p_perturbe = p_perturbe\n",
    "        args.p_drop_node = p_drop_node\n",
    "        args.p_drop_edges = p_drop_edges\n",
    "\n",
    "        params = {\n",
    "                \"lr\": lr,\n",
    "                \"lmbda\": lmbda,\n",
    "                \"hidden_dim\": hidden_dim,\n",
    "                \"patience\": patience,\n",
    "                \"dropout_rate\": dropout_rate,\n",
    "                \"p_perturbe\": p_perturbe,\n",
    "                \"p_drop_node\": p_drop_node,\n",
    "                \"p_drop_edges\": p_drop_edges,\n",
    "            }\n",
    "        print(f\"Trial {trial.number}: {params}\")\n",
    "\n",
    "        cv = KFold(n_splits=args.splits, random_state=random_seed, shuffle=True)\n",
    "\n",
    "        mae_list = []\n",
    "        i = 1\n",
    "        for train_index, test_index in cv.split(X):\n",
    "            print(f\"----- Fold {i} -----\")\n",
    "            subjects_adj, test_adj, subjects_ground_truth, test_ground_truth = X[\n",
    "                train_index], X[test_index], Y[train_index], Y[test_index]\n",
    "\n",
    "            modelG = GSRNet(ks, args).to(device)\n",
    "            optimizerG = torch.optim.Adam(modelG.parameters(), lr=args.lr)\n",
    "\n",
    "            modelD = Discriminator(args).to(device)\n",
    "            optimizerD = optim.Adam(modelD.parameters(), lr=args.lr)\n",
    "\n",
    "            # return_model = train(modelG, optimizerG, subjects_adj, subjects_ground_truth, args, test_adj, test_ground_truth)\n",
    "            return_model = train_gan(modelG, optimizerG, modelD, optimizerD, subjects_adj, subjects_ground_truth, args, test_adj, test_ground_truth)\n",
    "            test_mae = test(return_model, test_adj, test_ground_truth, args)\n",
    "            mae_list.append(test_mae)\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        mean_mae = np.mean(mae_list)\n",
    "        if mean_mae < best_mae:\n",
    "            best_mae = mean_mae\n",
    "            best_params = params\n",
    "            best_model = return_model\n",
    "            print(f\"New best MAE: {test_mae} with params: {best_params}\")\n",
    "\n",
    "        return mean_mae\n",
    "    \n",
    "    study = optuna.create_study(direction='minimize', study_name='GSR_hyperparameter_search', sampler=optuna.samplers.TPESampler())\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    return study.best_params\n",
    "\n",
    "best_params = hyperparameter_search(20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress:   0%|                                                                                         | 0/200 [00:00<?, ?epoch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Fold 1 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Progress:  10%|██▌                        | 19/200 [03:17<27:40,  9.17s/epoch, test_error=0.159, train_error=0.158, train_loss=8.8]"
     ]
    }
   ],
   "source": [
    "cv = KFold(n_splits=args.splits, random_state=random_seed, shuffle=True)\n",
    "\n",
    "ks = [0.8, 0.5]\n",
    "\n",
    "best_model_fold_list = []\n",
    "data_fold_list = []\n",
    "i = 1\n",
    "for train_index, test_index in cv.split(X):\n",
    "\n",
    "    print(f\"----- Fold {i} -----\")\n",
    "\n",
    "    subjects_adj, test_adj, subjects_ground_truth, test_ground_truth = X[\n",
    "        train_index], X[test_index], Y[train_index], Y[test_index]\n",
    "    data_fold_list.append((subjects_adj, test_adj, subjects_ground_truth, test_ground_truth))\n",
    "\n",
    "\n",
    "    netG = GSRNet(ks, args).to(device)\n",
    "    optimizerG = optim.Adam(netG.parameters(), lr=args.lr)\n",
    "\n",
    "    netD = Discriminator(args).to(device)\n",
    "    optimizerD = optim.Adam(netD.parameters(), lr=args.lr)\n",
    "\n",
    "    return_model = train_gan(\n",
    "        netG, \n",
    "        optimizerG, \n",
    "        netD,\n",
    "        optimizerD,\n",
    "        subjects_adj, \n",
    "        subjects_ground_truth, \n",
    "        args, \n",
    "        test_adj=test_adj, \n",
    "        test_ground_truth=test_ground_truth\n",
    "    )\n",
    "\n",
    "    # return_model = train(netG, optimizerG, subjects_adj, subjects_ground_truth, args, test_adj, test_ground_truth)\n",
    "    test_mae = test(return_model, test_adj, test_ground_truth, args)\n",
    "    print(f\"Val MAE: {test_mae}\")\n",
    "    best_model_fold_list.append(return_model)\n",
    "\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MatrixVectorizer import MatrixVectorizer\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "import torch\n",
    "import networkx as nx\n",
    "\n",
    "def evaluate(pred_matrices, gt_matrices, cal_graph=False):\n",
    "\n",
    "    # pred_matrices = pred_matrices.cpu().detach().numpy()\n",
    "    # gt_matrices = gt_matrices.cpu().detach().numpy()\n",
    "\n",
    "    num_test_samples = gt_matrices.shape[0]\n",
    "\n",
    "    # Initialize lists to store MAEs for each centrality measure\n",
    "    mae_bc = []\n",
    "    mae_ec = []\n",
    "    mae_pc = []\n",
    "    \n",
    "    pred_1d = []\n",
    "    gt_1d = []\n",
    "\n",
    "    # Iterate over each test sample\n",
    "    for i in tqdm(range(num_test_samples)):\n",
    "\n",
    "        pred_1d.append(MatrixVectorizer.vectorize(pred_matrices[i]))\n",
    "        gt_1d.append(MatrixVectorizer.vectorize(gt_matrices[i]))\n",
    "\n",
    "        if cal_graph:\n",
    "            # Convert adjacency matrices to NetworkX graphs\n",
    "            pred_graph = nx.from_numpy_array(pred_matrices[i])\n",
    "            gt_graph = nx.from_numpy_array(gt_matrices[i])\n",
    "\n",
    "            # Compute centrality measures\n",
    "            pred_bc = nx.betweenness_centrality(pred_graph, weight=\"weight\")\n",
    "            pred_ec = nx.eigenvector_centrality(pred_graph, weight=\"weight\")\n",
    "            pred_pc = nx.pagerank(pred_graph, weight=\"weight\")\n",
    "\n",
    "            gt_bc = nx.betweenness_centrality(gt_graph, weight=\"weight\")\n",
    "            gt_ec = nx.eigenvector_centrality(gt_graph, weight=\"weight\")\n",
    "            gt_pc = nx.pagerank(gt_graph, weight=\"weight\")\n",
    "\n",
    "            # Convert centrality dictionaries to lists\n",
    "            pred_bc_values = list(pred_bc.values())\n",
    "            pred_ec_values = list(pred_ec.values())\n",
    "            pred_pc_values = list(pred_pc.values())\n",
    "\n",
    "            gt_bc_values = list(gt_bc.values())\n",
    "            gt_ec_values = list(gt_ec.values())\n",
    "            gt_pc_values = list(gt_pc.values())\n",
    "\n",
    "            # Compute MAEs\n",
    "            mae_bc.append(mean_absolute_error(pred_bc_values, gt_bc_values))\n",
    "            mae_ec.append(mean_absolute_error(pred_ec_values, gt_ec_values))\n",
    "            mae_pc.append(mean_absolute_error(pred_pc_values, gt_pc_values))\n",
    "\n",
    "    if cal_graph:\n",
    "        # Compute average MAEs\n",
    "        avg_mae_bc = sum(mae_bc) / len(mae_bc)\n",
    "        avg_mae_ec = sum(mae_ec) / len(mae_ec)\n",
    "        avg_mae_pc = sum(mae_pc) / len(mae_pc)\n",
    "\n",
    "    # vectorize and flatten\n",
    "    pred_1d = np.concatenate(pred_1d, axis=0).flatten()\n",
    "    gt_1d = np.concatenate(gt_1d, axis=0).flatten()\n",
    "\n",
    "    mae = mean_absolute_error(pred_1d, gt_1d)\n",
    "    pcc = pearsonr(pred_1d, gt_1d)[0]\n",
    "    js_dis = jensenshannon(pred_1d, gt_1d)\n",
    "\n",
    "    print(\"MAE: \", mae)\n",
    "    print(\"PCC: \", pcc)\n",
    "    print(\"Jensen-Shannon Distance: \", js_dis)\n",
    "    if cal_graph:\n",
    "        print(\"Average MAE betweenness centrality:\", avg_mae_bc)\n",
    "        print(\"Average MAE eigenvector centrality:\", avg_mae_ec)\n",
    "        print(\"Average MAE PageRank centrality:\", avg_mae_pc)\n",
    "\n",
    "    if cal_graph:\n",
    "\n",
    "        res = {\n",
    "            \"MAE\": mae,\n",
    "            \"PCC\": pcc,\n",
    "            \"JSD\": js_dis,\n",
    "            \"MAE_(BC)\": avg_mae_bc,\n",
    "            \"MAE_(EC)\": avg_mae_ec,\n",
    "            \"MAE_(PC)\": avg_mae_pc\n",
    "        }\n",
    "    else:\n",
    "        res = {\n",
    "            \"MAE\": mae,\n",
    "            \"PCC\": pcc,\n",
    "            \"JSD\": js_dis,\n",
    "            # \"MAE_(BC)\": avg_mae_bc,\n",
    "            # \"MAE_(EC)\": avg_mae_ec,\n",
    "            # \"MAE_(PC)\": avg_mae_pc\n",
    "        }\n",
    "\n",
    "    return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_list = []\n",
    "\n",
    "for i in range(args.splits):\n",
    "    _, test_adjs, _, gt_matrices = data_fold_list[i]\n",
    "    model = best_model_fold_list[i]\n",
    "    model.eval()\n",
    "    pred_matrices = np.zeros(gt_matrices.shape)\n",
    "    with torch.no_grad():\n",
    "        for j, test_adj in enumerate(test_adjs):\n",
    "            pred_matrices[j], _, _, _ = model(torch.from_numpy(test_adj))\n",
    "    res_list.append(evaluate(pred_matrices, gt_matrices, cal_graph=False))\n",
    "\n",
    "pd.DataFrame(res_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res_list = []\n",
    "\n",
    "# for i in range(args.splits):\n",
    "#     _, test_adjs, _, gt_matrices = data_fold_list[i]\n",
    "#     model = best_model_fold_list[i]\n",
    "#     model.eval()\n",
    "#     pred_matrices = np.zeros(gt_matrices.shape)\n",
    "#     with torch.no_grad():\n",
    "#         for j, test_adj in enumerate(test_adjs):\n",
    "#             pred_matrices[j], _, _, _ = model(torch.from_numpy(test_adj))\n",
    "#     res_list.append(evaluate(pred_matrices, gt_matrices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics_fold(res_list):\n",
    "    df = pd.DataFrame(res_list)\n",
    "    df = df.rename(columns={\"mae\": \"MAE\", \"pcc\": \"PCC\", \"js_dis\": \"JSD\", \"avg_mae_bc\": \"MAE_(BC)\", \"avg_mae_ec\": \"MAE_(EC)\", \"avg_mae_pc\": \"MAE_(PC)\"})\n",
    "    df.index = df.index.set_names(['Fold'])\n",
    "    df.loc['mean'] = df.mean()\n",
    "    avg_data = df.iloc[-1, :]\n",
    "    df.loc['std'] = df.std()\n",
    "    errors = df.iloc[-1, :].tolist()\n",
    "    df = df.reset_index()\n",
    "    df = df.iloc[:-2, :]\n",
    "    df_long = df.melt(id_vars='Fold', var_name='Metric', value_name='Value')\n",
    "    palette = sns.color_palette(\"muted\", n_colors=len(df_long['Metric'].unique()))\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "    for fold in range(3):   \n",
    "        i = fold // 2\n",
    "        j = fold % 2\n",
    "        sns.barplot(x='Metric', y='Value', data=df_long[df_long['Fold'] == fold], ax=axs[i, j], palette=palette)\n",
    "        axs[i, j].set_title(f\"Fold: {fold+1}\")\n",
    "\n",
    "    sns.barplot(x=avg_data.index, y=avg_data.values, ax=axs[1, 1], palette=palette, yerr=errors, capsize=5)\n",
    "    axs[1, 1].set_title(\"Avg. Across Folds\")\n",
    "    plt.show()\n",
    "\n",
    "plot_metrics_fold(res_list)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_HR_train = pd.read_csv(\"../data/hr_train.csv\")\n",
    "\n",
    "pca = PCA(n_components=0.99, whiten=False)\n",
    "A_HR_train_pca = pca.fit_transform(A_HR_train)\n",
    "print(A_HR_train_pca.shape)\n",
    "\n",
    "gm = GaussianMixture(n_components=5, random_state=random_seed)\n",
    "A_HR_train_label = gm.fit_predict(A_HR_train_pca)\n",
    "unique, counts = np.unique(A_HR_train_label, return_counts=True)\n",
    "print(np.asarray((unique, counts)).T)\n",
    "\n",
    "X = np.load('A_LR_train_matrix.npy')\n",
    "y = np.load('A_HR_train_matrix.npy')\n",
    "\n",
    "n_sample = X.shape[0]\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X.reshape(n_sample, -1), \n",
    "    y.reshape(n_sample, -1), \n",
    "    test_size=0.20, \n",
    "    random_state=random_seed,\n",
    "    stratify=A_HR_train_label\n",
    ")\n",
    "\n",
    "X_train = X_train.reshape(-1, LR_size, LR_size)\n",
    "X_val = X_val.reshape(-1, LR_size, LR_size)\n",
    "y_train = y_train.reshape(-1, HR_size, HR_size)\n",
    "y_val = y_val.reshape(-1, HR_size, HR_size)\n",
    "\n",
    "print(\"Train size:\", len(X_train))\n",
    "print(\"Val size:\", len(X_val))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netG = GSRNet(ks, args).to(device)\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=args.lr)\n",
    "\n",
    "netD = Discriminator(args).to(device)\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=args.lr)\n",
    "\n",
    "final_model = train_gan(\n",
    "    netG, \n",
    "    optimizerG, \n",
    "    netD,\n",
    "    optimizerD,\n",
    "    X_train, \n",
    "    y_train, \n",
    "    args, \n",
    "    test_adj=X_val, \n",
    "    test_ground_truth=y_val\n",
    ")\n",
    "# final_model = train(netG, optimizerG, X_train, y_train, args, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.eval()\n",
    "pred_train_matrices = np.zeros(y_train.shape)\n",
    "pred_val_matrices = np.zeros(y_val.shape)\n",
    "with torch.no_grad():\n",
    "    for j, test_adj in enumerate(X_train):\n",
    "        pred_train_matrices[j], _, _, _ = final_model(torch.from_numpy(test_adj))\n",
    "\n",
    "    print(\"Train\")\n",
    "    evaluate(pred_train_matrices, y_train)\n",
    "\n",
    "    for j, test_adj in enumerate(X_val):\n",
    "        pred_val_matrices[j], _, _, _ = final_model(torch.from_numpy(test_adj))\n",
    "\n",
    "    print(\"Val\")\n",
    "    evaluate(pred_val_matrices, y_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_pred_list = []\n",
    "final_model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(A_LR_test_matrix.shape[0]):\n",
    "        output_pred, _, _, _ = final_model(torch.Tensor(A_LR_test_matrix[i]))\n",
    "        output_pred = MatrixVectorizer.vectorize(output_pred).tolist()\n",
    "        output_pred_list.append(output_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_pred_stack = np.stack(output_pred_list, axis=0)\n",
    "output_pred_1d = output_pred_stack.flatten()\n",
    "assert output_pred_1d.shape == (4007136, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"ID\": [i+1 for i in range(len(output_pred_1d))],\n",
    "    \"Predicted\": output_pred_1d.tolist()\n",
    "})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"gsr_gan_gat_relu.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
