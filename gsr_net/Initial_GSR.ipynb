{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from MatrixVectorizer import *\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A_LR_train = pd.read_csv(\"../data/lr_train.csv\")\n",
    "# A_HR_train = pd.read_csv(\"../data/hr_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_size = 160\n",
    "HR_size = 268"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(A_LR_train.shape)\n",
    "# A_LR_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "MatrixVectorizer = MatrixVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_subject = A_LR_train.shape[0]\n",
    "# A_LR_train_matrix = np.zeros((num_subject, LR_size, LR_size)) #torch.zeros((num_subject, LR_size, LR_size))\n",
    "# for i in range(num_subject):\n",
    "#     A_LR_train_matrix[i] = MatrixVectorizer.anti_vectorize(A_LR_train.iloc[i], LR_size) # torch.from_numpy(MatrixVectorizer.anti_vectorize(A_LR_train.iloc[i], LR_size))\n",
    "\n",
    "# A_HR_train_matrix = np.zeros((num_subject, HR_size, HR_size)) #torch.zeros((num_subject, LR_size, LR_size))\n",
    "# for i in range(num_subject):\n",
    "#     A_HR_train_matrix[i] = MatrixVectorizer.anti_vectorize(A_HR_train.iloc[i], HR_size) \n",
    "# np.save('A_LR_train_matrix.npy', A_LR_train_matrix)\n",
    "# np.save('A_HR_train_matrix.npy', A_HR_train_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_LR_train_matrix = np.load('A_LR_train_matrix.npy')\n",
    "A_HR_train_matrix = np.load('A_HR_train_matrix.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(167, 160, 160)\n",
      "(167, 268, 268)\n"
     ]
    }
   ],
   "source": [
    "#A_LR_train_matrix.size()\n",
    "print(A_LR_train_matrix.shape)\n",
    "print(A_HR_train_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(epochs=200, lr=0.0001, splits=3, lmbda=16, lr_dim=160, hr_dim=268, hidden_dim=280, padding=26)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Main function of Graph Super-Resolution Network (GSR-Net) framework \n",
    "   for predicting high-resolution brain connectomes from low-resolution connectomes. \n",
    "    \n",
    "    ---------------------------------------------------------------------\n",
    "    \n",
    "    This file contains the implementation of the training and testing process of our GSR-Net model.\n",
    "        train(model, optimizer, subjects_adj, subjects_ground_truth, args)\n",
    "\n",
    "                Inputs:\n",
    "                        model:        constructor of our GSR-Net model:  model = GSRNet(ks,args)\n",
    "                                      ks:   array that stores reduction rates of nodes in Graph U-Net pooling layers\n",
    "                                      args: parsed command line arguments\n",
    "\n",
    "                        optimizer:    constructor of our model's optimizer (borrowed from PyTorch)  \n",
    "\n",
    "                        subjects_adj: (n × l x l) tensor stacking LR connectivity matrices of all training subjects\n",
    "                                       n: the total number of subjects\n",
    "                                       l: the dimensions of the LR connectivity matrices\n",
    "\n",
    "                        subjects_ground_truth: (n × h x h) tensor stacking LR connectivity matrices of all training subjects\n",
    "                                                n: the total number of subjects\n",
    "                                                h: the dimensions of the LR connectivity matrices\n",
    "\n",
    "                        args:          parsed command line arguments, to learn more about the arguments run: \n",
    "                                       python demo.py --help\n",
    "                Output:\n",
    "                        for each epoch, prints out the mean training MSE error\n",
    "\n",
    "\n",
    "            \n",
    "        test(model, test_adj,test_ground_truth,args)\n",
    "\n",
    "                Inputs:\n",
    "                        test_adj:      (n × l x l) tensor stacking LR connectivity matrices of all testing subjects\n",
    "                                        n: the total number of subjects\n",
    "                                        l: the dimensions of the LR connectivity matrices\n",
    "\n",
    "                        test_ground_truth:      (n × h x h) tensor stacking LR connectivity matrices of all testing subjects\n",
    "                                                 n: the total number of subjects\n",
    "                                                 h: the dimensions of the LR connectivity matrices\n",
    "\n",
    "                        see train method above for model and args.\n",
    "\n",
    "                Outputs:\n",
    "                        for each epoch, prints out the mean testing MSE error\n",
    "\n",
    "\n",
    "    To evaluate our framework we used 5-fold cross-validation strategy.\n",
    "\n",
    "    ---------------------------------------------------------------------\n",
    "    Copyright 2020 Megi Isallari, Istanbul Technical University.\n",
    "    All rights reserved.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import KFold\n",
    "from preprocessing import *\n",
    "from model import *\n",
    "from train import *\n",
    "import argparse\n",
    "\n",
    "# args = {\n",
    "#     'epochs': 200, \n",
    "#     'lr': 0.0001, \n",
    "#     'splits': 3, \n",
    "#     'lmbda': 16, \n",
    "#     'lr_dim': LR_size, \n",
    "#     'hr_dim': HR_size, \n",
    "#     'hidden_dim': 320, \n",
    "#     'padding': 26\n",
    "# }\n",
    "\n",
    "parser = argparse.ArgumentParser(description='GSR-Net')\n",
    "parser.add_argument('--epochs', type=int, default=200, metavar='no_epochs',\n",
    "                help='number of episode to train ')\n",
    "parser.add_argument('--lr', type=float, default=0.0001, metavar='lr',\n",
    "                help='learning rate (default: 0.0001 using Adam Optimizer)')\n",
    "parser.add_argument('--splits', type=int, default=3, metavar='n_splits',\n",
    "                help='no of cross validation folds')\n",
    "parser.add_argument('--lmbda', type=int, default=16, metavar='L',\n",
    "                help='self-reconstruction error hyperparameter')\n",
    "parser.add_argument('--lr_dim', type=int, default=LR_size, metavar='N',\n",
    "                help='adjacency matrix input dimensions')\n",
    "parser.add_argument('--hr_dim', type=int, default=HR_size, metavar='N',\n",
    "                help='super-resolved adjacency matrix output dimensions')\n",
    "parser.add_argument('--hidden_dim', type=int, default=280, metavar='N',\n",
    "                help='hidden GraphConvolutional layer dimensions')\n",
    "parser.add_argument('--padding', type=int, default=26, metavar='padding',\n",
    "                help='dimensions of padding')\n",
    "\n",
    "# Create an empty Namespace to hold the default arguments\n",
    "args = parser.parse_args([]) \n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(167, 160, 160)\n",
      "(167, 268, 268)\n"
     ]
    }
   ],
   "source": [
    "# SIMULATING THE DATA: EDIT TO ENTER YOUR OWN DATA\n",
    "X = A_LR_train_matrix #np.random.normal(0, 0.5, (167, 160, 160))\n",
    "Y = A_HR_train_matrix #np.random.normal(0, 0.5, (167, 288, 288))\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: \n",
      "Epoch: 1, Train Loss: 0.108182, Train Error: 0.233764, Test Error: 0.199420\n",
      "Epoch: 2, Train Loss: 0.076344, Train Error: 0.201713, Test Error: 0.187566\n",
      "Epoch: 3, Train Loss: 0.070387, Train Error: 0.195285, Test Error: 0.184023\n",
      "Epoch: 4, Train Loss: 0.067677, Train Error: 0.192533, Test Error: 0.181722\n",
      "Epoch: 5, Train Loss: 0.065420, Train Error: 0.189978, Test Error: 0.178983\n",
      "Epoch: 6, Train Loss: 0.063383, Train Error: 0.187779, Test Error: 0.177794\n",
      "Epoch: 7, Train Loss: 0.062370, Train Error: 0.186877, Test Error: 0.177235\n",
      "Epoch: 8, Train Loss: 0.061786, Train Error: 0.186343, Test Error: 0.176785\n",
      "Epoch: 9, Train Loss: 0.061282, Train Error: 0.185858, Test Error: 0.176380\n",
      "Epoch: 10, Train Loss: 0.060862, Train Error: 0.185460, Test Error: 0.176116\n",
      "Epoch: 11, Train Loss: 0.060565, Train Error: 0.185201, Test Error: 0.175907\n",
      "Epoch: 12, Train Loss: 0.060258, Train Error: 0.184861, Test Error: 0.175726\n",
      "Epoch: 13, Train Loss: 0.059981, Train Error: 0.184551, Test Error: 0.175604\n",
      "Epoch: 14, Train Loss: 0.059744, Train Error: 0.184312, Test Error: 0.175520\n",
      "Epoch: 15, Train Loss: 0.059490, Train Error: 0.183995, Test Error: 0.175467\n",
      "Epoch: 16, Train Loss: 0.059295, Train Error: 0.183774, Test Error: 0.175362\n",
      "Epoch: 17, Train Loss: 0.059081, Train Error: 0.183490, Test Error: 0.175298\n",
      "Epoch: 18, Train Loss: 0.058877, Train Error: 0.183237, Test Error: 0.175305\n",
      "Epoch: 19, Train Loss: 0.058727, Train Error: 0.183081, Test Error: 0.175239\n",
      "Epoch: 20, Train Loss: 0.058476, Train Error: 0.182719, Test Error: 0.175151\n",
      "Epoch: 21, Train Loss: 0.058272, Train Error: 0.182401, Test Error: 0.175068\n",
      "Epoch: 22, Train Loss: 0.058045, Train Error: 0.182032, Test Error: 0.174989\n",
      "Epoch: 23, Train Loss: 0.057832, Train Error: 0.181678, Test Error: 0.174851\n",
      "Epoch: 24, Train Loss: 0.057538, Train Error: 0.181190, Test Error: 0.174807\n",
      "Epoch: 25, Train Loss: 0.057275, Train Error: 0.180768, Test Error: 0.174738\n",
      "Epoch: 26, Train Loss: 0.056945, Train Error: 0.180204, Test Error: 0.174558\n",
      "Epoch: 27, Train Loss: 0.056621, Train Error: 0.179653, Test Error: 0.174430\n",
      "Epoch: 28, Train Loss: 0.056264, Train Error: 0.179015, Test Error: 0.174094\n",
      "Epoch: 29, Train Loss: 0.055808, Train Error: 0.178216, Test Error: 0.173668\n",
      "Epoch: 30, Train Loss: 0.055323, Train Error: 0.177361, Test Error: 0.173097\n",
      "Epoch: 31, Train Loss: 0.054777, Train Error: 0.176367, Test Error: 0.172468\n",
      "Epoch: 32, Train Loss: 0.054162, Train Error: 0.175292, Test Error: 0.171821\n",
      "Epoch: 33, Train Loss: 0.053495, Train Error: 0.174106, Test Error: 0.171074\n",
      "Epoch: 34, Train Loss: 0.052791, Train Error: 0.172839, Test Error: 0.170369\n",
      "Epoch: 35, Train Loss: 0.052058, Train Error: 0.171538, Test Error: 0.169526\n",
      "Epoch: 36, Train Loss: 0.051290, Train Error: 0.170182, Test Error: 0.168438\n",
      "Epoch: 37, Train Loss: 0.050517, Train Error: 0.168785, Test Error: 0.167497\n",
      "Epoch: 38, Train Loss: 0.049770, Train Error: 0.167455, Test Error: 0.166452\n",
      "Epoch: 39, Train Loss: 0.049022, Train Error: 0.166146, Test Error: 0.165326\n",
      "Epoch: 40, Train Loss: 0.048231, Train Error: 0.164756, Test Error: 0.164268\n",
      "Epoch: 41, Train Loss: 0.047461, Train Error: 0.163383, Test Error: 0.163236\n",
      "Epoch: 42, Train Loss: 0.046682, Train Error: 0.161972, Test Error: 0.162098\n",
      "Epoch: 43, Train Loss: 0.045904, Train Error: 0.160547, Test Error: 0.161142\n",
      "Epoch: 44, Train Loss: 0.045185, Train Error: 0.159218, Test Error: 0.160326\n",
      "Epoch: 45, Train Loss: 0.044558, Train Error: 0.158037, Test Error: 0.159218\n",
      "Epoch: 46, Train Loss: 0.044005, Train Error: 0.156999, Test Error: 0.158634\n",
      "Epoch: 47, Train Loss: 0.043488, Train Error: 0.155962, Test Error: 0.158023\n",
      "Epoch: 48, Train Loss: 0.043012, Train Error: 0.154987, Test Error: 0.157609\n",
      "Epoch: 49, Train Loss: 0.042575, Train Error: 0.154056, Test Error: 0.157002\n",
      "Epoch: 50, Train Loss: 0.042213, Train Error: 0.153277, Test Error: 0.156514\n",
      "Epoch: 51, Train Loss: 0.041810, Train Error: 0.152396, Test Error: 0.155879\n",
      "Epoch: 52, Train Loss: 0.041485, Train Error: 0.151670, Test Error: 0.155481\n",
      "Epoch: 53, Train Loss: 0.041137, Train Error: 0.150901, Test Error: 0.155140\n",
      "Epoch: 54, Train Loss: 0.040917, Train Error: 0.150396, Test Error: 0.154733\n",
      "Epoch: 55, Train Loss: 0.040579, Train Error: 0.149636, Test Error: 0.154474\n",
      "Epoch: 56, Train Loss: 0.040409, Train Error: 0.149249, Test Error: 0.154086\n",
      "Epoch: 57, Train Loss: 0.040119, Train Error: 0.148573, Test Error: 0.153798\n",
      "Epoch: 58, Train Loss: 0.039901, Train Error: 0.148052, Test Error: 0.153623\n",
      "Epoch: 59, Train Loss: 0.039645, Train Error: 0.147475, Test Error: 0.153311\n",
      "Epoch: 60, Train Loss: 0.039410, Train Error: 0.146909, Test Error: 0.153232\n",
      "Epoch: 61, Train Loss: 0.039171, Train Error: 0.146342, Test Error: 0.152935\n",
      "Epoch: 62, Train Loss: 0.038985, Train Error: 0.145872, Test Error: 0.152780\n",
      "Epoch: 63, Train Loss: 0.038800, Train Error: 0.145442, Test Error: 0.152512\n",
      "Epoch: 64, Train Loss: 0.038669, Train Error: 0.145085, Test Error: 0.152376\n",
      "Epoch: 65, Train Loss: 0.038490, Train Error: 0.144692, Test Error: 0.152280\n",
      "Epoch: 66, Train Loss: 0.038359, Train Error: 0.144348, Test Error: 0.152336\n",
      "Epoch: 67, Train Loss: 0.038161, Train Error: 0.143866, Test Error: 0.152076\n",
      "Epoch: 68, Train Loss: 0.038028, Train Error: 0.143513, Test Error: 0.152064\n",
      "Epoch: 69, Train Loss: 0.037864, Train Error: 0.143139, Test Error: 0.151866\n",
      "Epoch: 70, Train Loss: 0.037751, Train Error: 0.142835, Test Error: 0.151928\n",
      "Epoch: 71, Train Loss: 0.037579, Train Error: 0.142422, Test Error: 0.151791\n",
      "Epoch: 72, Train Loss: 0.037488, Train Error: 0.142171, Test Error: 0.151761\n",
      "Epoch: 73, Train Loss: 0.037327, Train Error: 0.141773, Test Error: 0.151633\n",
      "Epoch: 74, Train Loss: 0.037252, Train Error: 0.141556, Test Error: 0.151494\n",
      "Epoch: 75, Train Loss: 0.037079, Train Error: 0.141146, Test Error: 0.151216\n",
      "Epoch: 76, Train Loss: 0.037044, Train Error: 0.141021, Test Error: 0.151253\n",
      "Epoch: 77, Train Loss: 0.036904, Train Error: 0.140720, Test Error: 0.151128\n",
      "Epoch: 78, Train Loss: 0.036884, Train Error: 0.140625, Test Error: 0.151238\n",
      "Epoch: 79, Train Loss: 0.036748, Train Error: 0.140308, Test Error: 0.151086\n",
      "Epoch: 80, Train Loss: 0.036676, Train Error: 0.140095, Test Error: 0.151143\n",
      "Epoch: 81, Train Loss: 0.036534, Train Error: 0.139757, Test Error: 0.151026\n",
      "Epoch: 82, Train Loss: 0.036457, Train Error: 0.139537, Test Error: 0.151016\n",
      "Epoch: 83, Train Loss: 0.036329, Train Error: 0.139215, Test Error: 0.150975\n",
      "Epoch: 84, Train Loss: 0.036294, Train Error: 0.139108, Test Error: 0.151001\n",
      "Epoch: 85, Train Loss: 0.036198, Train Error: 0.138859, Test Error: 0.150409\n",
      "Epoch: 86, Train Loss: 0.036141, Train Error: 0.138689, Test Error: 0.150940\n",
      "Epoch: 87, Train Loss: 0.036020, Train Error: 0.138427, Test Error: 0.150837\n",
      "Epoch: 88, Train Loss: 0.036045, Train Error: 0.138461, Test Error: 0.150814\n",
      "Epoch: 1, Train Loss: 0.105415, Train Error: 0.232294, Test Error: 0.208011\n",
      "Epoch: 2, Train Loss: 0.074686, Train Error: 0.199932, Test Error: 0.196219\n",
      "Epoch: 3, Train Loss: 0.068394, Train Error: 0.192683, Test Error: 0.191740\n",
      "Epoch: 4, Train Loss: 0.065545, Train Error: 0.189635, Test Error: 0.189837\n",
      "Epoch: 5, Train Loss: 0.063866, Train Error: 0.188007, Test Error: 0.188666\n",
      "Epoch: 6, Train Loss: 0.062833, Train Error: 0.187116, Test Error: 0.188105\n",
      "Epoch: 7, Train Loss: 0.062234, Train Error: 0.186666, Test Error: 0.187778\n",
      "Epoch: 8, Train Loss: 0.061807, Train Error: 0.186340, Test Error: 0.187575\n",
      "Epoch: 9, Train Loss: 0.061431, Train Error: 0.186012, Test Error: 0.187354\n",
      "Epoch: 10, Train Loss: 0.061147, Train Error: 0.185777, Test Error: 0.187214\n",
      "Epoch: 11, Train Loss: 0.060890, Train Error: 0.185542, Test Error: 0.187047\n",
      "Epoch: 12, Train Loss: 0.060630, Train Error: 0.185273, Test Error: 0.186937\n",
      "Epoch: 13, Train Loss: 0.060385, Train Error: 0.185018, Test Error: 0.186842\n",
      "Epoch: 14, Train Loss: 0.060114, Train Error: 0.184709, Test Error: 0.186689\n",
      "Epoch: 15, Train Loss: 0.059833, Train Error: 0.184380, Test Error: 0.186563\n",
      "Epoch: 16, Train Loss: 0.059541, Train Error: 0.184025, Test Error: 0.186379\n",
      "Epoch: 17, Train Loss: 0.059178, Train Error: 0.183528, Test Error: 0.186112\n",
      "Epoch: 18, Train Loss: 0.058776, Train Error: 0.182978, Test Error: 0.185700\n",
      "Epoch: 19, Train Loss: 0.058292, Train Error: 0.182281, Test Error: 0.185191\n",
      "Epoch: 20, Train Loss: 0.057717, Train Error: 0.181450, Test Error: 0.184486\n",
      "Epoch: 21, Train Loss: 0.057057, Train Error: 0.180457, Test Error: 0.183625\n",
      "Epoch: 22, Train Loss: 0.056285, Train Error: 0.179301, Test Error: 0.182658\n",
      "Epoch: 23, Train Loss: 0.055474, Train Error: 0.178058, Test Error: 0.181710\n",
      "Epoch: 24, Train Loss: 0.054688, Train Error: 0.176834, Test Error: 0.180770\n",
      "Epoch: 25, Train Loss: 0.053969, Train Error: 0.175684, Test Error: 0.180015\n",
      "Epoch: 26, Train Loss: 0.053351, Train Error: 0.174676, Test Error: 0.179297\n",
      "Epoch: 27, Train Loss: 0.052774, Train Error: 0.173698, Test Error: 0.178647\n",
      "Epoch: 28, Train Loss: 0.052199, Train Error: 0.172685, Test Error: 0.178024\n",
      "Epoch: 29, Train Loss: 0.051621, Train Error: 0.171644, Test Error: 0.177301\n",
      "Epoch: 30, Train Loss: 0.051002, Train Error: 0.170530, Test Error: 0.176567\n",
      "Epoch: 31, Train Loss: 0.050349, Train Error: 0.169337, Test Error: 0.175678\n",
      "Epoch: 32, Train Loss: 0.049647, Train Error: 0.168058, Test Error: 0.174725\n",
      "Epoch: 33, Train Loss: 0.048867, Train Error: 0.166640, Test Error: 0.173625\n",
      "Epoch: 34, Train Loss: 0.048029, Train Error: 0.165116, Test Error: 0.172354\n",
      "Epoch: 35, Train Loss: 0.047110, Train Error: 0.163465, Test Error: 0.170982\n",
      "Epoch: 36, Train Loss: 0.046169, Train Error: 0.161786, Test Error: 0.169575\n",
      "Epoch: 37, Train Loss: 0.045204, Train Error: 0.160076, Test Error: 0.168244\n",
      "Epoch: 38, Train Loss: 0.044273, Train Error: 0.158430, Test Error: 0.167010\n",
      "Epoch: 39, Train Loss: 0.043451, Train Error: 0.156946, Test Error: 0.166019\n",
      "Epoch: 40, Train Loss: 0.042757, Train Error: 0.155664, Test Error: 0.165221\n",
      "Epoch: 41, Train Loss: 0.042181, Train Error: 0.154560, Test Error: 0.164591\n",
      "Epoch: 42, Train Loss: 0.041682, Train Error: 0.153555, Test Error: 0.164041\n",
      "Epoch: 43, Train Loss: 0.041253, Train Error: 0.152672, Test Error: 0.163587\n",
      "Epoch: 44, Train Loss: 0.040860, Train Error: 0.151818, Test Error: 0.163110\n",
      "Epoch: 45, Train Loss: 0.040483, Train Error: 0.150981, Test Error: 0.162729\n",
      "Epoch: 46, Train Loss: 0.040143, Train Error: 0.150198, Test Error: 0.162365\n",
      "Epoch: 47, Train Loss: 0.039838, Train Error: 0.149492, Test Error: 0.161969\n",
      "Epoch: 48, Train Loss: 0.039527, Train Error: 0.148771, Test Error: 0.161661\n",
      "Epoch: 49, Train Loss: 0.039241, Train Error: 0.148090, Test Error: 0.161285\n",
      "Epoch: 50, Train Loss: 0.038975, Train Error: 0.147446, Test Error: 0.161005\n",
      "Epoch: 51, Train Loss: 0.038719, Train Error: 0.146838, Test Error: 0.160692\n",
      "Epoch: 52, Train Loss: 0.038470, Train Error: 0.146220, Test Error: 0.160407\n",
      "Epoch: 53, Train Loss: 0.038223, Train Error: 0.145621, Test Error: 0.160042\n",
      "Epoch: 54, Train Loss: 0.037999, Train Error: 0.145054, Test Error: 0.159728\n",
      "Epoch: 55, Train Loss: 0.037797, Train Error: 0.144543, Test Error: 0.159552\n",
      "Epoch: 56, Train Loss: 0.037573, Train Error: 0.143965, Test Error: 0.159183\n",
      "Epoch: 57, Train Loss: 0.037391, Train Error: 0.143510, Test Error: 0.159006\n",
      "Epoch: 58, Train Loss: 0.037179, Train Error: 0.142981, Test Error: 0.158784\n",
      "Epoch: 59, Train Loss: 0.037003, Train Error: 0.142532, Test Error: 0.158642\n",
      "Epoch: 60, Train Loss: 0.036814, Train Error: 0.142054, Test Error: 0.158402\n",
      "Epoch: 61, Train Loss: 0.036631, Train Error: 0.141599, Test Error: 0.158245\n",
      "Epoch: 62, Train Loss: 0.036457, Train Error: 0.141159, Test Error: 0.158045\n",
      "Epoch: 63, Train Loss: 0.036296, Train Error: 0.140749, Test Error: 0.157886\n",
      "Epoch: 64, Train Loss: 0.036139, Train Error: 0.140346, Test Error: 0.157793\n",
      "Epoch: 65, Train Loss: 0.036000, Train Error: 0.139998, Test Error: 0.157630\n",
      "Epoch: 66, Train Loss: 0.035867, Train Error: 0.139637, Test Error: 0.157566\n",
      "Epoch: 67, Train Loss: 0.035717, Train Error: 0.139274, Test Error: 0.157373\n",
      "Epoch: 68, Train Loss: 0.035587, Train Error: 0.138934, Test Error: 0.157232\n",
      "Epoch: 69, Train Loss: 0.035455, Train Error: 0.138594, Test Error: 0.157107\n",
      "Epoch: 70, Train Loss: 0.035339, Train Error: 0.138287, Test Error: 0.157027\n",
      "Epoch: 71, Train Loss: 0.035206, Train Error: 0.137955, Test Error: 0.156916\n",
      "Epoch: 72, Train Loss: 0.035115, Train Error: 0.137700, Test Error: 0.156846\n",
      "Epoch: 73, Train Loss: 0.035006, Train Error: 0.137429, Test Error: 0.156728\n",
      "Epoch: 74, Train Loss: 0.034930, Train Error: 0.137209, Test Error: 0.156636\n",
      "Epoch: 75, Train Loss: 0.034844, Train Error: 0.136993, Test Error: 0.156509\n",
      "Epoch: 76, Train Loss: 0.034767, Train Error: 0.136775, Test Error: 0.156433\n",
      "Epoch: 77, Train Loss: 0.034684, Train Error: 0.136585, Test Error: 0.156231\n",
      "Epoch: 78, Train Loss: 0.034579, Train Error: 0.136288, Test Error: 0.156260\n",
      "Epoch: 79, Train Loss: 0.034468, Train Error: 0.136009, Test Error: 0.156096\n",
      "Epoch: 80, Train Loss: 0.034391, Train Error: 0.135791, Test Error: 0.156153\n",
      "Epoch: 81, Train Loss: 0.034309, Train Error: 0.135588, Test Error: 0.155964\n",
      "Epoch: 82, Train Loss: 0.034242, Train Error: 0.135399, Test Error: 0.156037\n",
      "Epoch: 83, Train Loss: 0.034168, Train Error: 0.135219, Test Error: 0.155943\n",
      "Epoch: 84, Train Loss: 0.034081, Train Error: 0.134982, Test Error: 0.155873\n",
      "Epoch: 85, Train Loss: 0.033989, Train Error: 0.134762, Test Error: 0.155735\n",
      "Epoch: 86, Train Loss: 0.033898, Train Error: 0.134507, Test Error: 0.155643\n",
      "Epoch: 87, Train Loss: 0.033821, Train Error: 0.134312, Test Error: 0.155587\n",
      "Epoch: 88, Train Loss: 0.033760, Train Error: 0.134144, Test Error: 0.155590\n",
      "Epoch: 89, Train Loss: 0.033681, Train Error: 0.133947, Test Error: 0.155492\n",
      "Epoch: 90, Train Loss: 0.033626, Train Error: 0.133786, Test Error: 0.155505\n",
      "Epoch: 91, Train Loss: 0.033558, Train Error: 0.133595, Test Error: 0.155436\n",
      "Epoch: 92, Train Loss: 0.033464, Train Error: 0.133337, Test Error: 0.155412\n",
      "Epoch: 93, Train Loss: 0.033390, Train Error: 0.133135, Test Error: 0.155317\n",
      "Epoch: 94, Train Loss: 0.033342, Train Error: 0.133000, Test Error: 0.155242\n",
      "Epoch: 95, Train Loss: 0.033316, Train Error: 0.132906, Test Error: 0.155098\n",
      "Epoch: 96, Train Loss: 0.033271, Train Error: 0.132803, Test Error: 0.155086\n",
      "Epoch: 97, Train Loss: 0.033201, Train Error: 0.132596, Test Error: 0.154913\n",
      "Epoch: 98, Train Loss: 0.033131, Train Error: 0.132425, Test Error: 0.154996\n",
      "Epoch: 99, Train Loss: 0.033117, Train Error: 0.132363, Test Error: 0.154953\n",
      "Epoch: 100, Train Loss: 0.033039, Train Error: 0.132190, Test Error: 0.155072\n",
      "Epoch: 1, Train Loss: 0.105819, Train Error: 0.231633, Test Error: 0.204357\n",
      "Epoch: 2, Train Loss: 0.076329, Train Error: 0.201471, Test Error: 0.192404\n",
      "Epoch: 3, Train Loss: 0.069290, Train Error: 0.193540, Test Error: 0.187344\n",
      "Epoch: 4, Train Loss: 0.065615, Train Error: 0.189296, Test Error: 0.183886\n",
      "Epoch: 5, Train Loss: 0.063155, Train Error: 0.186497, Test Error: 0.181810\n",
      "Epoch: 6, Train Loss: 0.061489, Train Error: 0.184753, Test Error: 0.180880\n",
      "Epoch: 7, Train Loss: 0.060553, Train Error: 0.183890, Test Error: 0.180428\n",
      "Epoch: 8, Train Loss: 0.059899, Train Error: 0.183277, Test Error: 0.179948\n",
      "Epoch: 9, Train Loss: 0.059358, Train Error: 0.182718, Test Error: 0.179523\n",
      "Epoch: 10, Train Loss: 0.058918, Train Error: 0.182285, Test Error: 0.179148\n",
      "Epoch: 11, Train Loss: 0.058568, Train Error: 0.181945, Test Error: 0.178890\n",
      "Epoch: 12, Train Loss: 0.058275, Train Error: 0.181654, Test Error: 0.178669\n",
      "Epoch: 13, Train Loss: 0.057998, Train Error: 0.181312, Test Error: 0.178324\n",
      "Epoch: 14, Train Loss: 0.057683, Train Error: 0.180965, Test Error: 0.178165\n",
      "Epoch: 15, Train Loss: 0.057430, Train Error: 0.180668, Test Error: 0.177980\n",
      "Epoch: 16, Train Loss: 0.057208, Train Error: 0.180384, Test Error: 0.177767\n",
      "Epoch: 17, Train Loss: 0.057023, Train Error: 0.180143, Test Error: 0.177551\n",
      "Epoch: 18, Train Loss: 0.056842, Train Error: 0.179903, Test Error: 0.177380\n",
      "Epoch: 19, Train Loss: 0.056660, Train Error: 0.179654, Test Error: 0.177226\n",
      "Epoch: 20, Train Loss: 0.056444, Train Error: 0.179348, Test Error: 0.177011\n",
      "Epoch: 21, Train Loss: 0.056222, Train Error: 0.179025, Test Error: 0.176799\n",
      "Epoch: 22, Train Loss: 0.055964, Train Error: 0.178629, Test Error: 0.176601\n",
      "Epoch: 23, Train Loss: 0.055706, Train Error: 0.178206, Test Error: 0.176367\n",
      "Epoch: 24, Train Loss: 0.055408, Train Error: 0.177693, Test Error: 0.176131\n",
      "Epoch: 25, Train Loss: 0.055087, Train Error: 0.177120, Test Error: 0.175844\n",
      "Epoch: 26, Train Loss: 0.054743, Train Error: 0.176517, Test Error: 0.175491\n",
      "Epoch: 27, Train Loss: 0.054343, Train Error: 0.175797, Test Error: 0.175069\n",
      "Epoch: 28, Train Loss: 0.053920, Train Error: 0.175036, Test Error: 0.174662\n",
      "Epoch: 29, Train Loss: 0.053474, Train Error: 0.174240, Test Error: 0.174133\n",
      "Epoch: 30, Train Loss: 0.052973, Train Error: 0.173339, Test Error: 0.173548\n",
      "Epoch: 31, Train Loss: 0.052454, Train Error: 0.172380, Test Error: 0.172884\n",
      "Epoch: 32, Train Loss: 0.051928, Train Error: 0.171422, Test Error: 0.172280\n",
      "Epoch: 33, Train Loss: 0.051342, Train Error: 0.170352, Test Error: 0.171585\n",
      "Epoch: 34, Train Loss: 0.050763, Train Error: 0.169281, Test Error: 0.170838\n",
      "Epoch: 35, Train Loss: 0.050158, Train Error: 0.168154, Test Error: 0.170062\n",
      "Epoch: 36, Train Loss: 0.049539, Train Error: 0.166998, Test Error: 0.169235\n",
      "Epoch: 37, Train Loss: 0.048907, Train Error: 0.165804, Test Error: 0.168399\n",
      "Epoch: 38, Train Loss: 0.048288, Train Error: 0.164634, Test Error: 0.167426\n",
      "Epoch: 39, Train Loss: 0.047644, Train Error: 0.163428, Test Error: 0.166423\n",
      "Epoch: 40, Train Loss: 0.046994, Train Error: 0.162212, Test Error: 0.165501\n",
      "Epoch: 41, Train Loss: 0.046355, Train Error: 0.161010, Test Error: 0.164624\n",
      "Epoch: 42, Train Loss: 0.045717, Train Error: 0.159825, Test Error: 0.163735\n",
      "Epoch: 43, Train Loss: 0.045064, Train Error: 0.158621, Test Error: 0.162913\n",
      "Epoch: 44, Train Loss: 0.044422, Train Error: 0.157425, Test Error: 0.161948\n",
      "Epoch: 45, Train Loss: 0.043799, Train Error: 0.156258, Test Error: 0.161173\n",
      "Epoch: 46, Train Loss: 0.043186, Train Error: 0.155089, Test Error: 0.160387\n",
      "Epoch: 47, Train Loss: 0.042684, Train Error: 0.154134, Test Error: 0.159663\n",
      "Epoch: 48, Train Loss: 0.042190, Train Error: 0.153192, Test Error: 0.159077\n",
      "Epoch: 49, Train Loss: 0.041750, Train Error: 0.152342, Test Error: 0.158378\n",
      "Epoch: 50, Train Loss: 0.041339, Train Error: 0.151532, Test Error: 0.157850\n",
      "Epoch: 51, Train Loss: 0.040988, Train Error: 0.150842, Test Error: 0.157465\n",
      "Epoch: 52, Train Loss: 0.040642, Train Error: 0.150140, Test Error: 0.157065\n",
      "Epoch: 53, Train Loss: 0.040304, Train Error: 0.149431, Test Error: 0.156451\n",
      "Epoch: 54, Train Loss: 0.039981, Train Error: 0.148741, Test Error: 0.156020\n",
      "Epoch: 55, Train Loss: 0.039654, Train Error: 0.148015, Test Error: 0.155472\n",
      "Epoch: 56, Train Loss: 0.039368, Train Error: 0.147370, Test Error: 0.155146\n",
      "Epoch: 57, Train Loss: 0.039130, Train Error: 0.146841, Test Error: 0.154759\n",
      "Epoch: 58, Train Loss: 0.038875, Train Error: 0.146230, Test Error: 0.154501\n",
      "Epoch: 59, Train Loss: 0.038710, Train Error: 0.145859, Test Error: 0.154351\n",
      "Epoch: 60, Train Loss: 0.038485, Train Error: 0.145313, Test Error: 0.154041\n",
      "Epoch: 61, Train Loss: 0.038445, Train Error: 0.145110, Test Error: 0.154085\n",
      "Epoch: 62, Train Loss: 0.038137, Train Error: 0.144503, Test Error: 0.153815\n",
      "Epoch: 63, Train Loss: 0.037897, Train Error: 0.143907, Test Error: 0.153430\n",
      "Epoch: 64, Train Loss: 0.037697, Train Error: 0.143405, Test Error: 0.153221\n",
      "Epoch: 65, Train Loss: 0.037536, Train Error: 0.143012, Test Error: 0.152861\n",
      "Epoch: 66, Train Loss: 0.037372, Train Error: 0.142623, Test Error: 0.152546\n",
      "Epoch: 67, Train Loss: 0.037193, Train Error: 0.142173, Test Error: 0.152331\n",
      "Epoch: 68, Train Loss: 0.036999, Train Error: 0.141723, Test Error: 0.152256\n",
      "Epoch: 69, Train Loss: 0.036828, Train Error: 0.141282, Test Error: 0.152061\n",
      "Epoch: 70, Train Loss: 0.036595, Train Error: 0.140720, Test Error: 0.151805\n",
      "Epoch: 71, Train Loss: 0.036457, Train Error: 0.140349, Test Error: 0.151770\n",
      "Epoch: 72, Train Loss: 0.036271, Train Error: 0.139925, Test Error: 0.151515\n",
      "Epoch: 73, Train Loss: 0.036194, Train Error: 0.139688, Test Error: 0.151428\n",
      "Epoch: 74, Train Loss: 0.036086, Train Error: 0.139431, Test Error: 0.151173\n",
      "Epoch: 75, Train Loss: 0.035983, Train Error: 0.139160, Test Error: 0.150989\n",
      "Epoch: 76, Train Loss: 0.035821, Train Error: 0.138770, Test Error: 0.150853\n",
      "Epoch: 77, Train Loss: 0.035704, Train Error: 0.138453, Test Error: 0.150688\n",
      "Epoch: 78, Train Loss: 0.035541, Train Error: 0.138065, Test Error: 0.150527\n",
      "Epoch: 79, Train Loss: 0.035500, Train Error: 0.137940, Test Error: 0.150408\n",
      "Epoch: 80, Train Loss: 0.035371, Train Error: 0.137613, Test Error: 0.150352\n",
      "Epoch: 81, Train Loss: 0.035313, Train Error: 0.137457, Test Error: 0.150306\n",
      "Epoch: 82, Train Loss: 0.035203, Train Error: 0.137173, Test Error: 0.150214\n",
      "Epoch: 83, Train Loss: 0.035137, Train Error: 0.136982, Test Error: 0.150087\n",
      "Epoch: 84, Train Loss: 0.035037, Train Error: 0.136717, Test Error: 0.150011\n",
      "Epoch: 85, Train Loss: 0.034983, Train Error: 0.136571, Test Error: 0.150037\n",
      "Epoch: 86, Train Loss: 0.034864, Train Error: 0.136252, Test Error: 0.149655\n",
      "Epoch: 87, Train Loss: 0.034746, Train Error: 0.135960, Test Error: 0.149521\n",
      "Epoch: 88, Train Loss: 0.034617, Train Error: 0.135616, Test Error: 0.149442\n",
      "Epoch: 89, Train Loss: 0.034511, Train Error: 0.135350, Test Error: 0.149315\n",
      "Epoch: 90, Train Loss: 0.034442, Train Error: 0.135167, Test Error: 0.149261\n",
      "Epoch: 91, Train Loss: 0.034349, Train Error: 0.134928, Test Error: 0.149189\n",
      "Epoch: 92, Train Loss: 0.034265, Train Error: 0.134686, Test Error: 0.149184\n",
      "Epoch: 93, Train Loss: 0.034172, Train Error: 0.134447, Test Error: 0.149105\n",
      "Epoch: 94, Train Loss: 0.034093, Train Error: 0.134236, Test Error: 0.149004\n",
      "Epoch: 95, Train Loss: 0.033994, Train Error: 0.133983, Test Error: 0.148909\n",
      "Epoch: 96, Train Loss: 0.033923, Train Error: 0.133791, Test Error: 0.148848\n",
      "Epoch: 97, Train Loss: 0.033914, Train Error: 0.133782, Test Error: 0.148877\n",
      "Epoch: 98, Train Loss: 0.033823, Train Error: 0.133500, Test Error: 0.148967\n",
      "Epoch: 99, Train Loss: 0.033756, Train Error: 0.133329, Test Error: 0.148948\n",
      "Epoch: 100, Train Loss: 0.033687, Train Error: 0.133126, Test Error: 0.148754\n",
      "Epoch: 101, Train Loss: 0.033763, Train Error: 0.133321, Test Error: 0.148909\n",
      "Epoch: 102, Train Loss: 0.033645, Train Error: 0.133020, Test Error: 0.148717\n",
      "Epoch: 103, Train Loss: 0.033575, Train Error: 0.132825, Test Error: 0.148787\n",
      "Epoch: 104, Train Loss: 0.033504, Train Error: 0.132655, Test Error: 0.148612\n",
      "Epoch: 105, Train Loss: 0.033446, Train Error: 0.132502, Test Error: 0.148452\n",
      "Epoch: 106, Train Loss: 0.033366, Train Error: 0.132280, Test Error: 0.148400\n",
      "Epoch: 107, Train Loss: 0.033328, Train Error: 0.132188, Test Error: 0.148354\n",
      "Epoch: 108, Train Loss: 0.033245, Train Error: 0.131959, Test Error: 0.148315\n",
      "Epoch: 109, Train Loss: 0.033221, Train Error: 0.131890, Test Error: 0.148268\n",
      "Epoch: 110, Train Loss: 0.033133, Train Error: 0.131672, Test Error: 0.148275\n",
      "Epoch: 111, Train Loss: 0.033090, Train Error: 0.131534, Test Error: 0.148315\n",
      "Epoch: 112, Train Loss: 0.033018, Train Error: 0.131340, Test Error: 0.148365\n",
      "Epoch: 113, Train Loss: 0.032997, Train Error: 0.131267, Test Error: 0.148215\n",
      "Epoch: 114, Train Loss: 0.032935, Train Error: 0.131116, Test Error: 0.148233\n",
      "Epoch: 115, Train Loss: 0.032962, Train Error: 0.131189, Test Error: 0.148154\n",
      "Epoch: 116, Train Loss: 0.032898, Train Error: 0.131011, Test Error: 0.148138\n",
      "Epoch: 117, Train Loss: 0.032878, Train Error: 0.130942, Test Error: 0.148177\n",
      "Epoch: 118, Train Loss: 0.032870, Train Error: 0.130911, Test Error: 0.148117\n",
      "Epoch: 119, Train Loss: 0.032815, Train Error: 0.130761, Test Error: 0.148103\n",
      "Epoch: 120, Train Loss: 0.032821, Train Error: 0.130797, Test Error: 0.148072\n",
      "Epoch: 121, Train Loss: 0.032795, Train Error: 0.130701, Test Error: 0.148037\n",
      "Epoch: 122, Train Loss: 0.032734, Train Error: 0.130540, Test Error: 0.148104\n",
      "Epoch: 123, Train Loss: 0.032679, Train Error: 0.130382, Test Error: 0.148082\n",
      "Epoch: 124, Train Loss: 0.032653, Train Error: 0.130301, Test Error: 0.148068\n",
      "Epoch: 125, Train Loss: 0.032601, Train Error: 0.130162, Test Error: 0.148022\n",
      "Epoch: 126, Train Loss: 0.032563, Train Error: 0.130077, Test Error: 0.148242\n",
      "Epoch: 127, Train Loss: 0.032497, Train Error: 0.129893, Test Error: 0.148340\n",
      "Epoch: 128, Train Loss: 0.032500, Train Error: 0.129891, Test Error: 0.148378\n"
     ]
    }
   ],
   "source": [
    "# def train\n",
    "# subjects_adj, subjects_ground_truth, test_adj, test_ground_truth = data()\n",
    "# X = np.concatenate((subjects_adj, test_adj), axis=0)\n",
    "# Y = np.concatenate((subjects_ground_truth, test_ground_truth), axis=0)\n",
    "\n",
    "cv = KFold(n_splits=args.splits, random_state=42, shuffle=True)\n",
    "print(\"Torch: \")\n",
    "\n",
    "# ks = [0]\n",
    "ks = [0.9, 0.7, 0.6, 0.5]\n",
    "\n",
    "\n",
    "# layer = ULayer()\n",
    "# print(model)\n",
    "\n",
    "best_model_fold_list = []\n",
    "data_fold_list = []\n",
    "\n",
    "for train_index, test_index in cv.split(X):\n",
    "    model = GSRNet(ks, args).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "    subjects_adj, test_adj, subjects_ground_truth, test_ground_truth = X[\n",
    "        train_index], X[test_index], Y[train_index], Y[test_index]\n",
    "    data_fold_list.append((subjects_adj, test_adj, subjects_ground_truth, test_ground_truth))\n",
    "\n",
    "    return_model = train(model, optimizer, subjects_adj, subjects_ground_truth, args, test_adj, test_ground_truth)\n",
    "    test(return_model, test_adj, test_ground_truth, args)\n",
    "    best_model_fold_list.append(return_model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MatrixVectorizer import MatrixVectorizer\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "import torch\n",
    "import networkx as nx\n",
    "\n",
    "def evaluate(pred_matrices, gt_matrices):\n",
    "\n",
    "    num_test_samples = gt_matrices.shape[0]\n",
    "\n",
    "    # Initialize lists to store MAEs for each centrality measure\n",
    "    mae_bc = []\n",
    "    mae_ec = []\n",
    "    mae_pc = []\n",
    "\n",
    "    # # Iterate over each test sample\n",
    "    # for i in range(num_test_samples):\n",
    "    #     # Convert adjacency matrices to NetworkX graphs\n",
    "    #     pred_graph = nx.from_numpy_array(pred_matrices[i], edge_attr=\"weight\")\n",
    "    #     gt_graph = nx.from_numpy_array(gt_matrices[i], edge_attr=\"weight\")\n",
    "\n",
    "    #     # Compute centrality measures\n",
    "    #     pred_bc = nx.betweenness_centrality(pred_graph, weight=\"weight\")\n",
    "    #     pred_ec = nx.eigenvector_centrality(pred_graph, weight=\"weight\")\n",
    "    #     pred_pc = nx.pagerank(pred_graph, weight=\"weight\")\n",
    "\n",
    "    #     gt_bc = nx.betweenness_centrality(gt_graph, weight=\"weight\")\n",
    "    #     gt_ec = nx.eigenvector_centrality(gt_graph, weight=\"weight\")\n",
    "    #     gt_pc = nx.pagerank(gt_graph, weight=\"weight\")\n",
    "\n",
    "    #     # Convert centrality dictionaries to lists\n",
    "    #     pred_bc_values = list(pred_bc.values())\n",
    "    #     pred_ec_values = list(pred_ec.values())\n",
    "    #     pred_pc_values = list(pred_pc.values())\n",
    "\n",
    "    #     gt_bc_values = list(gt_bc.values())\n",
    "    #     gt_ec_values = list(gt_ec.values())\n",
    "    #     gt_pc_values = list(gt_pc.values())\n",
    "\n",
    "    #     # Compute MAEs\n",
    "    #     mae_bc.append(mean_absolute_error(pred_bc_values, gt_bc_values))\n",
    "    #     mae_ec.append(mean_absolute_error(pred_ec_values, gt_ec_values))\n",
    "    #     mae_pc.append(mean_absolute_error(pred_pc_values, gt_pc_values))\n",
    "\n",
    "    # # Compute average MAEs\n",
    "    # avg_mae_bc = sum(mae_bc) / len(mae_bc)\n",
    "    # avg_mae_ec = sum(mae_ec) / len(mae_ec)\n",
    "    # avg_mae_pc = sum(mae_pc) / len(mae_pc)\n",
    "\n",
    "    # vectorize and flatten\n",
    "    pred_1d = MatrixVectorizer.vectorize(pred_matrices).flatten()\n",
    "    gt_1d = MatrixVectorizer.vectorize(gt_matrices).flatten()\n",
    "\n",
    "    mae = mean_absolute_error(pred_1d, gt_1d)\n",
    "    pcc = pearsonr(pred_1d, gt_1d)[0]\n",
    "    js_dis = jensenshannon(pred_1d, gt_1d)\n",
    "\n",
    "    print(\"MAE: \", mae)\n",
    "    print(\"PCC: \", pcc)\n",
    "    print(\"Jensen-Shannon Distance: \", js_dis)\n",
    "    print(\"Average MAE betweenness centrality:\", avg_mae_bc)\n",
    "    print(\"Average MAE eigenvector centrality:\", avg_mae_ec)\n",
    "    print(\"Average MAE PageRank centrality:\", avg_mae_pc)\n",
    "    return mae, pcc, js_dis, avg_mae_bc, avg_mae_ec, avg_mae_pc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  0.15223975956551844\n",
      "PCC:  0.5680369387507287\n",
      "Jensen-Shannon Distance:  0.304701655082658\n",
      "Average MAE betweenness centrality: 0.021107029270308084\n",
      "Average MAE eigenvector centrality: 0.015205685202668533\n",
      "Average MAE PageRank centrality: 0.0006510657165047646\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/wutikorn/Desktop/Imperial/Deep Graph-based Learning/Project/graph_super_resolution/gsr_net/Initial_GSR.ipynb Cell 14\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/wutikorn/Desktop/Imperial/Deep%20Graph-based%20Learning/Project/graph_super_resolution/gsr_net/Initial_GSR.ipynb#X24sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mfor\u001b[39;00m j, test_adj \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(test_adjs):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/wutikorn/Desktop/Imperial/Deep%20Graph-based%20Learning/Project/graph_super_resolution/gsr_net/Initial_GSR.ipynb#X24sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         pred_matrices[j], _, _, _ \u001b[39m=\u001b[39m model(torch\u001b[39m.\u001b[39mfrom_numpy(test_adj))\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/wutikorn/Desktop/Imperial/Deep%20Graph-based%20Learning/Project/graph_super_resolution/gsr_net/Initial_GSR.ipynb#X24sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m evaluate(pred_matrices, gt_matrices)\n",
      "\u001b[1;32m/Users/wutikorn/Desktop/Imperial/Deep Graph-based Learning/Project/graph_super_resolution/gsr_net/Initial_GSR.ipynb Cell 14\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/wutikorn/Desktop/Imperial/Deep%20Graph-based%20Learning/Project/graph_super_resolution/gsr_net/Initial_GSR.ipynb#X24sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m gt_graph \u001b[39m=\u001b[39m nx\u001b[39m.\u001b[39mfrom_numpy_array(gt_matrices[i], edge_attr\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mweight\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/wutikorn/Desktop/Imperial/Deep%20Graph-based%20Learning/Project/graph_super_resolution/gsr_net/Initial_GSR.ipynb#X24sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# Compute centrality measures\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/wutikorn/Desktop/Imperial/Deep%20Graph-based%20Learning/Project/graph_super_resolution/gsr_net/Initial_GSR.ipynb#X24sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m pred_bc \u001b[39m=\u001b[39m nx\u001b[39m.\u001b[39;49mbetweenness_centrality(pred_graph, weight\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mweight\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/wutikorn/Desktop/Imperial/Deep%20Graph-based%20Learning/Project/graph_super_resolution/gsr_net/Initial_GSR.ipynb#X24sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m pred_ec \u001b[39m=\u001b[39m nx\u001b[39m.\u001b[39meigenvector_centrality(pred_graph, weight\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mweight\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/wutikorn/Desktop/Imperial/Deep%20Graph-based%20Learning/Project/graph_super_resolution/gsr_net/Initial_GSR.ipynb#X24sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m pred_pc \u001b[39m=\u001b[39m nx\u001b[39m.\u001b[39mpagerank(pred_graph, weight\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mweight\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m<class 'networkx.utils.decorators.argmap'> compilation 8:4\u001b[0m, in \u001b[0;36margmap_betweenness_centrality_5\u001b[0;34m(G, k, normalized, weight, endpoints, seed, backend, **backend_kwargs)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcollections\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mgzip\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39minspect\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mitertools\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mre\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/networkx/utils/backends.py:412\u001b[0m, in \u001b[0;36m_dispatch.__call__\u001b[0;34m(self, backend, *args, **kwargs)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m/\u001b[39m, \u001b[39m*\u001b[39margs, backend\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    410\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m backends:\n\u001b[1;32m    411\u001b[0m         \u001b[39m# Fast path if no backends are installed\u001b[39;00m\n\u001b[0;32m--> 412\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49morig_func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    414\u001b[0m     \u001b[39m# Use `backend_name` in this function instead of `backend`\u001b[39;00m\n\u001b[1;32m    415\u001b[0m     backend_name \u001b[39m=\u001b[39m backend\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/networkx/algorithms/centrality/betweenness.py:138\u001b[0m, in \u001b[0;36mbetweenness_centrality\u001b[0;34m(G, k, normalized, weight, endpoints, seed)\u001b[0m\n\u001b[1;32m    136\u001b[0m     S, P, sigma, _ \u001b[39m=\u001b[39m _single_source_shortest_path_basic(G, s)\n\u001b[1;32m    137\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# use Dijkstra's algorithm\u001b[39;00m\n\u001b[0;32m--> 138\u001b[0m     S, P, sigma, _ \u001b[39m=\u001b[39m _single_source_dijkstra_path_basic(G, s, weight)\n\u001b[1;32m    139\u001b[0m \u001b[39m# accumulation\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[39mif\u001b[39;00m endpoints:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/networkx/algorithms/centrality/betweenness.py:303\u001b[0m, in \u001b[0;36m_single_source_dijkstra_path_basic\u001b[0;34m(G, s, weight)\u001b[0m\n\u001b[1;32m    301\u001b[0m S\u001b[39m.\u001b[39mappend(v)\n\u001b[1;32m    302\u001b[0m D[v] \u001b[39m=\u001b[39m dist\n\u001b[0;32m--> 303\u001b[0m \u001b[39mfor\u001b[39;00m w, edgedata \u001b[39min\u001b[39;00m G[v]\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    304\u001b[0m     vw_dist \u001b[39m=\u001b[39m dist \u001b[39m+\u001b[39m weight(v, w, edgedata)\n\u001b[1;32m    305\u001b[0m     \u001b[39mif\u001b[39;00m w \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m D \u001b[39mand\u001b[39;00m (w \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m seen \u001b[39mor\u001b[39;00m vw_dist \u001b[39m<\u001b[39m seen[w]):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/_collections_abc.py:911\u001b[0m, in \u001b[0;36mItemsView.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__iter__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    910\u001b[0m     \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mapping:\n\u001b[0;32m--> 911\u001b[0m         \u001b[39myield\u001b[39;00m (key, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mapping[key])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(args.splits):\n",
    "    _, test_adjs, _, gt_matrices = data_fold_list[i]\n",
    "    model = best_model_fold_list[i]\n",
    "    model.eval()\n",
    "    pred_matrices = np.zeros(gt_matrices.shape)\n",
    "    with torch.no_grad():\n",
    "        for j, test_adj in enumerate(test_adjs):\n",
    "            pred_matrices[j], _, _, _ = model(torch.from_numpy(test_adj))\n",
    "    evaluate(pred_matrices, gt_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 88, 100, 128\n",
    "\n",
    "\n",
    "# the following numbers do not reflect the provided dataset, just for an example\n",
    "num_test_samples = 20\n",
    "num_roi = 10\n",
    "\n",
    "# create a random model output \n",
    "pred_matrices = torch.randn(num_test_samples, num_roi, num_roi).numpy()\n",
    "\n",
    "# post-processing\n",
    "pred_matrices[pred_matrices < 0] = 0\n",
    "\n",
    "# create random ground-truth data\n",
    "gt_matrices = torch.randn(num_test_samples, num_roi, num_roi).numpy()\n",
    "\n",
    "# you do NOT need to that since the ground-truth data we provided you is alread pre-processed.\n",
    "gt_matrices[gt_matrices < 0] = 0\n",
    "\n",
    "# Initialize lists to store MAEs for each centrality measure\n",
    "mae_bc = []\n",
    "mae_ec = []\n",
    "mae_pc = []\n",
    "\n",
    "# # Iterate over each test sample\n",
    "# for i in range(num_test_samples):\n",
    "#     # Convert adjacency matrices to NetworkX graphs\n",
    "#     pred_graph = nx.from_numpy_array(pred_matrices[i], edge_attr=\"weight\")\n",
    "#     gt_graph = nx.from_numpy_array(gt_matrices[i], edge_attr=\"weight\")\n",
    "\n",
    "#     # Compute centrality measures\n",
    "#     pred_bc = nx.betweenness_centrality(pred_graph, weight=\"weight\")\n",
    "#     pred_ec = nx.eigenvector_centrality(pred_graph, weight=\"weight\")\n",
    "#     pred_pc = nx.pagerank(pred_graph, weight=\"weight\")\n",
    "\n",
    "#     gt_bc = nx.betweenness_centrality(gt_graph, weight=\"weight\")\n",
    "#     gt_ec = nx.eigenvector_centrality(gt_graph, weight=\"weight\")\n",
    "#     gt_pc = nx.pagerank(gt_graph, weight=\"weight\")\n",
    "\n",
    "#     # Convert centrality dictionaries to lists\n",
    "#     pred_bc_values = list(pred_bc.values())\n",
    "#     pred_ec_values = list(pred_ec.values())\n",
    "#     pred_pc_values = list(pred_pc.values())\n",
    "\n",
    "#     gt_bc_values = list(gt_bc.values())\n",
    "#     gt_ec_values = list(gt_ec.values())\n",
    "#     gt_pc_values = list(gt_pc.values())\n",
    "\n",
    "#     # Compute MAEs\n",
    "#     mae_bc.append(mean_absolute_error(pred_bc_values, gt_bc_values))\n",
    "#     mae_ec.append(mean_absolute_error(pred_ec_values, gt_ec_values))\n",
    "#     mae_pc.append(mean_absolute_error(pred_pc_values, gt_pc_values))\n",
    "\n",
    "# # Compute average MAEs\n",
    "# avg_mae_bc = sum(mae_bc) / len(mae_bc)\n",
    "# avg_mae_ec = sum(mae_ec) / len(mae_ec)\n",
    "# avg_mae_pc = sum(mae_pc) / len(mae_pc)\n",
    "\n",
    "# vectorize and flatten\n",
    "pred_1d = MatrixVectorizer.vectorize(pred_matrices).flatten()\n",
    "gt_1d = MatrixVectorizer.vectorize(gt_matrices).flatten()\n",
    "\n",
    "mae = mean_absolute_error(pred_1d, gt_1d)\n",
    "pcc = pearsonr(pred_1d, gt_1d)[0]\n",
    "js_dis = jensenshannon(pred_1d, gt_1d)\n",
    "\n",
    "print(\"MAE: \", mae)\n",
    "print(\"PCC: \", pcc)\n",
    "print(\"Jensen-Shannon Distance: \", js_dis)\n",
    "print(\"Average MAE betweenness centrality:\", avg_mae_bc)\n",
    "print(\"Average MAE eigenvector centrality:\", avg_mae_ec)\n",
    "print(\"Average MAE PageRank centrality:\", avg_mae_pc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
