{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from MatrixVectorizer import *\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "# Check for CUDA (GPU support) and set device accordingly\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)  # For multi-GPU setups\n",
    "    # Additional settings for ensuring reproducibility on CUDA\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A_LR_train = pd.read_csv(\"../data/lr_train.csv\")\n",
    "# A_HR_train = pd.read_csv(\"../data/hr_train.csv\")\n",
    "# A_LR_test = pd.read_csv(\"../data/lr_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_size = 160\n",
    "HR_size = 268"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MatrixVectorizer = MatrixVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_subject = A_LR_train.shape[0]\n",
    "# A_LR_train_matrix = np.zeros((num_subject, LR_size, LR_size)) #torch.zeros((num_subject, LR_size, LR_size))\n",
    "# for i in range(num_subject):\n",
    "#     A_LR_train_matrix[i] = MatrixVectorizer.anti_vectorize(A_LR_train.iloc[i], LR_size) # torch.from_numpy(MatrixVectorizer.anti_vectorize(A_LR_train.iloc[i], LR_size))\n",
    "\n",
    "# A_HR_train_matrix = np.zeros((num_subject, HR_size, HR_size)) #torch.zeros((num_subject, LR_size, LR_size))\n",
    "# for i in range(num_subject):\n",
    "#     A_HR_train_matrix[i] = MatrixVectorizer.anti_vectorize(A_HR_train.iloc[i], HR_size) \n",
    "\n",
    "# num_subject = len(A_LR_test)\n",
    "# A_LR_test_matrix = np.zeros((num_subject, LR_size, LR_size)) #torch.zeros((num_subject, LR_size, LR_size))\n",
    "# for i in range(num_subject):\n",
    "#     A_LR_test_matrix[i] = MatrixVectorizer.anti_vectorize(A_LR_test.iloc[i], LR_size) \n",
    "\n",
    "# np.save('A_LR_train_matrix.npy', A_LR_train_matrix)\n",
    "# np.save('A_HR_train_matrix.npy', A_HR_train_matrix)\n",
    "# np.save('A_LR_test_matrix.npy', A_LR_test_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(167, 160, 160)\n",
      "(167, 268, 268)\n",
      "(112, 160, 160)\n"
     ]
    }
   ],
   "source": [
    "A_LR_train_matrix = np.load('A_LR_train_matrix.npy')\n",
    "A_HR_train_matrix = np.load('A_HR_train_matrix.npy')\n",
    "A_LR_test_matrix = np.load(\"A_LR_test_matrix.npy\")\n",
    "\n",
    "print(A_LR_train_matrix.shape)\n",
    "print(A_HR_train_matrix.shape)\n",
    "print(A_LR_test_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(epochs=200, lr=0.0001, splits=3, lmbda=16, lr_dim=160, hr_dim=268, hidden_dim=280, padding=26)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Main function of Graph Super-Resolution Network (GSR-Net) framework \n",
    "   for predicting high-resolution brain connectomes from low-resolution connectomes. \n",
    "    \n",
    "    ---------------------------------------------------------------------\n",
    "    \n",
    "    This file contains the implementation of the training and testing process of our GSR-Net model.\n",
    "        train(model, optimizer, subjects_adj, subjects_ground_truth, args)\n",
    "\n",
    "                Inputs:\n",
    "                        model:        constructor of our GSR-Net model:  model = GSRNet(ks,args)\n",
    "                                      ks:   array that stores reduction rates of nodes in Graph U-Net pooling layers\n",
    "                                      args: parsed command line arguments\n",
    "\n",
    "                        optimizer:    constructor of our model's optimizer (borrowed from PyTorch)  \n",
    "\n",
    "                        subjects_adj: (n × l x l) tensor stacking LR connectivity matrices of all training subjects\n",
    "                                       n: the total number of subjects\n",
    "                                       l: the dimensions of the LR connectivity matrices\n",
    "\n",
    "                        subjects_ground_truth: (n × h x h) tensor stacking LR connectivity matrices of all training subjects\n",
    "                                                n: the total number of subjects\n",
    "                                                h: the dimensions of the LR connectivity matrices\n",
    "\n",
    "                        args:          parsed command line arguments, to learn more about the arguments run: \n",
    "                                       python demo.py --help\n",
    "                Output:\n",
    "                        for each epoch, prints out the mean training MSE error\n",
    "\n",
    "\n",
    "            \n",
    "        test(model, test_adj,test_ground_truth,args)\n",
    "\n",
    "                Inputs:\n",
    "                        test_adj:      (n × l x l) tensor stacking LR connectivity matrices of all testing subjects\n",
    "                                        n: the total number of subjects\n",
    "                                        l: the dimensions of the LR connectivity matrices\n",
    "\n",
    "                        test_ground_truth:      (n × h x h) tensor stacking LR connectivity matrices of all testing subjects\n",
    "                                                 n: the total number of subjects\n",
    "                                                 h: the dimensions of the LR connectivity matrices\n",
    "\n",
    "                        see train method above for model and args.\n",
    "\n",
    "                Outputs:\n",
    "                        for each epoch, prints out the mean testing MSE error\n",
    "\n",
    "\n",
    "    To evaluate our framework we used 5-fold cross-validation strategy.\n",
    "\n",
    "    ---------------------------------------------------------------------\n",
    "    Copyright 2020 Megi Isallari, Istanbul Technical University.\n",
    "    All rights reserved.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import KFold\n",
    "from preprocessing import *\n",
    "from model import *\n",
    "from train import *\n",
    "import argparse\n",
    "\n",
    "\n",
    "\n",
    "epochs = 200\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='GSR-Net')\n",
    "parser.add_argument('--epochs', type=int, default=epochs, metavar='no_epochs',\n",
    "                help='number of episode to train ')\n",
    "parser.add_argument('--lr', type=float, default=0.0001, metavar='lr',\n",
    "                help='learning rate (default: 0.0001 using Adam Optimizer)')\n",
    "parser.add_argument('--splits', type=int, default=3, metavar='n_splits',\n",
    "                help='no of cross validation folds')\n",
    "parser.add_argument('--lmbda', type=int, default=16, metavar='L',\n",
    "                help='self-reconstruction error hyperparameter')\n",
    "parser.add_argument('--lr_dim', type=int, default=LR_size, metavar='N',\n",
    "                help='adjacency matrix input dimensions')\n",
    "parser.add_argument('--hr_dim', type=int, default=HR_size, metavar='N',\n",
    "                help='super-resolved adjacency matrix output dimensions')\n",
    "parser.add_argument('--hidden_dim', type=int, default=280, metavar='N',\n",
    "                help='hidden GraphConvolutional layer dimensions')\n",
    "parser.add_argument('--padding', type=int, default=26, metavar='padding',\n",
    "                help='dimensions of padding')\n",
    "\n",
    "# Create an empty Namespace to hold the default arguments\n",
    "args = parser.parse_args([]) \n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(167, 160, 160)\n",
      "(167, 268, 268)\n"
     ]
    }
   ],
   "source": [
    "# SIMULATING THE DATA: EDIT TO ENTER YOUR OWN DATA\n",
    "X = A_LR_train_matrix #np.random.normal(0, 0.5, (167, 160, 160))\n",
    "Y = A_HR_train_matrix #np.random.normal(0, 0.5, (167, 288, 288))\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = get_device()\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: \n",
      "----- Fold 0 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/metissotangkur/anaconda3/lib/python3.10/site-packages/torch/nn/functional.py:2948: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train Loss: -0.150055, Train Error: 0.235145, Test Error: 0.212103\n",
      "Epoch: 1, Train Loss: -0.215454, Train Error: 0.219775, Test Error: 0.209147\n",
      "Epoch: 2, Train Loss: -0.231699, Train Error: 0.216609, Test Error: 0.205205\n",
      "Epoch: 3, Train Loss: -0.242304, Train Error: 0.214259, Test Error: 0.202915\n",
      "Epoch: 4, Train Loss: -0.249648, Train Error: 0.213776, Test Error: 0.202728\n",
      "Epoch: 5, Train Loss: -0.253443, Train Error: 0.214087, Test Error: 0.202829\n",
      "Epoch: 6, Train Loss: -0.255424, Train Error: 0.214455, Test Error: 0.203289\n",
      "Epoch: 7, Train Loss: -0.256933, Train Error: 0.214521, Test Error: 0.203811\n",
      "----- Fold 1 -----\n",
      "Epoch: 0, Train Loss: -0.153071, Train Error: 0.233401, Test Error: 0.215906\n",
      "Epoch: 1, Train Loss: -0.216802, Train Error: 0.216268, Test Error: 0.209841\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 26\u001b[0m\n\u001b[1;32m     18\u001b[0m data_fold_list\u001b[39m.\u001b[39mappend((subjects_adj, test_adj, subjects_ground_truth, test_ground_truth))\n\u001b[1;32m     21\u001b[0m \u001b[39m##################\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[39m# subjects_adj = subjects_adj[:1]\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[39m# subjects_ground_truth = subjects_ground_truth[:1]\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[39m##################\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m return_model \u001b[39m=\u001b[39m train(model, optimizer, subjects_adj, subjects_ground_truth, args, test_adj, test_ground_truth)\n\u001b[1;32m     27\u001b[0m test(return_model, test_adj, test_ground_truth, args)\n\u001b[1;32m     28\u001b[0m best_model_fold_list\u001b[39m.\u001b[39mappend(return_model)\n",
      "File \u001b[0;32m~/Desktop/advanced_computing/deep_graph_learning/graph_super_resolution/gsr_net/train.py:54\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, subjects_adj, subjects_labels, args, test_adj, test_ground_truth)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39m# model_outputs  = unpad(model_outputs, args.padding)\u001b[39;00m\n\u001b[1;32m     53\u001b[0m padded_hr \u001b[39m=\u001b[39m pad_HR_adj(hr,args\u001b[39m.\u001b[39mpadding)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> 54\u001b[0m eig_val_hr, U_hr \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mlinalg\u001b[39m.\u001b[39;49meigh(padded_hr, UPLO\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mU\u001b[39;49m\u001b[39m'\u001b[39;49m) \n\u001b[1;32m     55\u001b[0m \u001b[39m# print(net_outs.size(),start_gcn_outs.size())\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# print(model.layer.weights.size(), U_hr.size())\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m# print(model_outputs.size(), hr.size())\u001b[39;00m\n\u001b[1;32m     59\u001b[0m loss \u001b[39m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m    args\u001b[39m.\u001b[39mlmbda \u001b[39m*\u001b[39m criterion(net_outs, start_gcn_outs) \n\u001b[1;32m     61\u001b[0m    \u001b[39m+\u001b[39m criterion(model\u001b[39m.\u001b[39mlayer\u001b[39m.\u001b[39mweights,U_hr) \n\u001b[1;32m     62\u001b[0m    \u001b[39m+\u001b[39m criterion(model_outputs, hr)\n\u001b[1;32m     63\u001b[0m    \u001b[39m+\u001b[39m kl_loss(model_outputs \u001b[39m+\u001b[39m \u001b[39m1e-6\u001b[39m, hr \u001b[39m+\u001b[39m \u001b[39m1e-6\u001b[39m)\n\u001b[1;32m     64\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cv = KFold(n_splits=args.splits, random_state=42, shuffle=True)\n",
    "print(\"Torch: \")\n",
    "\n",
    "ks = [0.9, 0.7, 0.6, 0.5]\n",
    "\n",
    "best_model_fold_list = []\n",
    "data_fold_list = []\n",
    "i = 0\n",
    "for train_index, test_index in cv.split(X):\n",
    "\n",
    "    print(f\"----- Fold {i} -----\")\n",
    "\n",
    "    model = GSRNet(ks, args).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "    subjects_adj, test_adj, subjects_ground_truth, test_ground_truth = X[\n",
    "        train_index], X[test_index], Y[train_index], Y[test_index]\n",
    "    data_fold_list.append((subjects_adj, test_adj, subjects_ground_truth, test_ground_truth))\n",
    "\n",
    "\n",
    "    ##################\n",
    "    # subjects_adj = subjects_adj[:1]\n",
    "    # subjects_ground_truth = subjects_ground_truth[:1]\n",
    "    ##################\n",
    "\n",
    "    return_model = train(model, optimizer, subjects_adj, subjects_ground_truth, args, test_adj, test_ground_truth)\n",
    "    test(return_model, test_adj, test_ground_truth, args)\n",
    "    best_model_fold_list.append(return_model)\n",
    "\n",
    "    i += 1\n",
    "\n",
    "    # break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MatrixVectorizer import MatrixVectorizer\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "import torch\n",
    "import networkx as nx\n",
    "\n",
    "def evaluate(pred_matrices, gt_matrices):\n",
    "\n",
    "    num_test_samples = gt_matrices.shape[0]\n",
    "\n",
    "    # Initialize lists to store MAEs for each centrality measure\n",
    "    mae_bc = []\n",
    "    mae_ec = []\n",
    "    mae_pc = []\n",
    "\n",
    "    # # Iterate over each test sample\n",
    "    # for i in range(num_test_samples):\n",
    "    #     # Convert adjacency matrices to NetworkX graphs\n",
    "    #     pred_graph = nx.from_numpy_array(pred_matrices[i], edge_attr=\"weight\")\n",
    "    #     gt_graph = nx.from_numpy_array(gt_matrices[i], edge_attr=\"weight\")\n",
    "\n",
    "    #     # Compute centrality measures\n",
    "    #     pred_bc = nx.betweenness_centrality(pred_graph, weight=\"weight\")\n",
    "    #     pred_ec = nx.eigenvector_centrality(pred_graph, weight=\"weight\")\n",
    "    #     pred_pc = nx.pagerank(pred_graph, weight=\"weight\")\n",
    "\n",
    "    #     gt_bc = nx.betweenness_centrality(gt_graph, weight=\"weight\")\n",
    "    #     gt_ec = nx.eigenvector_centrality(gt_graph, weight=\"weight\")\n",
    "    #     gt_pc = nx.pagerank(gt_graph, weight=\"weight\")\n",
    "\n",
    "    #     # Convert centrality dictionaries to lists\n",
    "    #     pred_bc_values = list(pred_bc.values())\n",
    "    #     pred_ec_values = list(pred_ec.values())\n",
    "    #     pred_pc_values = list(pred_pc.values())\n",
    "\n",
    "    #     gt_bc_values = list(gt_bc.values())\n",
    "    #     gt_ec_values = list(gt_ec.values())\n",
    "    #     gt_pc_values = list(gt_pc.values())\n",
    "\n",
    "    #     # Compute MAEs\n",
    "    #     mae_bc.append(mean_absolute_error(pred_bc_values, gt_bc_values))\n",
    "    #     mae_ec.append(mean_absolute_error(pred_ec_values, gt_ec_values))\n",
    "    #     mae_pc.append(mean_absolute_error(pred_pc_values, gt_pc_values))\n",
    "\n",
    "    # # Compute average MAEs\n",
    "    # avg_mae_bc = sum(mae_bc) / len(mae_bc)\n",
    "    # avg_mae_ec = sum(mae_ec) / len(mae_ec)\n",
    "    # avg_mae_pc = sum(mae_pc) / len(mae_pc)\n",
    "\n",
    "    # vectorize and flatten\n",
    "    pred_1d = MatrixVectorizer.vectorize(pred_matrices).flatten()\n",
    "    gt_1d = MatrixVectorizer.vectorize(gt_matrices).flatten()\n",
    "\n",
    "    mae = mean_absolute_error(pred_1d, gt_1d)\n",
    "    pcc = pearsonr(pred_1d, gt_1d)[0]\n",
    "    js_dis = jensenshannon(pred_1d, gt_1d)\n",
    "\n",
    "    print(\"MAE: \", mae)\n",
    "    print(\"PCC: \", pcc)\n",
    "    print(\"Jensen-Shannon Distance: \", js_dis)\n",
    "    # print(\"Average MAE betweenness centrality:\", avg_mae_bc)\n",
    "    # print(\"Average MAE eigenvector centrality:\", avg_mae_ec)\n",
    "    # print(\"Average MAE PageRank centrality:\", avg_mae_pc)\n",
    "    # return mae, pcc, js_dis, avg_mae_bc, avg_mae_ec, avg_mae_pc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  0.14835803751452897\n",
      "PCC:  0.5928507311234573\n",
      "Jensen-Shannon Distance:  0.29957283150860553\n",
      "MAE:  0.1578448729087319\n",
      "PCC:  0.5642614149988106\n",
      "Jensen-Shannon Distance:  0.3078476609803951\n",
      "MAE:  0.15251106211067725\n",
      "PCC:  0.586215228686752\n",
      "Jensen-Shannon Distance:  0.29401755777854227\n"
     ]
    }
   ],
   "source": [
    "for i in range(args.splits):\n",
    "    _, test_adjs, _, gt_matrices = data_fold_list[i]\n",
    "    model = best_model_fold_list[i]\n",
    "    model.eval()\n",
    "    pred_matrices = np.zeros(gt_matrices.shape)\n",
    "    with torch.no_grad():\n",
    "        for j, test_adj in enumerate(test_adjs):\n",
    "            pred_matrices[j], _, _, _ = model(torch.from_numpy(test_adj))\n",
    "    evaluate(pred_matrices, gt_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train Loss: 0.097804, Train Error: 0.223402\n",
      "Epoch: 1, Train Loss: 0.070335, Train Error: 0.194659\n",
      "Epoch: 2, Train Loss: 0.065212, Train Error: 0.189207\n",
      "Epoch: 3, Train Loss: 0.063117, Train Error: 0.187332\n",
      "Epoch: 4, Train Loss: 0.062106, Train Error: 0.186549\n",
      "Epoch: 5, Train Loss: 0.061458, Train Error: 0.186059\n",
      "Epoch: 6, Train Loss: 0.060990, Train Error: 0.185715\n",
      "Epoch: 7, Train Loss: 0.060643, Train Error: 0.185435\n",
      "Epoch: 8, Train Loss: 0.060365, Train Error: 0.185202\n",
      "Epoch: 9, Train Loss: 0.060122, Train Error: 0.184983\n",
      "Epoch: 10, Train Loss: 0.059893, Train Error: 0.184758\n",
      "Epoch: 11, Train Loss: 0.059676, Train Error: 0.184524\n",
      "Epoch: 12, Train Loss: 0.059452, Train Error: 0.184265\n",
      "Epoch: 13, Train Loss: 0.059220, Train Error: 0.183973\n",
      "Epoch: 14, Train Loss: 0.058967, Train Error: 0.183634\n",
      "Epoch: 15, Train Loss: 0.058665, Train Error: 0.183199\n",
      "Epoch: 16, Train Loss: 0.058321, Train Error: 0.182682\n",
      "Epoch: 17, Train Loss: 0.057894, Train Error: 0.182019\n",
      "Epoch: 18, Train Loss: 0.057370, Train Error: 0.181187\n",
      "Epoch: 19, Train Loss: 0.056716, Train Error: 0.180126\n",
      "Epoch: 20, Train Loss: 0.055906, Train Error: 0.178783\n",
      "Epoch: 21, Train Loss: 0.054878, Train Error: 0.177062\n",
      "Epoch: 22, Train Loss: 0.053596, Train Error: 0.174918\n",
      "Epoch: 23, Train Loss: 0.052068, Train Error: 0.172417\n",
      "Epoch: 24, Train Loss: 0.050516, Train Error: 0.169907\n",
      "Epoch: 25, Train Loss: 0.049145, Train Error: 0.167672\n",
      "Epoch: 26, Train Loss: 0.048058, Train Error: 0.165815\n",
      "Epoch: 27, Train Loss: 0.047215, Train Error: 0.164295\n",
      "Epoch: 28, Train Loss: 0.046498, Train Error: 0.162907\n",
      "Epoch: 29, Train Loss: 0.045879, Train Error: 0.161646\n",
      "Epoch: 30, Train Loss: 0.045325, Train Error: 0.160479\n",
      "Epoch: 31, Train Loss: 0.044812, Train Error: 0.159364\n",
      "Epoch: 32, Train Loss: 0.044342, Train Error: 0.158329\n",
      "Epoch: 33, Train Loss: 0.043886, Train Error: 0.157319\n",
      "Epoch: 34, Train Loss: 0.043471, Train Error: 0.156380\n",
      "Epoch: 35, Train Loss: 0.043059, Train Error: 0.155437\n",
      "Epoch: 36, Train Loss: 0.042681, Train Error: 0.154559\n",
      "Epoch: 37, Train Loss: 0.042309, Train Error: 0.153690\n",
      "Epoch: 38, Train Loss: 0.041981, Train Error: 0.152917\n",
      "Epoch: 39, Train Loss: 0.041649, Train Error: 0.152132\n",
      "Epoch: 40, Train Loss: 0.041333, Train Error: 0.151367\n",
      "Epoch: 41, Train Loss: 0.041045, Train Error: 0.150674\n",
      "Epoch: 42, Train Loss: 0.040748, Train Error: 0.149975\n",
      "Epoch: 43, Train Loss: 0.040482, Train Error: 0.149328\n",
      "Epoch: 44, Train Loss: 0.040185, Train Error: 0.148602\n",
      "Epoch: 45, Train Loss: 0.039957, Train Error: 0.148061\n",
      "Epoch: 46, Train Loss: 0.039695, Train Error: 0.147422\n",
      "Epoch: 47, Train Loss: 0.039481, Train Error: 0.146912\n",
      "Epoch: 48, Train Loss: 0.039268, Train Error: 0.146380\n",
      "Epoch: 49, Train Loss: 0.039071, Train Error: 0.145922\n",
      "Epoch: 50, Train Loss: 0.038849, Train Error: 0.145356\n",
      "Epoch: 51, Train Loss: 0.038657, Train Error: 0.144903\n",
      "Epoch: 52, Train Loss: 0.038447, Train Error: 0.144378\n",
      "Epoch: 53, Train Loss: 0.038277, Train Error: 0.143968\n",
      "Epoch: 54, Train Loss: 0.038070, Train Error: 0.143462\n",
      "Epoch: 55, Train Loss: 0.037921, Train Error: 0.143112\n",
      "Epoch: 56, Train Loss: 0.037730, Train Error: 0.142646\n",
      "Epoch: 57, Train Loss: 0.037564, Train Error: 0.142243\n",
      "Epoch: 58, Train Loss: 0.037366, Train Error: 0.141729\n",
      "Epoch: 59, Train Loss: 0.037263, Train Error: 0.141512\n",
      "Epoch: 60, Train Loss: 0.037097, Train Error: 0.141060\n",
      "Epoch: 61, Train Loss: 0.037038, Train Error: 0.140939\n",
      "Epoch: 62, Train Loss: 0.036832, Train Error: 0.140440\n",
      "Epoch: 63, Train Loss: 0.036678, Train Error: 0.140070\n",
      "Epoch: 64, Train Loss: 0.036541, Train Error: 0.139728\n",
      "Epoch: 65, Train Loss: 0.036374, Train Error: 0.139326\n",
      "Epoch: 66, Train Loss: 0.036263, Train Error: 0.139087\n",
      "Epoch: 67, Train Loss: 0.036080, Train Error: 0.138640\n",
      "Epoch: 68, Train Loss: 0.035971, Train Error: 0.138448\n",
      "Epoch: 69, Train Loss: 0.035800, Train Error: 0.138040\n",
      "Epoch: 70, Train Loss: 0.035712, Train Error: 0.137878\n",
      "Epoch: 71, Train Loss: 0.035558, Train Error: 0.137537\n",
      "Epoch: 72, Train Loss: 0.035443, Train Error: 0.137303\n",
      "Epoch: 73, Train Loss: 0.035295, Train Error: 0.136983\n",
      "Epoch: 74, Train Loss: 0.035185, Train Error: 0.136745\n",
      "Epoch: 75, Train Loss: 0.035048, Train Error: 0.136467\n",
      "Epoch: 76, Train Loss: 0.034937, Train Error: 0.136240\n",
      "Epoch: 77, Train Loss: 0.034795, Train Error: 0.135942\n",
      "Epoch: 78, Train Loss: 0.034684, Train Error: 0.135692\n",
      "Epoch: 79, Train Loss: 0.034561, Train Error: 0.135446\n",
      "Epoch: 80, Train Loss: 0.034427, Train Error: 0.135140\n",
      "Epoch: 81, Train Loss: 0.034318, Train Error: 0.134917\n",
      "Epoch: 82, Train Loss: 0.034196, Train Error: 0.134655\n",
      "Epoch: 83, Train Loss: 0.034073, Train Error: 0.134395\n",
      "Epoch: 84, Train Loss: 0.033982, Train Error: 0.134197\n",
      "Epoch: 85, Train Loss: 0.033870, Train Error: 0.133948\n",
      "Epoch: 86, Train Loss: 0.033789, Train Error: 0.133763\n",
      "Epoch: 87, Train Loss: 0.033698, Train Error: 0.133577\n",
      "Epoch: 88, Train Loss: 0.033630, Train Error: 0.133420\n",
      "Epoch: 89, Train Loss: 0.033535, Train Error: 0.133193\n"
     ]
    }
   ],
   "source": [
    "args.epochs = 90\n",
    "\n",
    "final_model = GSRNet(ks, args).to(device)\n",
    "optimizer = optim.Adam(final_model.parameters(), lr=args.lr)\n",
    "\n",
    "# subjects_adj, test_adj, subjects_ground_truth, test_ground_truth = X[\n",
    "#     train_index], X[test_index], Y[train_index], Y[test_index]\n",
    "# data_fold_list.append((subjects_adj, test_adj, subjects_ground_truth, test_ground_truth))\n",
    "\n",
    "\n",
    "##################\n",
    "# subjects_adj = subjects_adj[:1]\n",
    "# subjects_ground_truth = subjects_ground_truth[:1]\n",
    "##################\n",
    "\n",
    "final_model = train(final_model, optimizer, X, Y, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_pred_list = []\n",
    "final_model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(A_LR_test_matrix.shape[0]):\n",
    "        output_pred, _, _, _ = final_model(torch.Tensor(A_LR_test_matrix[i]))\n",
    "        output_pred = MatrixVectorizer.vectorize(output_pred).tolist()\n",
    "        output_pred_list.append(output_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4007136,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_pred_stack = np.stack(output_pred_list, axis=0)\n",
    "output_pred_1d = output_pred_stack.flatten()\n",
    "output_pred_1d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.281961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.517760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.613025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.391788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.382644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4007131</th>\n",
       "      <td>4007132</td>\n",
       "      <td>0.096619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4007132</th>\n",
       "      <td>4007133</td>\n",
       "      <td>0.077400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4007133</th>\n",
       "      <td>4007134</td>\n",
       "      <td>0.215894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4007134</th>\n",
       "      <td>4007135</td>\n",
       "      <td>0.309871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4007135</th>\n",
       "      <td>4007136</td>\n",
       "      <td>0.251942</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4007136 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID  Predicted\n",
       "0              1   0.281961\n",
       "1              2   0.517760\n",
       "2              3   0.613025\n",
       "3              4   0.391788\n",
       "4              5   0.382644\n",
       "...          ...        ...\n",
       "4007131  4007132   0.096619\n",
       "4007132  4007133   0.077400\n",
       "4007133  4007134   0.215894\n",
       "4007134  4007135   0.309871\n",
       "4007135  4007136   0.251942\n",
       "\n",
       "[4007136 rows x 2 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"ID\": [i+1 for i in range(len(output_pred_1d))],\n",
    "    \"Predicted\": output_pred_1d.tolist()\n",
    "})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
