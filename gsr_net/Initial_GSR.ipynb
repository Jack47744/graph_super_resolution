{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from MatrixVectorizer import *\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA not available. Using CPU.\n"
     ]
    }
   ],
   "source": [
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "# Check for CUDA (GPU support) and set device accordingly\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)  # For multi-GPU setups\n",
    "    # Additional settings for ensuring reproducibility on CUDA\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A_LR_train = pd.read_csv(\"../data/lr_train.csv\")\n",
    "# A_HR_train = pd.read_csv(\"../data/hr_train.csv\")\n",
    "# A_LR_test = pd.read_csv(\"../data/lr_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_size = 160\n",
    "HR_size = 268"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "MatrixVectorizer = MatrixVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_subject = A_LR_train.shape[0]\n",
    "# A_LR_train_matrix = np.zeros((num_subject, LR_size, LR_size)) #torch.zeros((num_subject, LR_size, LR_size))\n",
    "# for i in range(num_subject):\n",
    "#     A_LR_train_matrix[i] = MatrixVectorizer.anti_vectorize(A_LR_train.iloc[i], LR_size) # torch.from_numpy(MatrixVectorizer.anti_vectorize(A_LR_train.iloc[i], LR_size))\n",
    "\n",
    "# A_HR_train_matrix = np.zeros((num_subject, HR_size, HR_size)) #torch.zeros((num_subject, LR_size, LR_size))\n",
    "# for i in range(num_subject):\n",
    "#     A_HR_train_matrix[i] = MatrixVectorizer.anti_vectorize(A_HR_train.iloc[i], HR_size) \n",
    "\n",
    "# num_subject = len(A_LR_test)\n",
    "# A_LR_test_matrix = np.zeros((num_subject, LR_size, LR_size)) #torch.zeros((num_subject, LR_size, LR_size))\n",
    "# for i in range(num_subject):\n",
    "#     A_LR_test_matrix[i] = MatrixVectorizer.anti_vectorize(A_LR_test.iloc[i], LR_size) \n",
    "\n",
    "# np.save('A_LR_train_matrix.npy', A_LR_train_matrix)\n",
    "# np.save('A_HR_train_matrix.npy', A_HR_train_matrix)\n",
    "# np.save('A_LR_test_matrix.npy', A_LR_test_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(167, 160, 160)\n",
      "(167, 268, 268)\n",
      "(112, 160, 160)\n"
     ]
    }
   ],
   "source": [
    "A_LR_train_matrix = np.load('A_LR_train_matrix.npy')\n",
    "A_HR_train_matrix = np.load('A_HR_train_matrix.npy')\n",
    "A_LR_test_matrix = np.load(\"A_LR_test_matrix.npy\")\n",
    "\n",
    "print(A_LR_train_matrix.shape)\n",
    "print(A_HR_train_matrix.shape)\n",
    "print(A_LR_test_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(epochs=200, lr=0.0001, splits=3, lmbda=16, lr_dim=160, hr_dim=268, hidden_dim=280, padding=26, embedding_size=32, early_stop_patient=3)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Main function of Graph Super-Resolution Network (GSR-Net) framework \n",
    "   for predicting high-resolution brain connectomes from low-resolution connectomes. \n",
    "    \n",
    "    ---------------------------------------------------------------------\n",
    "    \n",
    "    This file contains the implementation of the training and testing process of our GSR-Net model.\n",
    "        train(model, optimizer, subjects_adj, subjects_ground_truth, args)\n",
    "\n",
    "                Inputs:\n",
    "                        model:        constructor of our GSR-Net model:  model = GSRNet(ks,args)\n",
    "                                      ks:   array that stores reduction rates of nodes in Graph U-Net pooling layers\n",
    "                                      args: parsed command line arguments\n",
    "\n",
    "                        optimizer:    constructor of our model's optimizer (borrowed from PyTorch)  \n",
    "\n",
    "                        subjects_adj: (n × l x l) tensor stacking LR connectivity matrices of all training subjects\n",
    "                                       n: the total number of subjects\n",
    "                                       l: the dimensions of the LR connectivity matrices\n",
    "\n",
    "                        subjects_ground_truth: (n × h x h) tensor stacking LR connectivity matrices of all training subjects\n",
    "                                                n: the total number of subjects\n",
    "                                                h: the dimensions of the LR connectivity matrices\n",
    "\n",
    "                        args:          parsed command line arguments, to learn more about the arguments run: \n",
    "                                       python demo.py --help\n",
    "                Output:\n",
    "                        for each epoch, prints out the mean training MSE error\n",
    "\n",
    "\n",
    "            \n",
    "        test(model, test_adj,test_ground_truth,args)\n",
    "\n",
    "                Inputs:\n",
    "                        test_adj:      (n × l x l) tensor stacking LR connectivity matrices of all testing subjects\n",
    "                                        n: the total number of subjects\n",
    "                                        l: the dimensions of the LR connectivity matrices\n",
    "\n",
    "                        test_ground_truth:      (n × h x h) tensor stacking LR connectivity matrices of all testing subjects\n",
    "                                                 n: the total number of subjects\n",
    "                                                 h: the dimensions of the LR connectivity matrices\n",
    "\n",
    "                        see train method above for model and args.\n",
    "\n",
    "                Outputs:\n",
    "                        for each epoch, prints out the mean testing MSE error\n",
    "\n",
    "\n",
    "    To evaluate our framework we used 5-fold cross-validation strategy.\n",
    "\n",
    "    ---------------------------------------------------------------------\n",
    "    Copyright 2020 Megi Isallari, Istanbul Technical University.\n",
    "    All rights reserved.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import KFold\n",
    "from preprocessing import *\n",
    "from model import *\n",
    "from train import *\n",
    "import argparse\n",
    "\n",
    "\n",
    "\n",
    "epochs = 200\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='GSR-Net')\n",
    "parser.add_argument('--epochs', type=int, default=epochs, metavar='no_epochs',\n",
    "                help='number of episode to train ')\n",
    "parser.add_argument('--lr', type=float, default=0.0001, metavar='lr',\n",
    "                help='learning rate (default: 0.0001 using Adam Optimizer)')\n",
    "parser.add_argument('--splits', type=int, default=3, metavar='n_splits',\n",
    "                help='no of cross validation folds')\n",
    "parser.add_argument('--lmbda', type=int, default=16, metavar='L',\n",
    "                help='self-reconstruction error hyperparameter')\n",
    "parser.add_argument('--lr_dim', type=int, default=LR_size, metavar='N',\n",
    "                help='adjacency matrix input dimensions')\n",
    "parser.add_argument('--hr_dim', type=int, default=HR_size, metavar='N',\n",
    "                help='super-resolved adjacency matrix output dimensions')\n",
    "parser.add_argument('--hidden_dim', type=int, default=280, metavar='N',\n",
    "                help='hidden GraphConvolutional layer dimensions')\n",
    "parser.add_argument('--padding', type=int, default=26, metavar='padding',\n",
    "                help='dimensions of padding')\n",
    "parser.add_argument('--embedding_size', type=int, default=32, metavar='embedding_size',\n",
    "                help='node embedding size')\n",
    "parser.add_argument('--early_stop_patient', type=int, default=3, metavar='early_stop_patient',\n",
    "                help='early_stop_patience')\n",
    "\n",
    "\n",
    "\n",
    "# Create an empty Namespace to hold the default arguments\n",
    "args = parser.parse_args([]) \n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(167, 160, 160)\n",
      "(167, 268, 268)\n"
     ]
    }
   ],
   "source": [
    "# SIMULATING THE DATA: EDIT TO ENTER YOUR OWN DATA\n",
    "X = A_LR_train_matrix #np.random.normal(0, 0.5, (167, 160, 160))\n",
    "Y = A_HR_train_matrix #np.random.normal(0, 0.5, (167, 288, 288))\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = get_device()\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(167, 160, 160)\n"
     ]
    }
   ],
   "source": [
    "def compute_degree_matrix_normalization_batch_numpy(adjacency_batch):\n",
    "    \"\"\"\n",
    "    Optimizes the degree matrix normalization for a batch of adjacency matrices using NumPy.\n",
    "    Computes the normalized adjacency matrix D^-1 * A for each graph in the batch.\n",
    "    \n",
    "    Parameters:\n",
    "    - adjacency_batch: A NumPy array of shape (batch_size, num_nodes, num_nodes) representing\n",
    "                       a batch of adjacency matrices.\n",
    "\n",
    "    Returns:\n",
    "    - A NumPy array of normalized adjacency matrices.\n",
    "    \"\"\"\n",
    "    epsilon = 1e-6  # Small constant to avoid division by zero\n",
    "    # Calculate the degree for each node in the batch\n",
    "    d = adjacency_batch.sum(axis=2) + epsilon\n",
    "    \n",
    "    # Compute the inverse degree matrix D^-1 for the batch\n",
    "    D_inv = np.reciprocal(d)[:, :, np.newaxis] * np.eye(adjacency_batch.shape[1])[np.newaxis, :, :]\n",
    "    \n",
    "    # Normalize the adjacency matrix using batch matrix multiplication\n",
    "    normalized_adjacency_batch = np.matmul(D_inv, adjacency_batch)\n",
    "    \n",
    "    return normalized_adjacency_batch\n",
    "X = compute_degree_matrix_normalization_batch_numpy(X)\n",
    "A_LR_test_matrix = compute_degree_matrix_normalization_batch_numpy(A_LR_test_matrix)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Fold 1 -----\n",
      "Epoch: 1, Train Loss: 0.107470, Train Error: 0.244110, Test Error: 0.229523\n",
      "Epoch: 2, Train Loss: 0.097290, Train Error: 0.232959, Test Error: 0.217644\n",
      "Epoch: 3, Train Loss: 0.086692, Train Error: 0.220732, Test Error: 0.205058\n",
      "Epoch: 4, Train Loss: 0.076348, Train Error: 0.208222, Test Error: 0.192642\n",
      "Epoch: 5, Train Loss: 0.067046, Train Error: 0.196436, Test Error: 0.181423\n",
      "Epoch: 6, Train Loss: 0.059265, Train Error: 0.186093, Test Error: 0.171872\n",
      "Epoch: 7, Train Loss: 0.053066, Train Error: 0.177438, Test Error: 0.164049\n",
      "Epoch: 8, Train Loss: 0.048301, Train Error: 0.170447, Test Error: 0.157831\n",
      "Epoch: 9, Train Loss: 0.044718, Train Error: 0.164924, Test Error: 0.152984\n",
      "Epoch: 10, Train Loss: 0.042065, Train Error: 0.160641, Test Error: 0.149269\n",
      "Epoch: 11, Train Loss: 0.040120, Train Error: 0.157362, Test Error: 0.146459\n",
      "Epoch: 12, Train Loss: 0.038702, Train Error: 0.154878, Test Error: 0.144352\n",
      "Epoch: 13, Train Loss: 0.037669, Train Error: 0.153006, Test Error: 0.142776\n",
      "Epoch: 14, Train Loss: 0.036913, Train Error: 0.151592, Test Error: 0.141596\n",
      "Epoch: 15, Train Loss: 0.036355, Train Error: 0.150519, Test Error: 0.140702\n",
      "Epoch: 16, Train Loss: 0.035936, Train Error: 0.149692, Test Error: 0.140012\n",
      "Epoch: 17, Train Loss: 0.035608, Train Error: 0.149025, Test Error: 0.139450\n",
      "Epoch: 18, Train Loss: 0.035325, Train Error: 0.148424, Test Error: 0.138925\n",
      "Epoch: 19, Train Loss: 0.035034, Train Error: 0.147767, Test Error: 0.138362\n",
      "Epoch: 20, Train Loss: 0.034719, Train Error: 0.146996, Test Error: 0.137940\n",
      "Epoch: 21, Train Loss: 0.034364, Train Error: 0.146114, Test Error: 0.137688\n",
      "Epoch: 22, Train Loss: 0.034061, Train Error: 0.145301, Test Error: 0.137430\n",
      "Epoch: 23, Train Loss: 0.033677, Train Error: 0.144343, Test Error: 0.137251\n",
      "Epoch: 24, Train Loss: 0.033301, Train Error: 0.143407, Test Error: 0.137059\n",
      "Epoch: 25, Train Loss: 0.032976, Train Error: 0.142619, Test Error: 0.136998\n",
      "Epoch: 26, Train Loss: 0.032567, Train Error: 0.141528, Test Error: 0.137086\n",
      "Epoch: 27, Train Loss: 0.032194, Train Error: 0.140505, Test Error: 0.137241\n",
      "Epoch: 28, Train Loss: 0.031880, Train Error: 0.139653, Test Error: 0.137414\n",
      "Val Error: 0.136998\n",
      "Val MAE: 0.1369979569156255\n",
      "----- Fold 2 -----\n",
      "Epoch: 1, Train Loss: 0.103771, Train Error: 0.240730, Test Error: 0.235511\n",
      "Epoch: 2, Train Loss: 0.093522, Train Error: 0.229209, Test Error: 0.223935\n",
      "Epoch: 3, Train Loss: 0.082905, Train Error: 0.216624, Test Error: 0.211516\n",
      "Epoch: 4, Train Loss: 0.072614, Train Error: 0.203782, Test Error: 0.199390\n",
      "Epoch: 5, Train Loss: 0.063420, Train Error: 0.191710, Test Error: 0.188434\n",
      "Epoch: 6, Train Loss: 0.055792, Train Error: 0.181166, Test Error: 0.179192\n",
      "Epoch: 7, Train Loss: 0.049785, Train Error: 0.172411, Test Error: 0.171699\n",
      "Epoch: 8, Train Loss: 0.045203, Train Error: 0.165371, Test Error: 0.165776\n",
      "Epoch: 9, Train Loss: 0.041779, Train Error: 0.159828, Test Error: 0.161178\n",
      "Epoch: 10, Train Loss: 0.039254, Train Error: 0.155539, Test Error: 0.157662\n",
      "Epoch: 11, Train Loss: 0.037410, Train Error: 0.152261, Test Error: 0.155002\n",
      "Epoch: 12, Train Loss: 0.036069, Train Error: 0.149781, Test Error: 0.153002\n",
      "Epoch: 13, Train Loss: 0.035095, Train Error: 0.147914, Test Error: 0.151505\n",
      "Epoch: 14, Train Loss: 0.034383, Train Error: 0.146510, Test Error: 0.150380\n",
      "Epoch: 15, Train Loss: 0.033860, Train Error: 0.145447, Test Error: 0.149531\n",
      "Epoch: 16, Train Loss: 0.033469, Train Error: 0.144633, Test Error: 0.148880\n",
      "Epoch: 17, Train Loss: 0.033168, Train Error: 0.143992, Test Error: 0.148362\n",
      "Epoch: 18, Train Loss: 0.032922, Train Error: 0.143448, Test Error: 0.147920\n",
      "Epoch: 19, Train Loss: 0.032691, Train Error: 0.142905, Test Error: 0.147523\n",
      "Epoch: 20, Train Loss: 0.032447, Train Error: 0.142276, Test Error: 0.147256\n",
      "Epoch: 21, Train Loss: 0.032189, Train Error: 0.141599, Test Error: 0.147305\n",
      "Epoch: 22, Train Loss: 0.031951, Train Error: 0.140997, Test Error: 0.146966\n",
      "Epoch: 23, Train Loss: 0.031718, Train Error: 0.140406, Test Error: 0.146336\n",
      "Epoch: 24, Train Loss: 0.031474, Train Error: 0.139768, Test Error: 0.145721\n",
      "Epoch: 25, Train Loss: 0.031255, Train Error: 0.139113, Test Error: 0.145506\n",
      "Epoch: 26, Train Loss: 0.030885, Train Error: 0.138111, Test Error: 0.145398\n",
      "Epoch: 27, Train Loss: 0.030571, Train Error: 0.137203, Test Error: 0.145284\n",
      "Epoch: 28, Train Loss: 0.030330, Train Error: 0.136492, Test Error: 0.145419\n",
      "Epoch: 29, Train Loss: 0.030008, Train Error: 0.135630, Test Error: 0.145600\n",
      "Epoch: 30, Train Loss: 0.029868, Train Error: 0.135181, Test Error: 0.145899\n",
      "Val Error: 0.145284\n",
      "Val MAE: 0.14528362200196301\n",
      "----- Fold 3 -----\n",
      "Epoch: 1, Train Loss: 0.105285, Train Error: 0.241794, Test Error: 0.233567\n",
      "Epoch: 2, Train Loss: 0.095024, Train Error: 0.230436, Test Error: 0.221595\n",
      "Epoch: 3, Train Loss: 0.084390, Train Error: 0.218048, Test Error: 0.208838\n",
      "Epoch: 4, Train Loss: 0.074037, Train Error: 0.205367, Test Error: 0.196267\n",
      "Epoch: 5, Train Loss: 0.064788, Train Error: 0.193455, Test Error: 0.184897\n",
      "Epoch: 6, Train Loss: 0.057126, Train Error: 0.183065, Test Error: 0.175254\n",
      "Epoch: 7, Train Loss: 0.051117, Train Error: 0.174483, Test Error: 0.167423\n",
      "Epoch: 8, Train Loss: 0.046562, Train Error: 0.167636, Test Error: 0.161244\n",
      "Epoch: 9, Train Loss: 0.043177, Train Error: 0.162284, Test Error: 0.156450\n",
      "Epoch: 10, Train Loss: 0.040691, Train Error: 0.158169, Test Error: 0.152780\n",
      "Epoch: 11, Train Loss: 0.038880, Train Error: 0.155040, Test Error: 0.149999\n",
      "Epoch: 12, Train Loss: 0.037563, Train Error: 0.152676, Test Error: 0.147902\n",
      "Epoch: 13, Train Loss: 0.036604, Train Error: 0.150895, Test Error: 0.146321\n",
      "Epoch: 14, Train Loss: 0.035902, Train Error: 0.149550, Test Error: 0.145127\n",
      "Epoch: 15, Train Loss: 0.035381, Train Error: 0.148524, Test Error: 0.144217\n",
      "Epoch: 16, Train Loss: 0.034985, Train Error: 0.147719, Test Error: 0.143510\n",
      "Epoch: 17, Train Loss: 0.034659, Train Error: 0.147035, Test Error: 0.142932\n",
      "Epoch: 18, Train Loss: 0.034350, Train Error: 0.146343, Test Error: 0.142421\n",
      "Epoch: 19, Train Loss: 0.034023, Train Error: 0.145564, Test Error: 0.141978\n",
      "Epoch: 20, Train Loss: 0.033681, Train Error: 0.144723, Test Error: 0.141613\n",
      "Epoch: 21, Train Loss: 0.033351, Train Error: 0.143879, Test Error: 0.141394\n",
      "Epoch: 22, Train Loss: 0.033035, Train Error: 0.143063, Test Error: 0.141505\n",
      "Epoch: 23, Train Loss: 0.032761, Train Error: 0.142354, Test Error: 0.141359\n",
      "Epoch: 24, Train Loss: 0.032361, Train Error: 0.141355, Test Error: 0.141342\n",
      "Epoch: 25, Train Loss: 0.032017, Train Error: 0.140438, Test Error: 0.141367\n",
      "Epoch: 26, Train Loss: 0.031690, Train Error: 0.139534, Test Error: 0.141421\n",
      "Epoch: 27, Train Loss: 0.031381, Train Error: 0.138682, Test Error: 0.141484\n",
      "Val Error: 0.141342\n",
      "Val MAE: 0.14134228920394723\n"
     ]
    }
   ],
   "source": [
    "cv = KFold(n_splits=args.splits, random_state=42, shuffle=True)\n",
    "\n",
    "ks = [0.9, 0.7, 0.6, 0.5]\n",
    "\n",
    "best_model_fold_list = []\n",
    "data_fold_list = []\n",
    "i = 1\n",
    "for train_index, test_index in cv.split(X):\n",
    "\n",
    "    print(f\"----- Fold {i} -----\")\n",
    "\n",
    "\n",
    "    netG = GSRNet(ks, args).to(device)\n",
    "    optimizerG = optim.Adam(netG.parameters(), lr=args.lr)\n",
    "\n",
    "    netD = Discriminator(input_dim=args.embedding_size, hidden_sizes=[16, 8]).to(device)\n",
    "    optimizerD = optim.Adam(netD.parameters(), lr=args.lr)\n",
    "\n",
    "    subjects_adj, test_adj, subjects_ground_truth, test_ground_truth = X[\n",
    "        train_index], X[test_index], Y[train_index], Y[test_index]\n",
    "    data_fold_list.append((subjects_adj, test_adj, subjects_ground_truth, test_ground_truth))\n",
    "\n",
    "\n",
    "    ##################\n",
    "    # subjects_adj = subjects_adj[:1]\n",
    "    # subjects_ground_truth = subjects_ground_truth[:1]\n",
    "    ##################\n",
    "\n",
    "    # return_model = train_gan(\n",
    "    #     netG, \n",
    "    #     optimizerG, \n",
    "    #     netD,\n",
    "    #     optimizerD,\n",
    "    #     subjects_adj, \n",
    "    #     subjects_ground_truth, \n",
    "    #     args, \n",
    "    #     test_adj=test_adj, \n",
    "    #     test_ground_truth=test_ground_truth\n",
    "    # )\n",
    "\n",
    "    return_model = train(netG, optimizerG, subjects_adj, subjects_ground_truth, args, test_adj, test_ground_truth)\n",
    "    test_mae = test(return_model, test_adj, test_ground_truth, args)\n",
    "    print(f\"Val MAE: {test_mae}\")\n",
    "    best_model_fold_list.append(return_model)\n",
    "\n",
    "    i += 1\n",
    "\n",
    "    # break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MatrixVectorizer import MatrixVectorizer\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "import torch\n",
    "import networkx as nx\n",
    "\n",
    "def evaluate(pred_matrices, gt_matrices):\n",
    "\n",
    "    num_test_samples = gt_matrices.shape[0]\n",
    "\n",
    "    # Initialize lists to store MAEs for each centrality measure\n",
    "    mae_bc = []\n",
    "    mae_ec = []\n",
    "    mae_pc = []\n",
    "\n",
    "    # # Iterate over each test sample\n",
    "    # for i in range(num_test_samples):\n",
    "    #     # Convert adjacency matrices to NetworkX graphs\n",
    "    #     pred_graph = nx.from_numpy_array(pred_matrices[i], edge_attr=\"weight\")\n",
    "    #     gt_graph = nx.from_numpy_array(gt_matrices[i], edge_attr=\"weight\")\n",
    "\n",
    "    #     # Compute centrality measures\n",
    "    #     pred_bc = nx.betweenness_centrality(pred_graph, weight=\"weight\")\n",
    "    #     pred_ec = nx.eigenvector_centrality(pred_graph, weight=\"weight\")\n",
    "    #     pred_pc = nx.pagerank(pred_graph, weight=\"weight\")\n",
    "\n",
    "    #     gt_bc = nx.betweenness_centrality(gt_graph, weight=\"weight\")\n",
    "    #     gt_ec = nx.eigenvector_centrality(gt_graph, weight=\"weight\")\n",
    "    #     gt_pc = nx.pagerank(gt_graph, weight=\"weight\")\n",
    "\n",
    "    #     # Convert centrality dictionaries to lists\n",
    "    #     pred_bc_values = list(pred_bc.values())\n",
    "    #     pred_ec_values = list(pred_ec.values())\n",
    "    #     pred_pc_values = list(pred_pc.values())\n",
    "\n",
    "    #     gt_bc_values = list(gt_bc.values())\n",
    "    #     gt_ec_values = list(gt_ec.values())\n",
    "    #     gt_pc_values = list(gt_pc.values())\n",
    "\n",
    "    #     # Compute MAEs\n",
    "    #     mae_bc.append(mean_absolute_error(pred_bc_values, gt_bc_values))\n",
    "    #     mae_ec.append(mean_absolute_error(pred_ec_values, gt_ec_values))\n",
    "    #     mae_pc.append(mean_absolute_error(pred_pc_values, gt_pc_values))\n",
    "\n",
    "    # # Compute average MAEs\n",
    "    # avg_mae_bc = sum(mae_bc) / len(mae_bc)\n",
    "    # avg_mae_ec = sum(mae_ec) / len(mae_ec)\n",
    "    # avg_mae_pc = sum(mae_pc) / len(mae_pc)\n",
    "\n",
    "    # vectorize and flatten\n",
    "    pred_1d = MatrixVectorizer.vectorize(pred_matrices).flatten()\n",
    "    gt_1d = MatrixVectorizer.vectorize(gt_matrices).flatten()\n",
    "\n",
    "    mae = mean_absolute_error(pred_1d, gt_1d)\n",
    "    pcc = pearsonr(pred_1d, gt_1d)[0]\n",
    "    js_dis = jensenshannon(pred_1d, gt_1d)\n",
    "\n",
    "    print(\"MAE: \", mae)\n",
    "    print(\"PCC: \", pcc)\n",
    "    print(\"Jensen-Shannon Distance: \", js_dis)\n",
    "    # print(\"Average MAE betweenness centrality:\", avg_mae_bc)\n",
    "    # print(\"Average MAE eigenvector centrality:\", avg_mae_ec)\n",
    "    # print(\"Average MAE PageRank centrality:\", avg_mae_pc)\n",
    "    # return mae, pcc, js_dis, avg_mae_bc, avg_mae_ec, avg_mae_pc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  0.1390555265969765\n",
      "PCC:  0.6648184331883957\n",
      "Jensen-Shannon Distance:  0.2866054335981063\n",
      "MAE:  0.14923091565433497\n",
      "PCC:  0.6250675749208241\n",
      "Jensen-Shannon Distance:  0.29624882139917336\n",
      "MAE:  0.14600203846260335\n",
      "PCC:  0.6388454911520622\n",
      "Jensen-Shannon Distance:  0.2851355655471688\n"
     ]
    }
   ],
   "source": [
    "for i in range(args.splits):\n",
    "    _, test_adjs, _, gt_matrices = data_fold_list[i]\n",
    "    model = best_model_fold_list[i]\n",
    "    model.eval()\n",
    "    pred_matrices = np.zeros(gt_matrices.shape)\n",
    "    with torch.no_grad():\n",
    "        for j, test_adj in enumerate(test_adjs):\n",
    "            pred_matrices[j], _, _, _ = model(torch.from_numpy(test_adj))\n",
    "    evaluate(pred_matrices, gt_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args.epochs = 90\n",
    "\n",
    "# final_model = GSRNet(ks, args).to(device)\n",
    "# optimizer = optim.Adam(final_model.parameters(), lr=args.lr)\n",
    "\n",
    "# # subjects_adj, test_adj, subjects_ground_truth, test_ground_truth = X[\n",
    "# #     train_index], X[test_index], Y[train_index], Y[test_index]\n",
    "# # data_fold_list.append((subjects_adj, test_adj, subjects_ground_truth, test_ground_truth))\n",
    "\n",
    "\n",
    "# ##################\n",
    "# # subjects_adj = subjects_adj[:1]\n",
    "# # subjects_ground_truth = subjects_ground_truth[:1]\n",
    "# ##################\n",
    "\n",
    "# final_model = train(final_model, optimizer, X, Y, args)\n",
    "\n",
    "final_model = best_model_fold_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_pred_list = []\n",
    "final_model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(A_LR_test_matrix.shape[0]):\n",
    "        output_pred, _, _, _ = final_model(torch.Tensor(A_LR_test_matrix[i]))\n",
    "        output_pred = MatrixVectorizer.vectorize(output_pred).tolist()\n",
    "        output_pred_list.append(output_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_pred_stack = np.stack(output_pred_list, axis=0)\n",
    "output_pred_1d = output_pred_stack.flatten()\n",
    "assert output_pred_1d.shape == (4007136, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.532314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.519469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.662688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.578524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.641651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4007131</th>\n",
       "      <td>4007132</td>\n",
       "      <td>0.105268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4007132</th>\n",
       "      <td>4007133</td>\n",
       "      <td>0.101079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4007133</th>\n",
       "      <td>4007134</td>\n",
       "      <td>0.258134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4007134</th>\n",
       "      <td>4007135</td>\n",
       "      <td>0.346373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4007135</th>\n",
       "      <td>4007136</td>\n",
       "      <td>0.560653</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4007136 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID  Predicted\n",
       "0              1   0.532314\n",
       "1              2   0.519469\n",
       "2              3   0.662688\n",
       "3              4   0.578524\n",
       "4              5   0.641651\n",
       "...          ...        ...\n",
       "4007131  4007132   0.105268\n",
       "4007132  4007133   0.101079\n",
       "4007133  4007134   0.258134\n",
       "4007134  4007135   0.346373\n",
       "4007135  4007136   0.560653\n",
       "\n",
       "[4007136 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"ID\": [i+1 for i in range(len(output_pred_1d))],\n",
    "    \"Predicted\": output_pred_1d.tolist()\n",
    "})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"gsr_gat.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
