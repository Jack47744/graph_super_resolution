{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "\n",
    "import pandas as pd\n",
    "from MatrixVectorizer import *\n",
    "import torch\n",
    "import random\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "# Check for CUDA (GPU support) and set device accordingly\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)  # For multi-GPU setups\n",
    "    # Additional settings for ensuring reproducibility on CUDA\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_LR_train = pd.read_csv(\"../data/lr_train.csv\")\n",
    "A_HR_train = pd.read_csv(\"../data/hr_train.csv\")\n",
    "A_LR_test = pd.read_csv(\"../data/lr_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_size = 160\n",
    "HR_size = 268"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(167, 161)\n",
      "[[ 0 13]\n",
      " [ 1 41]\n",
      " [ 2 14]\n",
      " [ 3 48]\n",
      " [ 4 51]]\n",
      "Train size: 133\n",
      "Val size: 34\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "A_HR_train = pd.read_csv(\"../data/hr_train.csv\")\n",
    "\n",
    "pca = PCA(n_components=0.99, whiten=False)\n",
    "A_HR_train_pca = pca.fit_transform(A_HR_train)\n",
    "print(A_HR_train_pca.shape)\n",
    "\n",
    "gm = GaussianMixture(n_components=5, random_state=random_seed)\n",
    "A_HR_train_label = gm.fit_predict(A_HR_train_pca)\n",
    "unique, counts = np.unique(A_HR_train_label, return_counts=True)\n",
    "print(np.asarray((unique, counts)).T)\n",
    "\n",
    "X = np.load('A_LR_train_matrix.npy')\n",
    "y = np.load('A_HR_train_matrix.npy')\n",
    "\n",
    "n_sample = X.shape[0]\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X.reshape(n_sample, -1), \n",
    "    y.reshape(n_sample, -1), \n",
    "    test_size=0.20, \n",
    "    random_state=random_seed,\n",
    "    stratify=A_HR_train_label\n",
    ")\n",
    "\n",
    "X_train = X_train.reshape(-1, LR_size, LR_size)\n",
    "X_val = X_val.reshape(-1, LR_size, LR_size)\n",
    "y_train = y_train.reshape(-1, HR_size, HR_size)\n",
    "y_val = y_val.reshape(-1, HR_size, HR_size)\n",
    "\n",
    "print(\"Train size:\", len(X_train))\n",
    "print(\"Val size:\", len(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def antivectorize_df(adj_mtx_df, size):\n",
    "    \n",
    "#     num_subject = adj_mtx_df.shape[0]\n",
    "#     adj_mtx = np.zeros((num_subject, size, size)) #torch.zeros((num_subject, LR_size, LR_size))\n",
    "#     for i in range(num_subject):\n",
    "#         adj_mtx[i] = MatrixVectorizer.anti_vectorize(adj_mtx_df.iloc[i], size) # torch.from_numpy(MatrixVectorizer.anti_vectorize(A_LR_train.iloc[i], LR_size))\n",
    "#     return adj_mtx\n",
    "\n",
    "# np.save('A_LR_train_matrix.npy', antivectorize_df(A_LR_train, LR_size))\n",
    "# np.save('A_HR_train_matrix.npy', antivectorize_df(A_HR_train, HR_size))\n",
    "# np.save('A_LR_test_matrix.npy', antivectorize_df(A_LR_test, LR_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(167, 160, 160)\n",
      "(167, 268, 268)\n",
      "(112, 160, 160)\n"
     ]
    }
   ],
   "source": [
    "A_LR_train_matrix = np.load('A_LR_train_matrix.npy')\n",
    "A_HR_train_matrix = np.load('A_HR_train_matrix.npy')\n",
    "A_LR_test_matrix = np.load(\"A_LR_test_matrix.npy\")\n",
    "\n",
    "print(A_LR_train_matrix.shape)\n",
    "print(A_HR_train_matrix.shape)\n",
    "print(A_LR_test_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(epochs=200, lr=5e-05, splits=3, lmbda=16, lr_dim=160, hr_dim=268, hidden_dim=268, padding=26, embedding_size=32, early_stop_patient=10)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Main function of Graph Super-Resolution Network (GSR-Net) framework \n",
    "   for predicting high-resolution brain connectomes from low-resolution connectomes. \n",
    "    \n",
    "    ---------------------------------------------------------------------\n",
    "    \n",
    "    This file contains the implementation of the training and testing process of our GSR-Net model.\n",
    "        train(model, optimizer, subjects_adj, subjects_ground_truth, args)\n",
    "\n",
    "                Inputs:\n",
    "                        model:        constructor of our GSR-Net model:  model = GSRNet(ks,args)\n",
    "                                      ks:   array that stores reduction rates of nodes in Graph U-Net pooling layers\n",
    "                                      args: parsed command line arguments\n",
    "\n",
    "                        optimizer:    constructor of our model's optimizer (borrowed from PyTorch)  \n",
    "\n",
    "                        subjects_adj: (n × l x l) tensor stacking LR connectivity matrices of all training subjects\n",
    "                                       n: the total number of subjects\n",
    "                                       l: the dimensions of the LR connectivity matrices\n",
    "\n",
    "                        subjects_ground_truth: (n × h x h) tensor stacking LR connectivity matrices of all training subjects\n",
    "                                                n: the total number of subjects\n",
    "                                                h: the dimensions of the LR connectivity matrices\n",
    "\n",
    "                        args:          parsed command line arguments, to learn more about the arguments run: \n",
    "                                       python demo.py --help\n",
    "                Output:\n",
    "                        for each epoch, prints out the mean training MSE error\n",
    "\n",
    "\n",
    "            \n",
    "        test(model, test_adj,test_ground_truth,args)\n",
    "\n",
    "                Inputs:\n",
    "                        test_adj:      (n × l x l) tensor stacking LR connectivity matrices of all testing subjects\n",
    "                                        n: the total number of subjects\n",
    "                                        l: the dimensions of the LR connectivity matrices\n",
    "\n",
    "                        test_ground_truth:      (n × h x h) tensor stacking LR connectivity matrices of all testing subjects\n",
    "                                                 n: the total number of subjects\n",
    "                                                 h: the dimensions of the LR connectivity matrices\n",
    "\n",
    "                        see train method above for model and args.\n",
    "\n",
    "                Outputs:\n",
    "                        for each epoch, prints out the mean testing MSE error\n",
    "\n",
    "\n",
    "    To evaluate our framework we used 5-fold cross-validation strategy.\n",
    "\n",
    "    ---------------------------------------------------------------------\n",
    "    Copyright 2020 Megi Isallari, Istanbul Technical University.\n",
    "    All rights reserved.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import KFold\n",
    "from preprocessing import *\n",
    "from model import *\n",
    "from train import *\n",
    "import argparse\n",
    "\n",
    "\n",
    "\n",
    "epochs = 200\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='GSR-Net')\n",
    "parser.add_argument('--epochs', type=int, default=epochs, metavar='no_epochs',\n",
    "                help='number of episode to train ')\n",
    "parser.add_argument('--lr', type=float, default=0.00005, metavar='lr',\n",
    "                help='learning rate (default: 0.0001 using Adam Optimizer)')\n",
    "parser.add_argument('--splits', type=int, default=3, metavar='n_splits',\n",
    "                help='no of cross validation folds')\n",
    "parser.add_argument('--lmbda', type=int, default=16, metavar='L',\n",
    "                help='self-reconstruction error hyperparameter')\n",
    "parser.add_argument('--lr_dim', type=int, default=LR_size, metavar='N',\n",
    "                help='adjacency matrix input dimensions')\n",
    "parser.add_argument('--hr_dim', type=int, default=HR_size, metavar='N',\n",
    "                help='super-resolved adjacency matrix output dimensions')\n",
    "parser.add_argument('--hidden_dim', type=int, default=268, metavar='N',\n",
    "                help='hidden GraphConvolutional layer dimensions')\n",
    "parser.add_argument('--padding', type=int, default=26, metavar='padding',\n",
    "                help='dimensions of padding')\n",
    "parser.add_argument('--embedding_size', type=int, default=32, metavar='embedding_size',\n",
    "                help='node embedding size')\n",
    "parser.add_argument('--early_stop_patient', type=int, default=10, metavar='early_stop_patient',\n",
    "                help='early_stop_patience')\n",
    "\n",
    "\n",
    "\n",
    "# Create an empty Namespace to hold the default arguments\n",
    "args = parser.parse_args([]) \n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(167, 160, 160)\n",
      "(167, 268, 268)\n"
     ]
    }
   ],
   "source": [
    "# SIMULATING THE DATA: EDIT TO ENTER YOUR OWN DATA\n",
    "X = A_LR_train_matrix #np.random.normal(0, 0.5, (167, 160, 160))\n",
    "Y = A_HR_train_matrix #np.random.normal(0, 0.5, (167, 288, 288))\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "device = get_device()\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(167, 160, 160)\n"
     ]
    }
   ],
   "source": [
    "def compute_degree_matrix_normalization_batch_numpy(adjacency_batch):\n",
    "    \"\"\"\n",
    "    Optimizes the degree matrix normalization for a batch of adjacency matrices using NumPy.\n",
    "    Computes the normalized adjacency matrix D^-1 * A for each graph in the batch.\n",
    "    \n",
    "    Parameters:\n",
    "    - adjacency_batch: A NumPy array of shape (batch_size, num_nodes, num_nodes) representing\n",
    "                       a batch of adjacency matrices.\n",
    "\n",
    "    Returns:\n",
    "    - A NumPy array of normalized adjacency matrices.\n",
    "    \"\"\"\n",
    "    epsilon = 1e-6  # Small constant to avoid division by zero\n",
    "    # Calculate the degree for each node in the batch\n",
    "    d = adjacency_batch.sum(axis=2) + epsilon\n",
    "    \n",
    "    # Compute the inverse degree matrix D^-1 for the batch\n",
    "    D_inv = np.reciprocal(d)[:, :, np.newaxis] * np.eye(adjacency_batch.shape[1])[np.newaxis, :, :]\n",
    "    \n",
    "    # Normalize the adjacency matrix using batch matrix multiplication\n",
    "    normalized_adjacency_batch = np.matmul(D_inv, adjacency_batch)\n",
    "    \n",
    "    return normalized_adjacency_batch\n",
    "X = compute_degree_matrix_normalization_batch_numpy(X)\n",
    "A_LR_test_matrix = compute_degree_matrix_normalization_batch_numpy(A_LR_test_matrix)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Fold 1 -----\n",
      "0.5 0.03 0.1\n",
      "Epoch: 1, Train Loss: 0.315796, Train Error: 0.256734, Test Error: 0.243449\n",
      "Epoch: 2, Train Loss: 0.297514, Train Error: 0.241573, Test Error: 0.214092\n",
      "Epoch: 3, Train Loss: 0.253451, Train Error: 0.200415, Test Error: 0.175281\n",
      "Epoch: 4, Train Loss: 0.230228, Train Error: 0.180214, Test Error: 0.167977\n",
      "Epoch: 5, Train Loss: 0.225313, Train Error: 0.176938, Test Error: 0.166638\n",
      "Epoch: 6, Train Loss: 0.221685, Train Error: 0.175916, Test Error: 0.166016\n",
      "Epoch: 7, Train Loss: 0.219457, Train Error: 0.175613, Test Error: 0.165778\n",
      "Epoch: 8, Train Loss: 0.217138, Train Error: 0.175156, Test Error: 0.165310\n",
      "Epoch: 9, Train Loss: 0.216335, Train Error: 0.174871, Test Error: 0.165321\n",
      "Epoch: 10, Train Loss: 0.214449, Train Error: 0.174674, Test Error: 0.164976\n",
      "Epoch: 11, Train Loss: 0.214296, Train Error: 0.174390, Test Error: 0.164728\n",
      "Epoch: 12, Train Loss: 0.211300, Train Error: 0.174088, Test Error: 0.164615\n",
      "Epoch: 13, Train Loss: 0.210690, Train Error: 0.173739, Test Error: 0.164295\n",
      "Epoch: 14, Train Loss: 0.208426, Train Error: 0.173490, Test Error: 0.164083\n",
      "Epoch: 15, Train Loss: 0.208794, Train Error: 0.173287, Test Error: 0.163826\n",
      "Epoch: 16, Train Loss: 0.207407, Train Error: 0.172869, Test Error: 0.163345\n",
      "Epoch: 17, Train Loss: 0.206066, Train Error: 0.172554, Test Error: 0.163038\n",
      "Epoch: 18, Train Loss: 0.206127, Train Error: 0.172279, Test Error: 0.162805\n",
      "Epoch: 19, Train Loss: 0.204050, Train Error: 0.171744, Test Error: 0.162302\n",
      "Epoch: 20, Train Loss: 0.203379, Train Error: 0.171240, Test Error: 0.161834\n",
      "Epoch: 21, Train Loss: 0.203471, Train Error: 0.170944, Test Error: 0.161355\n",
      "Epoch: 22, Train Loss: 0.201117, Train Error: 0.170109, Test Error: 0.160810\n",
      "Epoch: 23, Train Loss: 0.201140, Train Error: 0.169805, Test Error: 0.160182\n",
      "Epoch: 24, Train Loss: 0.198807, Train Error: 0.168980, Test Error: 0.159331\n",
      "Epoch: 25, Train Loss: 0.199845, Train Error: 0.168533, Test Error: 0.158684\n",
      "Epoch: 26, Train Loss: 0.199012, Train Error: 0.167991, Test Error: 0.158029\n",
      "Epoch: 27, Train Loss: 0.197561, Train Error: 0.167103, Test Error: 0.157170\n",
      "Epoch: 28, Train Loss: 0.196095, Train Error: 0.166277, Test Error: 0.156280\n",
      "Epoch: 29, Train Loss: 0.195819, Train Error: 0.165492, Test Error: 0.155552\n",
      "Epoch: 30, Train Loss: 0.194302, Train Error: 0.164480, Test Error: 0.154686\n",
      "Epoch: 31, Train Loss: 0.193035, Train Error: 0.163624, Test Error: 0.153876\n",
      "Epoch: 32, Train Loss: 0.192846, Train Error: 0.162936, Test Error: 0.152992\n",
      "Epoch: 33, Train Loss: 0.191589, Train Error: 0.162084, Test Error: 0.152386\n",
      "Epoch: 34, Train Loss: 0.190844, Train Error: 0.161232, Test Error: 0.151300\n",
      "Epoch: 35, Train Loss: 0.189138, Train Error: 0.160157, Test Error: 0.150699\n",
      "Epoch: 36, Train Loss: 0.187983, Train Error: 0.159083, Test Error: 0.149970\n",
      "Epoch: 37, Train Loss: 0.187783, Train Error: 0.158943, Test Error: 0.149476\n",
      "Epoch: 38, Train Loss: 0.186982, Train Error: 0.158195, Test Error: 0.148619\n",
      "Epoch: 39, Train Loss: 0.185903, Train Error: 0.157190, Test Error: 0.148202\n",
      "Epoch: 40, Train Loss: 0.185501, Train Error: 0.156810, Test Error: 0.147610\n",
      "Epoch: 41, Train Loss: 0.184528, Train Error: 0.155580, Test Error: 0.146796\n",
      "Epoch: 42, Train Loss: 0.183709, Train Error: 0.155190, Test Error: 0.146304\n",
      "Epoch: 43, Train Loss: 0.183085, Train Error: 0.154732, Test Error: 0.145745\n",
      "Epoch: 44, Train Loss: 0.181545, Train Error: 0.153860, Test Error: 0.145322\n",
      "Epoch: 45, Train Loss: 0.181249, Train Error: 0.153266, Test Error: 0.144847\n",
      "Epoch: 46, Train Loss: 0.181115, Train Error: 0.153196, Test Error: 0.144609\n",
      "Epoch: 47, Train Loss: 0.180232, Train Error: 0.152395, Test Error: 0.143996\n",
      "Epoch: 48, Train Loss: 0.179819, Train Error: 0.151639, Test Error: 0.143516\n",
      "Epoch: 49, Train Loss: 0.178966, Train Error: 0.151282, Test Error: 0.143094\n",
      "Epoch: 50, Train Loss: 0.178480, Train Error: 0.150823, Test Error: 0.142764\n",
      "Epoch: 51, Train Loss: 0.178897, Train Error: 0.150437, Test Error: 0.142562\n",
      "Epoch: 52, Train Loss: 0.177437, Train Error: 0.149760, Test Error: 0.142043\n",
      "Epoch: 53, Train Loss: 0.177518, Train Error: 0.149604, Test Error: 0.141648\n",
      "Epoch: 54, Train Loss: 0.176522, Train Error: 0.148806, Test Error: 0.141456\n",
      "Epoch: 55, Train Loss: 0.176824, Train Error: 0.148873, Test Error: 0.141309\n",
      "Epoch: 56, Train Loss: 0.175903, Train Error: 0.148210, Test Error: 0.140932\n",
      "Epoch: 57, Train Loss: 0.176136, Train Error: 0.148321, Test Error: 0.140781\n",
      "Epoch: 58, Train Loss: 0.175426, Train Error: 0.147572, Test Error: 0.140125\n",
      "Epoch: 59, Train Loss: 0.174453, Train Error: 0.146796, Test Error: 0.139758\n",
      "Epoch: 60, Train Loss: 0.174481, Train Error: 0.146698, Test Error: 0.139895\n",
      "Epoch: 61, Train Loss: 0.173878, Train Error: 0.146145, Test Error: 0.139210\n",
      "Epoch: 62, Train Loss: 0.173630, Train Error: 0.145838, Test Error: 0.139133\n",
      "Epoch: 63, Train Loss: 0.173362, Train Error: 0.145675, Test Error: 0.138622\n",
      "Epoch: 64, Train Loss: 0.172714, Train Error: 0.145236, Test Error: 0.138720\n",
      "Epoch: 65, Train Loss: 0.172152, Train Error: 0.144781, Test Error: 0.138497\n",
      "Epoch: 66, Train Loss: 0.172446, Train Error: 0.144643, Test Error: 0.138111\n",
      "Epoch: 67, Train Loss: 0.172092, Train Error: 0.144556, Test Error: 0.138041\n",
      "Epoch: 68, Train Loss: 0.171305, Train Error: 0.143954, Test Error: 0.137782\n",
      "Epoch: 69, Train Loss: 0.171204, Train Error: 0.143718, Test Error: 0.137635\n",
      "Epoch: 70, Train Loss: 0.170750, Train Error: 0.143776, Test Error: 0.137218\n",
      "Epoch: 71, Train Loss: 0.170214, Train Error: 0.143041, Test Error: 0.136858\n",
      "Epoch: 72, Train Loss: 0.170754, Train Error: 0.143280, Test Error: 0.136864\n",
      "Epoch: 73, Train Loss: 0.170384, Train Error: 0.142831, Test Error: 0.136640\n",
      "Epoch: 74, Train Loss: 0.170168, Train Error: 0.142736, Test Error: 0.136762\n",
      "Epoch: 75, Train Loss: 0.169854, Train Error: 0.142647, Test Error: 0.136308\n",
      "Epoch: 76, Train Loss: 0.169206, Train Error: 0.141990, Test Error: 0.136250\n",
      "Epoch: 77, Train Loss: 0.169550, Train Error: 0.142019, Test Error: 0.136103\n",
      "Epoch: 78, Train Loss: 0.169030, Train Error: 0.141452, Test Error: 0.135801\n",
      "Epoch: 79, Train Loss: 0.168405, Train Error: 0.141334, Test Error: 0.135693\n",
      "Epoch: 80, Train Loss: 0.168352, Train Error: 0.141318, Test Error: 0.135724\n",
      "Epoch: 81, Train Loss: 0.168622, Train Error: 0.141499, Test Error: 0.135400\n",
      "Epoch: 82, Train Loss: 0.167726, Train Error: 0.140809, Test Error: 0.135305\n",
      "Epoch: 83, Train Loss: 0.168016, Train Error: 0.140615, Test Error: 0.135349\n",
      "Epoch: 84, Train Loss: 0.167662, Train Error: 0.140634, Test Error: 0.134960\n",
      "Epoch: 85, Train Loss: 0.167032, Train Error: 0.140089, Test Error: 0.135228\n",
      "Epoch: 86, Train Loss: 0.166996, Train Error: 0.140016, Test Error: 0.134847\n",
      "Epoch: 87, Train Loss: 0.166437, Train Error: 0.139675, Test Error: 0.134458\n",
      "Epoch: 88, Train Loss: 0.166638, Train Error: 0.139403, Test Error: 0.134800\n",
      "Epoch: 89, Train Loss: 0.165969, Train Error: 0.139065, Test Error: 0.134370\n",
      "Epoch: 90, Train Loss: 0.166679, Train Error: 0.139251, Test Error: 0.134287\n",
      "Epoch: 91, Train Loss: 0.166093, Train Error: 0.139054, Test Error: 0.134441\n",
      "Epoch: 92, Train Loss: 0.165528, Train Error: 0.138550, Test Error: 0.134276\n",
      "Epoch: 93, Train Loss: 0.165064, Train Error: 0.138159, Test Error: 0.134136\n",
      "Epoch: 94, Train Loss: 0.166030, Train Error: 0.138925, Test Error: 0.134361\n",
      "Epoch: 95, Train Loss: 0.165141, Train Error: 0.138241, Test Error: 0.133871\n",
      "Epoch: 96, Train Loss: 0.165155, Train Error: 0.138265, Test Error: 0.133723\n",
      "Epoch: 97, Train Loss: 0.164727, Train Error: 0.138048, Test Error: 0.133852\n",
      "Epoch: 98, Train Loss: 0.164417, Train Error: 0.137602, Test Error: 0.134172\n",
      "Epoch: 99, Train Loss: 0.165523, Train Error: 0.137940, Test Error: 0.133995\n",
      "Epoch: 100, Train Loss: 0.164669, Train Error: 0.137732, Test Error: 0.133416\n",
      "Epoch: 101, Train Loss: 0.164343, Train Error: 0.137383, Test Error: 0.133574\n",
      "Epoch: 102, Train Loss: 0.164276, Train Error: 0.137216, Test Error: 0.133640\n",
      "Epoch: 103, Train Loss: 0.164503, Train Error: 0.137231, Test Error: 0.133477\n",
      "Epoch: 104, Train Loss: 0.163866, Train Error: 0.136767, Test Error: 0.133793\n",
      "Epoch: 105, Train Loss: 0.164476, Train Error: 0.137323, Test Error: 0.133515\n",
      "Epoch: 106, Train Loss: 0.164708, Train Error: 0.137124, Test Error: 0.133753\n",
      "Epoch: 107, Train Loss: 0.164509, Train Error: 0.137350, Test Error: 0.134378\n",
      "Epoch: 108, Train Loss: 0.164222, Train Error: 0.137210, Test Error: 0.133193\n",
      "Epoch: 109, Train Loss: 0.164351, Train Error: 0.137309, Test Error: 0.133534\n",
      "Epoch: 110, Train Loss: 0.163290, Train Error: 0.136460, Test Error: 0.133411\n",
      "Epoch: 111, Train Loss: 0.164306, Train Error: 0.137152, Test Error: 0.133127\n",
      "Epoch: 112, Train Loss: 0.163435, Train Error: 0.136405, Test Error: 0.133140\n",
      "Epoch: 113, Train Loss: 0.163153, Train Error: 0.136382, Test Error: 0.133018\n",
      "Epoch: 114, Train Loss: 0.163917, Train Error: 0.136780, Test Error: 0.134074\n",
      "Epoch: 115, Train Loss: 0.163328, Train Error: 0.136322, Test Error: 0.132664\n",
      "Epoch: 116, Train Loss: 0.162792, Train Error: 0.135819, Test Error: 0.132552\n",
      "Epoch: 117, Train Loss: 0.161961, Train Error: 0.135646, Test Error: 0.132547\n",
      "Epoch: 118, Train Loss: 0.162690, Train Error: 0.135692, Test Error: 0.132884\n",
      "Epoch: 119, Train Loss: 0.163009, Train Error: 0.135700, Test Error: 0.132369\n",
      "Epoch: 120, Train Loss: 0.163045, Train Error: 0.135862, Test Error: 0.132607\n",
      "Epoch: 121, Train Loss: 0.162115, Train Error: 0.135310, Test Error: 0.133115\n",
      "Epoch: 122, Train Loss: 0.162426, Train Error: 0.135538, Test Error: 0.132883\n",
      "Epoch: 123, Train Loss: 0.162491, Train Error: 0.135800, Test Error: 0.132531\n",
      "Epoch: 124, Train Loss: 0.162447, Train Error: 0.135650, Test Error: 0.132783\n",
      "Epoch: 125, Train Loss: 0.162450, Train Error: 0.135545, Test Error: 0.132603\n",
      "Epoch: 126, Train Loss: 0.162452, Train Error: 0.135469, Test Error: 0.132705\n",
      "Epoch: 127, Train Loss: 0.161651, Train Error: 0.134866, Test Error: 0.132350\n",
      "Epoch: 128, Train Loss: 0.162290, Train Error: 0.135144, Test Error: 0.132775\n",
      "Epoch: 129, Train Loss: 0.162373, Train Error: 0.135531, Test Error: 0.132173\n",
      "Epoch: 130, Train Loss: 0.162120, Train Error: 0.135182, Test Error: 0.132711\n",
      "Epoch: 131, Train Loss: 0.161660, Train Error: 0.134768, Test Error: 0.132234\n",
      "Epoch: 132, Train Loss: 0.161930, Train Error: 0.135020, Test Error: 0.132591\n",
      "Epoch: 133, Train Loss: 0.161677, Train Error: 0.134983, Test Error: 0.132292\n",
      "Epoch: 134, Train Loss: 0.161586, Train Error: 0.134796, Test Error: 0.132357\n",
      "Epoch: 135, Train Loss: 0.161263, Train Error: 0.134594, Test Error: 0.132808\n",
      "Epoch: 136, Train Loss: 0.161602, Train Error: 0.134612, Test Error: 0.132361\n",
      "Epoch: 137, Train Loss: 0.161683, Train Error: 0.134716, Test Error: 0.132084\n",
      "Epoch: 138, Train Loss: 0.161966, Train Error: 0.134798, Test Error: 0.132708\n",
      "Epoch: 139, Train Loss: 0.160629, Train Error: 0.134209, Test Error: 0.131941\n",
      "Epoch: 140, Train Loss: 0.161312, Train Error: 0.134677, Test Error: 0.132131\n",
      "Epoch: 141, Train Loss: 0.160569, Train Error: 0.134030, Test Error: 0.132076\n",
      "Epoch: 142, Train Loss: 0.160819, Train Error: 0.133972, Test Error: 0.131860\n",
      "Epoch: 143, Train Loss: 0.161172, Train Error: 0.134262, Test Error: 0.132100\n",
      "Epoch: 144, Train Loss: 0.161131, Train Error: 0.134176, Test Error: 0.132093\n",
      "Epoch: 145, Train Loss: 0.160976, Train Error: 0.134210, Test Error: 0.131947\n",
      "Epoch: 146, Train Loss: 0.160554, Train Error: 0.133991, Test Error: 0.131925\n",
      "Epoch: 147, Train Loss: 0.160720, Train Error: 0.133731, Test Error: 0.132446\n",
      "Epoch: 148, Train Loss: 0.161890, Train Error: 0.134569, Test Error: 0.131570\n",
      "Epoch: 149, Train Loss: 0.160363, Train Error: 0.133588, Test Error: 0.131455\n",
      "Epoch: 150, Train Loss: 0.160401, Train Error: 0.133698, Test Error: 0.131628\n",
      "Epoch: 151, Train Loss: 0.160101, Train Error: 0.133445, Test Error: 0.131430\n",
      "Epoch: 152, Train Loss: 0.160985, Train Error: 0.134005, Test Error: 0.132062\n",
      "Epoch: 153, Train Loss: 0.160017, Train Error: 0.133365, Test Error: 0.132279\n",
      "Epoch: 154, Train Loss: 0.160462, Train Error: 0.133803, Test Error: 0.131586\n",
      "Epoch: 155, Train Loss: 0.159670, Train Error: 0.133168, Test Error: 0.131864\n",
      "Epoch: 156, Train Loss: 0.159972, Train Error: 0.133232, Test Error: 0.132117\n",
      "Epoch: 157, Train Loss: 0.159788, Train Error: 0.132923, Test Error: 0.131416\n",
      "Epoch: 158, Train Loss: 0.159151, Train Error: 0.132783, Test Error: 0.131038\n",
      "Epoch: 159, Train Loss: 0.159969, Train Error: 0.133095, Test Error: 0.131006\n",
      "Epoch: 160, Train Loss: 0.160194, Train Error: 0.133287, Test Error: 0.131408\n",
      "Epoch: 161, Train Loss: 0.159900, Train Error: 0.133192, Test Error: 0.131061\n",
      "Epoch: 162, Train Loss: 0.159606, Train Error: 0.132940, Test Error: 0.131548\n",
      "Epoch: 163, Train Loss: 0.159484, Train Error: 0.132765, Test Error: 0.131734\n",
      "Epoch: 164, Train Loss: 0.159504, Train Error: 0.132630, Test Error: 0.131533\n",
      "Epoch: 165, Train Loss: 0.159531, Train Error: 0.132749, Test Error: 0.131077\n",
      "Epoch: 166, Train Loss: 0.159716, Train Error: 0.132800, Test Error: 0.131422\n",
      "Epoch: 167, Train Loss: 0.159070, Train Error: 0.132565, Test Error: 0.131065\n",
      "Epoch: 168, Train Loss: 0.158576, Train Error: 0.132121, Test Error: 0.131169\n",
      "Epoch: 169, Train Loss: 0.159276, Train Error: 0.132466, Test Error: 0.131229\n",
      "Val Error: 0.131006\n",
      "Val MAE: 0.13100647434060061\n",
      "----- Fold 2 -----\n",
      "0.5 0.03 0.1\n",
      "Epoch: 1, Train Loss: 0.314406, Train Error: 0.253681, Test Error: 0.248714\n",
      "Epoch: 2, Train Loss: 0.290901, Train Error: 0.234769, Test Error: 0.214868\n",
      "Epoch: 3, Train Loss: 0.245730, Train Error: 0.192211, Test Error: 0.181429\n",
      "Epoch: 4, Train Loss: 0.225330, Train Error: 0.175359, Test Error: 0.175682\n",
      "Epoch: 5, Train Loss: 0.219323, Train Error: 0.172556, Test Error: 0.174695\n",
      "Epoch: 6, Train Loss: 0.218343, Train Error: 0.171839, Test Error: 0.174289\n",
      "Epoch: 7, Train Loss: 0.215342, Train Error: 0.171355, Test Error: 0.173879\n",
      "Epoch: 8, Train Loss: 0.212680, Train Error: 0.171023, Test Error: 0.173836\n",
      "Epoch: 9, Train Loss: 0.212276, Train Error: 0.170821, Test Error: 0.173618\n",
      "Epoch: 10, Train Loss: 0.209478, Train Error: 0.170502, Test Error: 0.173343\n",
      "Epoch: 11, Train Loss: 0.208970, Train Error: 0.170209, Test Error: 0.173157\n",
      "Epoch: 12, Train Loss: 0.207667, Train Error: 0.169984, Test Error: 0.173038\n",
      "Epoch: 13, Train Loss: 0.206753, Train Error: 0.169855, Test Error: 0.172873\n",
      "Epoch: 14, Train Loss: 0.204849, Train Error: 0.169616, Test Error: 0.172741\n",
      "Epoch: 15, Train Loss: 0.203710, Train Error: 0.169356, Test Error: 0.172408\n",
      "Epoch: 16, Train Loss: 0.203646, Train Error: 0.169144, Test Error: 0.172156\n",
      "Epoch: 17, Train Loss: 0.202161, Train Error: 0.168847, Test Error: 0.172075\n",
      "Epoch: 18, Train Loss: 0.201919, Train Error: 0.168588, Test Error: 0.171772\n",
      "Epoch: 19, Train Loss: 0.200938, Train Error: 0.168271, Test Error: 0.171464\n",
      "Epoch: 20, Train Loss: 0.200086, Train Error: 0.168008, Test Error: 0.171156\n",
      "Epoch: 21, Train Loss: 0.199986, Train Error: 0.167596, Test Error: 0.170840\n",
      "Epoch: 22, Train Loss: 0.198703, Train Error: 0.167237, Test Error: 0.170587\n",
      "Epoch: 23, Train Loss: 0.198721, Train Error: 0.166811, Test Error: 0.170104\n",
      "Epoch: 24, Train Loss: 0.197751, Train Error: 0.166385, Test Error: 0.169688\n",
      "Epoch: 25, Train Loss: 0.197413, Train Error: 0.165959, Test Error: 0.169293\n",
      "Epoch: 26, Train Loss: 0.196991, Train Error: 0.165529, Test Error: 0.168701\n",
      "Epoch: 27, Train Loss: 0.195669, Train Error: 0.164712, Test Error: 0.168163\n",
      "Epoch: 28, Train Loss: 0.194735, Train Error: 0.164228, Test Error: 0.167564\n",
      "Epoch: 29, Train Loss: 0.193753, Train Error: 0.163509, Test Error: 0.166911\n",
      "Epoch: 30, Train Loss: 0.193637, Train Error: 0.162896, Test Error: 0.166316\n",
      "Epoch: 31, Train Loss: 0.191919, Train Error: 0.161991, Test Error: 0.165391\n",
      "Epoch: 32, Train Loss: 0.190956, Train Error: 0.161206, Test Error: 0.164689\n",
      "Epoch: 33, Train Loss: 0.190452, Train Error: 0.160554, Test Error: 0.163924\n",
      "Epoch: 34, Train Loss: 0.188317, Train Error: 0.159240, Test Error: 0.163237\n",
      "Epoch: 35, Train Loss: 0.187643, Train Error: 0.158417, Test Error: 0.162369\n",
      "Epoch: 36, Train Loss: 0.186842, Train Error: 0.157399, Test Error: 0.161519\n",
      "Epoch: 37, Train Loss: 0.185116, Train Error: 0.156521, Test Error: 0.160689\n",
      "Epoch: 38, Train Loss: 0.184792, Train Error: 0.155613, Test Error: 0.160113\n",
      "Epoch: 39, Train Loss: 0.183469, Train Error: 0.154657, Test Error: 0.159654\n",
      "Epoch: 40, Train Loss: 0.182408, Train Error: 0.153826, Test Error: 0.158405\n",
      "Epoch: 41, Train Loss: 0.182104, Train Error: 0.153076, Test Error: 0.157890\n",
      "Epoch: 42, Train Loss: 0.180420, Train Error: 0.151745, Test Error: 0.157267\n",
      "Epoch: 43, Train Loss: 0.179865, Train Error: 0.151251, Test Error: 0.156537\n",
      "Epoch: 44, Train Loss: 0.178510, Train Error: 0.150302, Test Error: 0.155798\n",
      "Epoch: 45, Train Loss: 0.177505, Train Error: 0.149601, Test Error: 0.155282\n",
      "Epoch: 46, Train Loss: 0.177957, Train Error: 0.149115, Test Error: 0.154959\n",
      "Epoch: 47, Train Loss: 0.176499, Train Error: 0.148293, Test Error: 0.154279\n",
      "Epoch: 48, Train Loss: 0.175443, Train Error: 0.147421, Test Error: 0.153839\n",
      "Epoch: 49, Train Loss: 0.174736, Train Error: 0.146806, Test Error: 0.153492\n",
      "Epoch: 50, Train Loss: 0.174823, Train Error: 0.146735, Test Error: 0.153334\n",
      "Epoch: 51, Train Loss: 0.173141, Train Error: 0.145449, Test Error: 0.153134\n",
      "Epoch: 52, Train Loss: 0.173463, Train Error: 0.145381, Test Error: 0.152325\n",
      "Epoch: 53, Train Loss: 0.172647, Train Error: 0.145005, Test Error: 0.152016\n",
      "Epoch: 54, Train Loss: 0.172009, Train Error: 0.144468, Test Error: 0.151223\n",
      "Epoch: 55, Train Loss: 0.172373, Train Error: 0.144131, Test Error: 0.151138\n",
      "Epoch: 56, Train Loss: 0.171453, Train Error: 0.143423, Test Error: 0.151092\n",
      "Epoch: 57, Train Loss: 0.170961, Train Error: 0.142880, Test Error: 0.150386\n",
      "Epoch: 58, Train Loss: 0.169854, Train Error: 0.142097, Test Error: 0.150363\n",
      "Epoch: 59, Train Loss: 0.169882, Train Error: 0.141876, Test Error: 0.150028\n",
      "Epoch: 60, Train Loss: 0.168774, Train Error: 0.141457, Test Error: 0.149158\n",
      "Epoch: 61, Train Loss: 0.168566, Train Error: 0.141291, Test Error: 0.148812\n",
      "Epoch: 62, Train Loss: 0.168273, Train Error: 0.140672, Test Error: 0.148781\n",
      "Epoch: 63, Train Loss: 0.167377, Train Error: 0.139961, Test Error: 0.148212\n",
      "Epoch: 64, Train Loss: 0.168151, Train Error: 0.140488, Test Error: 0.148605\n",
      "Epoch: 65, Train Loss: 0.167873, Train Error: 0.140133, Test Error: 0.148309\n",
      "Epoch: 66, Train Loss: 0.166316, Train Error: 0.139572, Test Error: 0.148316\n",
      "Epoch: 67, Train Loss: 0.167093, Train Error: 0.139527, Test Error: 0.147692\n",
      "Epoch: 68, Train Loss: 0.166187, Train Error: 0.138980, Test Error: 0.147609\n",
      "Epoch: 69, Train Loss: 0.165601, Train Error: 0.138421, Test Error: 0.146951\n",
      "Epoch: 70, Train Loss: 0.164956, Train Error: 0.137898, Test Error: 0.146684\n",
      "Epoch: 71, Train Loss: 0.165333, Train Error: 0.138003, Test Error: 0.146747\n",
      "Epoch: 72, Train Loss: 0.164673, Train Error: 0.137896, Test Error: 0.146706\n",
      "Epoch: 73, Train Loss: 0.164051, Train Error: 0.137156, Test Error: 0.146287\n",
      "Epoch: 74, Train Loss: 0.164122, Train Error: 0.137017, Test Error: 0.146135\n",
      "Epoch: 75, Train Loss: 0.163833, Train Error: 0.136798, Test Error: 0.146080\n",
      "Epoch: 76, Train Loss: 0.163510, Train Error: 0.136400, Test Error: 0.146952\n",
      "Epoch: 77, Train Loss: 0.164326, Train Error: 0.136628, Test Error: 0.146020\n",
      "Epoch: 78, Train Loss: 0.162633, Train Error: 0.135769, Test Error: 0.145599\n",
      "Epoch: 79, Train Loss: 0.163429, Train Error: 0.135918, Test Error: 0.145882\n",
      "Epoch: 80, Train Loss: 0.162341, Train Error: 0.135473, Test Error: 0.145494\n",
      "Epoch: 81, Train Loss: 0.163659, Train Error: 0.136033, Test Error: 0.146250\n",
      "Epoch: 82, Train Loss: 0.162845, Train Error: 0.135439, Test Error: 0.145501\n",
      "Epoch: 83, Train Loss: 0.162825, Train Error: 0.135621, Test Error: 0.145341\n",
      "Epoch: 84, Train Loss: 0.162225, Train Error: 0.135127, Test Error: 0.145145\n",
      "Epoch: 85, Train Loss: 0.161823, Train Error: 0.134781, Test Error: 0.145120\n",
      "Epoch: 86, Train Loss: 0.162095, Train Error: 0.134910, Test Error: 0.145307\n",
      "Epoch: 87, Train Loss: 0.161835, Train Error: 0.134761, Test Error: 0.144628\n",
      "Epoch: 88, Train Loss: 0.161809, Train Error: 0.134585, Test Error: 0.144481\n",
      "Epoch: 89, Train Loss: 0.161815, Train Error: 0.134507, Test Error: 0.144941\n",
      "Epoch: 90, Train Loss: 0.161505, Train Error: 0.134145, Test Error: 0.145034\n",
      "Epoch: 91, Train Loss: 0.160894, Train Error: 0.133981, Test Error: 0.144314\n",
      "Epoch: 92, Train Loss: 0.161623, Train Error: 0.134115, Test Error: 0.143882\n",
      "Epoch: 93, Train Loss: 0.161484, Train Error: 0.133972, Test Error: 0.144221\n",
      "Epoch: 94, Train Loss: 0.160679, Train Error: 0.133594, Test Error: 0.144789\n",
      "Epoch: 95, Train Loss: 0.160846, Train Error: 0.133686, Test Error: 0.143898\n",
      "Epoch: 96, Train Loss: 0.160393, Train Error: 0.133513, Test Error: 0.143868\n",
      "Epoch: 97, Train Loss: 0.159568, Train Error: 0.133090, Test Error: 0.143953\n",
      "Epoch: 98, Train Loss: 0.159952, Train Error: 0.132984, Test Error: 0.143782\n",
      "Epoch: 99, Train Loss: 0.159724, Train Error: 0.132939, Test Error: 0.143729\n",
      "Epoch: 100, Train Loss: 0.159583, Train Error: 0.132686, Test Error: 0.143718\n",
      "Epoch: 101, Train Loss: 0.159962, Train Error: 0.133027, Test Error: 0.143183\n",
      "Epoch: 102, Train Loss: 0.159951, Train Error: 0.132785, Test Error: 0.143484\n",
      "Epoch: 103, Train Loss: 0.158847, Train Error: 0.132292, Test Error: 0.143108\n",
      "Epoch: 104, Train Loss: 0.158774, Train Error: 0.132137, Test Error: 0.143840\n",
      "Epoch: 105, Train Loss: 0.159055, Train Error: 0.132305, Test Error: 0.143333\n",
      "Epoch: 106, Train Loss: 0.159192, Train Error: 0.132222, Test Error: 0.143000\n",
      "Epoch: 107, Train Loss: 0.158732, Train Error: 0.132002, Test Error: 0.142984\n",
      "Epoch: 108, Train Loss: 0.159026, Train Error: 0.131829, Test Error: 0.142664\n",
      "Epoch: 109, Train Loss: 0.159956, Train Error: 0.132573, Test Error: 0.142824\n",
      "Epoch: 110, Train Loss: 0.158735, Train Error: 0.131858, Test Error: 0.143203\n",
      "Epoch: 111, Train Loss: 0.158656, Train Error: 0.131734, Test Error: 0.142782\n",
      "Epoch: 112, Train Loss: 0.159230, Train Error: 0.131916, Test Error: 0.143840\n",
      "Epoch: 113, Train Loss: 0.158220, Train Error: 0.131700, Test Error: 0.143375\n",
      "Epoch: 114, Train Loss: 0.158506, Train Error: 0.131433, Test Error: 0.143351\n",
      "Epoch: 115, Train Loss: 0.158760, Train Error: 0.131592, Test Error: 0.142710\n",
      "Epoch: 116, Train Loss: 0.158594, Train Error: 0.131446, Test Error: 0.143025\n",
      "Epoch: 117, Train Loss: 0.158602, Train Error: 0.131587, Test Error: 0.142640\n",
      "Epoch: 118, Train Loss: 0.158316, Train Error: 0.131338, Test Error: 0.142845\n",
      "Epoch: 119, Train Loss: 0.158476, Train Error: 0.131651, Test Error: 0.142734\n",
      "Epoch: 120, Train Loss: 0.157732, Train Error: 0.130814, Test Error: 0.142802\n",
      "Epoch: 121, Train Loss: 0.157363, Train Error: 0.130777, Test Error: 0.142643\n",
      "Epoch: 122, Train Loss: 0.157636, Train Error: 0.131105, Test Error: 0.142469\n",
      "Epoch: 123, Train Loss: 0.157888, Train Error: 0.130968, Test Error: 0.141817\n",
      "Epoch: 124, Train Loss: 0.157663, Train Error: 0.130870, Test Error: 0.141728\n",
      "Epoch: 125, Train Loss: 0.157486, Train Error: 0.130746, Test Error: 0.142609\n",
      "Epoch: 126, Train Loss: 0.158249, Train Error: 0.130959, Test Error: 0.142148\n",
      "Epoch: 127, Train Loss: 0.157659, Train Error: 0.130751, Test Error: 0.142615\n",
      "Epoch: 128, Train Loss: 0.157599, Train Error: 0.130784, Test Error: 0.141825\n",
      "Epoch: 129, Train Loss: 0.157236, Train Error: 0.130417, Test Error: 0.142489\n",
      "Epoch: 130, Train Loss: 0.157610, Train Error: 0.130846, Test Error: 0.141899\n",
      "Epoch: 131, Train Loss: 0.157144, Train Error: 0.130409, Test Error: 0.141708\n",
      "Epoch: 132, Train Loss: 0.156951, Train Error: 0.130343, Test Error: 0.141883\n",
      "Epoch: 133, Train Loss: 0.156849, Train Error: 0.130145, Test Error: 0.142354\n",
      "Epoch: 134, Train Loss: 0.156847, Train Error: 0.130104, Test Error: 0.141656\n",
      "Epoch: 135, Train Loss: 0.156664, Train Error: 0.129971, Test Error: 0.142072\n",
      "Epoch: 136, Train Loss: 0.156950, Train Error: 0.130179, Test Error: 0.142085\n",
      "Epoch: 137, Train Loss: 0.156557, Train Error: 0.129979, Test Error: 0.141443\n",
      "Epoch: 138, Train Loss: 0.156390, Train Error: 0.129810, Test Error: 0.141681\n",
      "Epoch: 139, Train Loss: 0.156473, Train Error: 0.129708, Test Error: 0.141840\n",
      "Epoch: 140, Train Loss: 0.157293, Train Error: 0.130488, Test Error: 0.141355\n",
      "Epoch: 141, Train Loss: 0.155838, Train Error: 0.129421, Test Error: 0.142401\n",
      "Epoch: 142, Train Loss: 0.156958, Train Error: 0.130239, Test Error: 0.142663\n",
      "Epoch: 143, Train Loss: 0.156633, Train Error: 0.129877, Test Error: 0.141547\n",
      "Epoch: 144, Train Loss: 0.156421, Train Error: 0.129581, Test Error: 0.142436\n",
      "Epoch: 145, Train Loss: 0.156910, Train Error: 0.130137, Test Error: 0.141785\n",
      "Epoch: 146, Train Loss: 0.156148, Train Error: 0.129530, Test Error: 0.141582\n",
      "Epoch: 147, Train Loss: 0.155906, Train Error: 0.129111, Test Error: 0.142240\n",
      "Epoch: 148, Train Loss: 0.155863, Train Error: 0.129161, Test Error: 0.141483\n",
      "Epoch: 149, Train Loss: 0.155688, Train Error: 0.129170, Test Error: 0.141344\n",
      "Epoch: 150, Train Loss: 0.155581, Train Error: 0.128805, Test Error: 0.142309\n",
      "Epoch: 151, Train Loss: 0.155335, Train Error: 0.128721, Test Error: 0.141007\n",
      "Epoch: 152, Train Loss: 0.155227, Train Error: 0.128840, Test Error: 0.141929\n",
      "Epoch: 153, Train Loss: 0.155654, Train Error: 0.128951, Test Error: 0.141427\n",
      "Epoch: 154, Train Loss: 0.155228, Train Error: 0.128784, Test Error: 0.141471\n",
      "Epoch: 155, Train Loss: 0.154560, Train Error: 0.128461, Test Error: 0.140766\n",
      "Epoch: 156, Train Loss: 0.155265, Train Error: 0.128784, Test Error: 0.141156\n",
      "Epoch: 157, Train Loss: 0.155622, Train Error: 0.128692, Test Error: 0.141540\n",
      "Epoch: 158, Train Loss: 0.155255, Train Error: 0.128639, Test Error: 0.141129\n",
      "Epoch: 159, Train Loss: 0.155519, Train Error: 0.128629, Test Error: 0.141795\n",
      "Epoch: 160, Train Loss: 0.155873, Train Error: 0.129041, Test Error: 0.140826\n",
      "Epoch: 161, Train Loss: 0.155756, Train Error: 0.128905, Test Error: 0.141657\n",
      "Epoch: 162, Train Loss: 0.155118, Train Error: 0.128600, Test Error: 0.140841\n",
      "Epoch: 163, Train Loss: 0.155309, Train Error: 0.128681, Test Error: 0.140388\n",
      "Epoch: 164, Train Loss: 0.153775, Train Error: 0.127682, Test Error: 0.141147\n",
      "Epoch: 165, Train Loss: 0.155583, Train Error: 0.128867, Test Error: 0.141454\n",
      "Epoch: 166, Train Loss: 0.154963, Train Error: 0.128293, Test Error: 0.141264\n",
      "Epoch: 167, Train Loss: 0.155094, Train Error: 0.128454, Test Error: 0.141781\n",
      "Epoch: 168, Train Loss: 0.154897, Train Error: 0.128013, Test Error: 0.141303\n",
      "Epoch: 169, Train Loss: 0.155239, Train Error: 0.128425, Test Error: 0.141585\n",
      "Epoch: 170, Train Loss: 0.154171, Train Error: 0.127961, Test Error: 0.141187\n",
      "Epoch: 171, Train Loss: 0.154268, Train Error: 0.128017, Test Error: 0.141128\n",
      "Epoch: 172, Train Loss: 0.154931, Train Error: 0.128089, Test Error: 0.142039\n",
      "Epoch: 173, Train Loss: 0.154448, Train Error: 0.128009, Test Error: 0.141759\n",
      "Val Error: 0.140388\n",
      "Val MAE: 0.14038774424365588\n",
      "----- Fold 3 -----\n",
      "0.5 0.03 0.1\n",
      "Epoch: 1, Train Loss: 0.314781, Train Error: 0.254591, Test Error: 0.247205\n",
      "Epoch: 2, Train Loss: 0.292695, Train Error: 0.236822, Test Error: 0.214204\n",
      "Epoch: 3, Train Loss: 0.248529, Train Error: 0.195059, Test Error: 0.178360\n",
      "Epoch: 4, Train Loss: 0.228880, Train Error: 0.177725, Test Error: 0.172440\n",
      "Epoch: 5, Train Loss: 0.222734, Train Error: 0.174795, Test Error: 0.171098\n",
      "Epoch: 6, Train Loss: 0.218798, Train Error: 0.173861, Test Error: 0.170134\n",
      "Epoch: 7, Train Loss: 0.218217, Train Error: 0.173704, Test Error: 0.169963\n",
      "Epoch: 8, Train Loss: 0.215324, Train Error: 0.173267, Test Error: 0.169888\n",
      "Epoch: 9, Train Loss: 0.214145, Train Error: 0.172923, Test Error: 0.169539\n",
      "Epoch: 10, Train Loss: 0.211560, Train Error: 0.172663, Test Error: 0.169335\n",
      "Epoch: 11, Train Loss: 0.210607, Train Error: 0.172414, Test Error: 0.169152\n",
      "Epoch: 12, Train Loss: 0.208658, Train Error: 0.172105, Test Error: 0.169000\n",
      "Epoch: 13, Train Loss: 0.207718, Train Error: 0.172014, Test Error: 0.168677\n",
      "Epoch: 14, Train Loss: 0.206379, Train Error: 0.171667, Test Error: 0.168438\n",
      "Epoch: 15, Train Loss: 0.205872, Train Error: 0.171379, Test Error: 0.168113\n",
      "Epoch: 16, Train Loss: 0.205418, Train Error: 0.171046, Test Error: 0.167929\n",
      "Epoch: 17, Train Loss: 0.204617, Train Error: 0.170824, Test Error: 0.167585\n",
      "Epoch: 18, Train Loss: 0.204104, Train Error: 0.170502, Test Error: 0.167312\n",
      "Epoch: 19, Train Loss: 0.202243, Train Error: 0.170119, Test Error: 0.166973\n",
      "Epoch: 20, Train Loss: 0.202087, Train Error: 0.169808, Test Error: 0.166579\n",
      "Epoch: 21, Train Loss: 0.201686, Train Error: 0.169419, Test Error: 0.166180\n",
      "Epoch: 22, Train Loss: 0.200759, Train Error: 0.168962, Test Error: 0.165792\n",
      "Epoch: 23, Train Loss: 0.199841, Train Error: 0.168447, Test Error: 0.165228\n",
      "Epoch: 24, Train Loss: 0.198911, Train Error: 0.167832, Test Error: 0.164572\n",
      "Epoch: 25, Train Loss: 0.197994, Train Error: 0.167359, Test Error: 0.164018\n",
      "Epoch: 26, Train Loss: 0.197127, Train Error: 0.166755, Test Error: 0.163410\n",
      "Epoch: 27, Train Loss: 0.196622, Train Error: 0.166218, Test Error: 0.162538\n",
      "Epoch: 28, Train Loss: 0.195204, Train Error: 0.164991, Test Error: 0.161767\n",
      "Epoch: 29, Train Loss: 0.194926, Train Error: 0.164740, Test Error: 0.160928\n",
      "Epoch: 30, Train Loss: 0.193942, Train Error: 0.163879, Test Error: 0.160061\n",
      "Epoch: 31, Train Loss: 0.192780, Train Error: 0.162894, Test Error: 0.159389\n",
      "Epoch: 32, Train Loss: 0.191363, Train Error: 0.161730, Test Error: 0.158198\n",
      "Epoch: 33, Train Loss: 0.190503, Train Error: 0.160920, Test Error: 0.157048\n",
      "Epoch: 34, Train Loss: 0.189176, Train Error: 0.159755, Test Error: 0.156184\n",
      "Epoch: 35, Train Loss: 0.188004, Train Error: 0.158774, Test Error: 0.155224\n",
      "Epoch: 36, Train Loss: 0.185958, Train Error: 0.157577, Test Error: 0.154370\n",
      "Epoch: 37, Train Loss: 0.186301, Train Error: 0.157096, Test Error: 0.153879\n",
      "Epoch: 38, Train Loss: 0.185513, Train Error: 0.156064, Test Error: 0.152955\n",
      "Epoch: 39, Train Loss: 0.184335, Train Error: 0.155260, Test Error: 0.152331\n",
      "Epoch: 40, Train Loss: 0.182503, Train Error: 0.153785, Test Error: 0.151583\n",
      "Epoch: 41, Train Loss: 0.183315, Train Error: 0.153922, Test Error: 0.150997\n",
      "Epoch: 42, Train Loss: 0.180361, Train Error: 0.152266, Test Error: 0.150221\n",
      "Epoch: 43, Train Loss: 0.180408, Train Error: 0.152143, Test Error: 0.149525\n",
      "Epoch: 44, Train Loss: 0.179825, Train Error: 0.151377, Test Error: 0.149238\n",
      "Epoch: 45, Train Loss: 0.180201, Train Error: 0.151436, Test Error: 0.148800\n",
      "Epoch: 46, Train Loss: 0.178363, Train Error: 0.150135, Test Error: 0.148521\n",
      "Epoch: 47, Train Loss: 0.177933, Train Error: 0.149861, Test Error: 0.147895\n",
      "Epoch: 48, Train Loss: 0.176857, Train Error: 0.149018, Test Error: 0.147547\n",
      "Epoch: 49, Train Loss: 0.176915, Train Error: 0.148697, Test Error: 0.147398\n",
      "Epoch: 50, Train Loss: 0.175833, Train Error: 0.147682, Test Error: 0.146535\n",
      "Epoch: 51, Train Loss: 0.175796, Train Error: 0.147462, Test Error: 0.146337\n",
      "Epoch: 52, Train Loss: 0.175688, Train Error: 0.147334, Test Error: 0.145649\n",
      "Epoch: 53, Train Loss: 0.174165, Train Error: 0.146234, Test Error: 0.145529\n",
      "Epoch: 54, Train Loss: 0.173984, Train Error: 0.145981, Test Error: 0.145012\n",
      "Epoch: 55, Train Loss: 0.173683, Train Error: 0.145823, Test Error: 0.145143\n",
      "Epoch: 56, Train Loss: 0.172883, Train Error: 0.145541, Test Error: 0.145013\n",
      "Epoch: 57, Train Loss: 0.173465, Train Error: 0.145447, Test Error: 0.144730\n",
      "Epoch: 58, Train Loss: 0.172085, Train Error: 0.144593, Test Error: 0.143813\n",
      "Epoch: 59, Train Loss: 0.172582, Train Error: 0.144685, Test Error: 0.143682\n",
      "Epoch: 60, Train Loss: 0.171780, Train Error: 0.144219, Test Error: 0.143402\n",
      "Epoch: 61, Train Loss: 0.171066, Train Error: 0.143733, Test Error: 0.143019\n",
      "Epoch: 62, Train Loss: 0.170473, Train Error: 0.143084, Test Error: 0.142456\n",
      "Epoch: 63, Train Loss: 0.170104, Train Error: 0.142666, Test Error: 0.142248\n",
      "Epoch: 64, Train Loss: 0.169972, Train Error: 0.142396, Test Error: 0.142097\n",
      "Epoch: 65, Train Loss: 0.170013, Train Error: 0.142540, Test Error: 0.141990\n",
      "Epoch: 66, Train Loss: 0.169400, Train Error: 0.142044, Test Error: 0.141508\n",
      "Epoch: 67, Train Loss: 0.168491, Train Error: 0.141386, Test Error: 0.142318\n",
      "Epoch: 68, Train Loss: 0.169193, Train Error: 0.141555, Test Error: 0.141679\n",
      "Epoch: 69, Train Loss: 0.168933, Train Error: 0.141184, Test Error: 0.141570\n",
      "Epoch: 70, Train Loss: 0.168449, Train Error: 0.140816, Test Error: 0.141527\n",
      "Epoch: 71, Train Loss: 0.167559, Train Error: 0.140365, Test Error: 0.141155\n",
      "Epoch: 72, Train Loss: 0.167255, Train Error: 0.139989, Test Error: 0.141062\n",
      "Epoch: 73, Train Loss: 0.168284, Train Error: 0.140392, Test Error: 0.140961\n",
      "Epoch: 74, Train Loss: 0.167306, Train Error: 0.139526, Test Error: 0.140664\n",
      "Epoch: 75, Train Loss: 0.167361, Train Error: 0.140024, Test Error: 0.140966\n",
      "Epoch: 76, Train Loss: 0.167069, Train Error: 0.139952, Test Error: 0.140494\n",
      "Epoch: 77, Train Loss: 0.167279, Train Error: 0.139568, Test Error: 0.141016\n",
      "Epoch: 78, Train Loss: 0.166044, Train Error: 0.138908, Test Error: 0.140483\n",
      "Epoch: 79, Train Loss: 0.166416, Train Error: 0.139361, Test Error: 0.140317\n",
      "Epoch: 80, Train Loss: 0.166244, Train Error: 0.139018, Test Error: 0.139886\n",
      "Epoch: 81, Train Loss: 0.165845, Train Error: 0.138521, Test Error: 0.139389\n",
      "Epoch: 82, Train Loss: 0.165058, Train Error: 0.138153, Test Error: 0.139502\n",
      "Epoch: 83, Train Loss: 0.165472, Train Error: 0.138279, Test Error: 0.139773\n",
      "Epoch: 84, Train Loss: 0.165262, Train Error: 0.137945, Test Error: 0.139993\n",
      "Epoch: 85, Train Loss: 0.165923, Train Error: 0.138287, Test Error: 0.139369\n",
      "Epoch: 86, Train Loss: 0.165166, Train Error: 0.137722, Test Error: 0.139395\n",
      "Epoch: 87, Train Loss: 0.164538, Train Error: 0.137276, Test Error: 0.138869\n",
      "Epoch: 88, Train Loss: 0.164430, Train Error: 0.137468, Test Error: 0.138900\n",
      "Epoch: 89, Train Loss: 0.164013, Train Error: 0.137121, Test Error: 0.138369\n",
      "Epoch: 90, Train Loss: 0.163984, Train Error: 0.136756, Test Error: 0.138373\n",
      "Epoch: 91, Train Loss: 0.163954, Train Error: 0.136905, Test Error: 0.138758\n",
      "Epoch: 92, Train Loss: 0.163910, Train Error: 0.136657, Test Error: 0.138472\n",
      "Epoch: 93, Train Loss: 0.163678, Train Error: 0.136409, Test Error: 0.138897\n",
      "Epoch: 94, Train Loss: 0.163096, Train Error: 0.136419, Test Error: 0.137898\n",
      "Epoch: 95, Train Loss: 0.163117, Train Error: 0.136118, Test Error: 0.138264\n",
      "Epoch: 96, Train Loss: 0.163147, Train Error: 0.136132, Test Error: 0.138915\n",
      "Epoch: 97, Train Loss: 0.162584, Train Error: 0.135785, Test Error: 0.138430\n",
      "Epoch: 98, Train Loss: 0.163730, Train Error: 0.136394, Test Error: 0.138807\n",
      "Epoch: 99, Train Loss: 0.162175, Train Error: 0.135408, Test Error: 0.137825\n",
      "Epoch: 100, Train Loss: 0.163377, Train Error: 0.136238, Test Error: 0.139203\n",
      "Epoch: 101, Train Loss: 0.162724, Train Error: 0.135843, Test Error: 0.138644\n",
      "Epoch: 102, Train Loss: 0.162675, Train Error: 0.135595, Test Error: 0.138420\n",
      "Epoch: 103, Train Loss: 0.162981, Train Error: 0.135849, Test Error: 0.138704\n",
      "Epoch: 104, Train Loss: 0.162128, Train Error: 0.134997, Test Error: 0.138243\n",
      "Epoch: 105, Train Loss: 0.162478, Train Error: 0.135296, Test Error: 0.138351\n",
      "Epoch: 106, Train Loss: 0.163123, Train Error: 0.135956, Test Error: 0.137726\n",
      "Epoch: 107, Train Loss: 0.161113, Train Error: 0.134420, Test Error: 0.137852\n",
      "Epoch: 108, Train Loss: 0.161466, Train Error: 0.134271, Test Error: 0.137827\n",
      "Epoch: 109, Train Loss: 0.161464, Train Error: 0.134205, Test Error: 0.137612\n",
      "Epoch: 110, Train Loss: 0.161505, Train Error: 0.134530, Test Error: 0.137629\n",
      "Epoch: 111, Train Loss: 0.161725, Train Error: 0.134816, Test Error: 0.138143\n",
      "Epoch: 112, Train Loss: 0.160752, Train Error: 0.134094, Test Error: 0.137395\n",
      "Epoch: 113, Train Loss: 0.161312, Train Error: 0.134210, Test Error: 0.137668\n",
      "Epoch: 114, Train Loss: 0.162283, Train Error: 0.134980, Test Error: 0.137816\n",
      "Epoch: 115, Train Loss: 0.161092, Train Error: 0.134372, Test Error: 0.138197\n",
      "Epoch: 116, Train Loss: 0.161717, Train Error: 0.134491, Test Error: 0.137763\n",
      "Epoch: 117, Train Loss: 0.161269, Train Error: 0.134494, Test Error: 0.138102\n",
      "Epoch: 118, Train Loss: 0.161923, Train Error: 0.134734, Test Error: 0.138035\n",
      "Epoch: 119, Train Loss: 0.161163, Train Error: 0.134455, Test Error: 0.138363\n",
      "Epoch: 120, Train Loss: 0.160801, Train Error: 0.134164, Test Error: 0.137521\n",
      "Epoch: 121, Train Loss: 0.161496, Train Error: 0.134440, Test Error: 0.137508\n",
      "Epoch: 122, Train Loss: 0.161128, Train Error: 0.133935, Test Error: 0.137715\n",
      "Val Error: 0.137395\n",
      "Val MAE: 0.13739477694034577\n"
     ]
    }
   ],
   "source": [
    "cv = KFold(n_splits=args.splits, random_state=random_seed, shuffle=True)\n",
    "\n",
    "ks = [0.9, 0.7, 0.6, 0.5]\n",
    "\n",
    "best_model_fold_list = []\n",
    "data_fold_list = []\n",
    "i = 1\n",
    "for train_index, test_index in cv.split(X):\n",
    "\n",
    "    print(f\"----- Fold {i} -----\")\n",
    "\n",
    "\n",
    "    netG = GSRNet(ks, args).to(device)\n",
    "    optimizerG = optim.Adam(netG.parameters(), lr=args.lr)\n",
    "\n",
    "    netD = Discriminator(input_dim=args.embedding_size, hidden_sizes=[16, 8]).to(device)\n",
    "    optimizerD = optim.Adam(netD.parameters(), lr=args.lr)\n",
    "\n",
    "    subjects_adj, test_adj, subjects_ground_truth, test_ground_truth = X[\n",
    "        train_index], X[test_index], Y[train_index], Y[test_index]\n",
    "    data_fold_list.append((subjects_adj, test_adj, subjects_ground_truth, test_ground_truth))\n",
    "\n",
    "\n",
    "    ##################\n",
    "    # subjects_adj = subjects_adj[:1]\n",
    "    # subjects_ground_truth = subjects_ground_truth[:1]\n",
    "    ##################\n",
    "\n",
    "    # return_model = train_gan(\n",
    "    #     netG, \n",
    "    #     optimizerG, \n",
    "    #     netD,\n",
    "    #     optimizerD,\n",
    "    #     subjects_adj, \n",
    "    #     subjects_ground_truth, \n",
    "    #     args, \n",
    "    #     test_adj=test_adj, \n",
    "    #     test_ground_truth=test_ground_truth\n",
    "    # )\n",
    "\n",
    "    return_model = train(netG, optimizerG, subjects_adj, subjects_ground_truth, args, test_adj, test_ground_truth)\n",
    "    test_mae = test(return_model, test_adj, test_ground_truth, args)\n",
    "    print(f\"Val MAE: {test_mae}\")\n",
    "    best_model_fold_list.append(return_model)\n",
    "\n",
    "    i += 1\n",
    "\n",
    "    # break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MatrixVectorizer import MatrixVectorizer\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "import torch\n",
    "import networkx as nx\n",
    "\n",
    "def evaluate(pred_matrices, gt_matrices):\n",
    "\n",
    "    num_test_samples = gt_matrices.shape[0]\n",
    "\n",
    "    # Initialize lists to store MAEs for each centrality measure\n",
    "    mae_bc = []\n",
    "    mae_ec = []\n",
    "    mae_pc = []\n",
    "\n",
    "    # # Iterate over each test sample\n",
    "    # for i in range(num_test_samples):\n",
    "    #     # Convert adjacency matrices to NetworkX graphs\n",
    "    #     pred_graph = nx.from_numpy_array(pred_matrices[i], edge_attr=\"weight\")\n",
    "    #     gt_graph = nx.from_numpy_array(gt_matrices[i], edge_attr=\"weight\")\n",
    "\n",
    "    #     # Compute centrality measures\n",
    "    #     pred_bc = nx.betweenness_centrality(pred_graph, weight=\"weight\")\n",
    "    #     pred_ec = nx.eigenvector_centrality(pred_graph, weight=\"weight\")\n",
    "    #     pred_pc = nx.pagerank(pred_graph, weight=\"weight\")\n",
    "\n",
    "    #     gt_bc = nx.betweenness_centrality(gt_graph, weight=\"weight\")\n",
    "    #     gt_ec = nx.eigenvector_centrality(gt_graph, weight=\"weight\")\n",
    "    #     gt_pc = nx.pagerank(gt_graph, weight=\"weight\")\n",
    "\n",
    "    #     # Convert centrality dictionaries to lists\n",
    "    #     pred_bc_values = list(pred_bc.values())\n",
    "    #     pred_ec_values = list(pred_ec.values())\n",
    "    #     pred_pc_values = list(pred_pc.values())\n",
    "\n",
    "    #     gt_bc_values = list(gt_bc.values())\n",
    "    #     gt_ec_values = list(gt_ec.values())\n",
    "    #     gt_pc_values = list(gt_pc.values())\n",
    "\n",
    "    #     # Compute MAEs\n",
    "    #     mae_bc.append(mean_absolute_error(pred_bc_values, gt_bc_values))\n",
    "    #     mae_ec.append(mean_absolute_error(pred_ec_values, gt_ec_values))\n",
    "    #     mae_pc.append(mean_absolute_error(pred_pc_values, gt_pc_values))\n",
    "\n",
    "    # # Compute average MAEs\n",
    "    # avg_mae_bc = sum(mae_bc) / len(mae_bc)\n",
    "    # avg_mae_ec = sum(mae_ec) / len(mae_ec)\n",
    "    # avg_mae_pc = sum(mae_pc) / len(mae_pc)\n",
    "\n",
    "    # vectorize and flatten\n",
    "    pred_1d = MatrixVectorizer.vectorize(pred_matrices).flatten()\n",
    "    gt_1d = MatrixVectorizer.vectorize(gt_matrices).flatten()\n",
    "\n",
    "    mae = mean_absolute_error(pred_1d, gt_1d)\n",
    "    pcc = pearsonr(pred_1d, gt_1d)[0]\n",
    "    js_dis = jensenshannon(pred_1d, gt_1d)\n",
    "\n",
    "    print(\"MAE: \", mae)\n",
    "    print(\"PCC: \", pcc)\n",
    "    print(\"Jensen-Shannon Distance: \", js_dis)\n",
    "    # print(\"Average MAE betweenness centrality:\", avg_mae_bc)\n",
    "    # print(\"Average MAE eigenvector centrality:\", avg_mae_ec)\n",
    "    # print(\"Average MAE PageRank centrality:\", avg_mae_pc)\n",
    "    # return mae, pcc, js_dis, avg_mae_bc, avg_mae_ec, avg_mae_pc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  0.1341236565004594\n",
      "PCC:  0.6254681190368102\n",
      "Jensen-Shannon Distance:  0.28631966477106546\n",
      "MAE:  0.14527797451449923\n",
      "PCC:  0.5894614178362924\n",
      "Jensen-Shannon Distance:  0.3004112047910723\n",
      "MAE:  0.1451417916264916\n",
      "PCC:  0.5836879948789552\n",
      "Jensen-Shannon Distance:  0.2958431640728769\n"
     ]
    }
   ],
   "source": [
    "# for i in range(args.splits):\n",
    "#     _, test_adjs, _, gt_matrices = data_fold_list[i]\n",
    "#     model = best_model_fold_list[i]\n",
    "#     model.eval()\n",
    "#     pred_matrices = np.zeros(gt_matrices.shape)\n",
    "#     with torch.no_grad():\n",
    "#         for j, test_adj in enumerate(test_adjs):\n",
    "#             pred_matrices[j] = model(torch.from_numpy(test_adj))[0].cpu()\n",
    "#     evaluate(pred_matrices, gt_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(167, 161)\n",
      "[[ 0 13]\n",
      " [ 1 41]\n",
      " [ 2 14]\n",
      " [ 3 48]\n",
      " [ 4 51]]\n",
      "Train size: 133\n",
      "Val size: 34\n",
      "0.5 0.03 0.1\n",
      "Epoch: 1, Train Loss: 0.311588, Train Error: 0.252429, Test Error: 0.250990\n",
      "Epoch: 2, Train Loss: 0.281891, Train Error: 0.225976, Test Error: 0.201362\n",
      "Epoch: 3, Train Loss: 0.235306, Train Error: 0.183892, Test Error: 0.176163\n",
      "Epoch: 4, Train Loss: 0.224539, Train Error: 0.175089, Test Error: 0.173045\n",
      "Epoch: 5, Train Loss: 0.219150, Train Error: 0.173763, Test Error: 0.172302\n",
      "Epoch: 6, Train Loss: 0.216750, Train Error: 0.173324, Test Error: 0.171891\n",
      "Epoch: 7, Train Loss: 0.214777, Train Error: 0.173050, Test Error: 0.171370\n",
      "Epoch: 8, Train Loss: 0.212875, Train Error: 0.172668, Test Error: 0.171106\n",
      "Epoch: 9, Train Loss: 0.210912, Train Error: 0.172314, Test Error: 0.170619\n",
      "Epoch: 10, Train Loss: 0.208983, Train Error: 0.172040, Test Error: 0.170457\n",
      "Epoch: 11, Train Loss: 0.208138, Train Error: 0.171806, Test Error: 0.170268\n",
      "Epoch: 12, Train Loss: 0.206285, Train Error: 0.171449, Test Error: 0.169974\n",
      "Epoch: 13, Train Loss: 0.205315, Train Error: 0.171193, Test Error: 0.169718\n",
      "Epoch: 14, Train Loss: 0.204990, Train Error: 0.170868, Test Error: 0.169340\n",
      "Epoch: 15, Train Loss: 0.203854, Train Error: 0.170508, Test Error: 0.168918\n",
      "Epoch: 16, Train Loss: 0.202376, Train Error: 0.170215, Test Error: 0.168841\n",
      "Epoch: 17, Train Loss: 0.201788, Train Error: 0.169900, Test Error: 0.168394\n",
      "Epoch: 18, Train Loss: 0.201178, Train Error: 0.169541, Test Error: 0.168086\n",
      "Epoch: 19, Train Loss: 0.200450, Train Error: 0.168973, Test Error: 0.167639\n",
      "Epoch: 20, Train Loss: 0.199131, Train Error: 0.168433, Test Error: 0.166774\n",
      "Epoch: 21, Train Loss: 0.199368, Train Error: 0.167966, Test Error: 0.166091\n",
      "Epoch: 22, Train Loss: 0.197677, Train Error: 0.167202, Test Error: 0.165394\n",
      "Epoch: 23, Train Loss: 0.196863, Train Error: 0.166437, Test Error: 0.164453\n",
      "Epoch: 24, Train Loss: 0.195789, Train Error: 0.165499, Test Error: 0.163974\n",
      "Epoch: 25, Train Loss: 0.194393, Train Error: 0.164560, Test Error: 0.162826\n",
      "Epoch: 26, Train Loss: 0.192854, Train Error: 0.163516, Test Error: 0.161704\n",
      "Epoch: 27, Train Loss: 0.191806, Train Error: 0.162414, Test Error: 0.160809\n",
      "Epoch: 28, Train Loss: 0.190541, Train Error: 0.161331, Test Error: 0.159749\n",
      "Epoch: 29, Train Loss: 0.189555, Train Error: 0.160179, Test Error: 0.158393\n",
      "Epoch: 30, Train Loss: 0.187416, Train Error: 0.158812, Test Error: 0.157256\n",
      "Epoch: 31, Train Loss: 0.186202, Train Error: 0.157595, Test Error: 0.156263\n",
      "Epoch: 32, Train Loss: 0.185889, Train Error: 0.156819, Test Error: 0.155335\n",
      "Epoch: 33, Train Loss: 0.184356, Train Error: 0.155619, Test Error: 0.154437\n",
      "Epoch: 34, Train Loss: 0.183376, Train Error: 0.154584, Test Error: 0.153511\n",
      "Epoch: 35, Train Loss: 0.182883, Train Error: 0.153740, Test Error: 0.152648\n",
      "Epoch: 36, Train Loss: 0.181071, Train Error: 0.152960, Test Error: 0.151467\n",
      "Epoch: 37, Train Loss: 0.180212, Train Error: 0.151737, Test Error: 0.151145\n",
      "Epoch: 38, Train Loss: 0.179371, Train Error: 0.151152, Test Error: 0.150400\n",
      "Epoch: 39, Train Loss: 0.178028, Train Error: 0.149999, Test Error: 0.149364\n",
      "Epoch: 40, Train Loss: 0.177863, Train Error: 0.149546, Test Error: 0.148841\n",
      "Epoch: 41, Train Loss: 0.176711, Train Error: 0.148829, Test Error: 0.148530\n",
      "Epoch: 42, Train Loss: 0.175889, Train Error: 0.148125, Test Error: 0.147981\n",
      "Epoch: 43, Train Loss: 0.175392, Train Error: 0.147573, Test Error: 0.147428\n",
      "Epoch: 44, Train Loss: 0.175719, Train Error: 0.147192, Test Error: 0.147111\n",
      "Epoch: 45, Train Loss: 0.173923, Train Error: 0.146532, Test Error: 0.146598\n",
      "Epoch: 46, Train Loss: 0.173878, Train Error: 0.145988, Test Error: 0.146605\n",
      "Epoch: 47, Train Loss: 0.173577, Train Error: 0.145572, Test Error: 0.145359\n",
      "Epoch: 48, Train Loss: 0.172907, Train Error: 0.144987, Test Error: 0.145179\n",
      "Epoch: 49, Train Loss: 0.171904, Train Error: 0.144488, Test Error: 0.144528\n",
      "Epoch: 50, Train Loss: 0.171678, Train Error: 0.143987, Test Error: 0.144336\n",
      "Epoch: 51, Train Loss: 0.170470, Train Error: 0.143346, Test Error: 0.143900\n",
      "Epoch: 52, Train Loss: 0.170923, Train Error: 0.143414, Test Error: 0.143636\n",
      "Epoch: 53, Train Loss: 0.169522, Train Error: 0.142563, Test Error: 0.143582\n",
      "Epoch: 54, Train Loss: 0.170066, Train Error: 0.142493, Test Error: 0.142986\n",
      "Epoch: 55, Train Loss: 0.169023, Train Error: 0.141853, Test Error: 0.143370\n",
      "Epoch: 56, Train Loss: 0.169102, Train Error: 0.141851, Test Error: 0.142856\n",
      "Epoch: 57, Train Loss: 0.168513, Train Error: 0.141182, Test Error: 0.142365\n",
      "Epoch: 58, Train Loss: 0.169078, Train Error: 0.141329, Test Error: 0.143357\n",
      "Epoch: 59, Train Loss: 0.167372, Train Error: 0.140446, Test Error: 0.142176\n",
      "Epoch: 60, Train Loss: 0.167939, Train Error: 0.140650, Test Error: 0.142188\n",
      "Epoch: 61, Train Loss: 0.167693, Train Error: 0.140205, Test Error: 0.142034\n",
      "Epoch: 62, Train Loss: 0.167624, Train Error: 0.140070, Test Error: 0.141705\n",
      "Epoch: 63, Train Loss: 0.166687, Train Error: 0.139647, Test Error: 0.141300\n",
      "Epoch: 64, Train Loss: 0.166535, Train Error: 0.139416, Test Error: 0.141308\n",
      "Epoch: 65, Train Loss: 0.166393, Train Error: 0.139322, Test Error: 0.141348\n",
      "Epoch: 66, Train Loss: 0.166283, Train Error: 0.138990, Test Error: 0.140811\n",
      "Epoch: 67, Train Loss: 0.165803, Train Error: 0.138552, Test Error: 0.140281\n",
      "Epoch: 68, Train Loss: 0.166419, Train Error: 0.138649, Test Error: 0.140481\n",
      "Epoch: 69, Train Loss: 0.166000, Train Error: 0.138700, Test Error: 0.140575\n",
      "Epoch: 70, Train Loss: 0.165396, Train Error: 0.138208, Test Error: 0.140248\n",
      "Epoch: 71, Train Loss: 0.164257, Train Error: 0.137736, Test Error: 0.140530\n",
      "Epoch: 72, Train Loss: 0.164722, Train Error: 0.137892, Test Error: 0.140139\n",
      "Epoch: 73, Train Loss: 0.164194, Train Error: 0.137379, Test Error: 0.140197\n",
      "Epoch: 74, Train Loss: 0.164728, Train Error: 0.137590, Test Error: 0.140119\n",
      "Epoch: 75, Train Loss: 0.164189, Train Error: 0.137356, Test Error: 0.140184\n",
      "Epoch: 76, Train Loss: 0.164700, Train Error: 0.137399, Test Error: 0.139889\n",
      "Epoch: 77, Train Loss: 0.163350, Train Error: 0.136872, Test Error: 0.139849\n",
      "Epoch: 78, Train Loss: 0.163441, Train Error: 0.136673, Test Error: 0.139348\n",
      "Epoch: 79, Train Loss: 0.163643, Train Error: 0.136766, Test Error: 0.140125\n",
      "Epoch: 80, Train Loss: 0.164672, Train Error: 0.137127, Test Error: 0.139482\n",
      "Epoch: 81, Train Loss: 0.163809, Train Error: 0.136637, Test Error: 0.139515\n",
      "Epoch: 82, Train Loss: 0.163129, Train Error: 0.136547, Test Error: 0.139258\n",
      "Epoch: 83, Train Loss: 0.163247, Train Error: 0.136112, Test Error: 0.138650\n",
      "Epoch: 84, Train Loss: 0.163331, Train Error: 0.136198, Test Error: 0.138914\n",
      "Epoch: 85, Train Loss: 0.162724, Train Error: 0.135919, Test Error: 0.138849\n",
      "Epoch: 86, Train Loss: 0.162561, Train Error: 0.135818, Test Error: 0.138908\n",
      "Epoch: 87, Train Loss: 0.162496, Train Error: 0.135676, Test Error: 0.138615\n",
      "Epoch: 88, Train Loss: 0.162711, Train Error: 0.135577, Test Error: 0.138154\n",
      "Epoch: 89, Train Loss: 0.163070, Train Error: 0.135843, Test Error: 0.138982\n",
      "Epoch: 90, Train Loss: 0.162304, Train Error: 0.135446, Test Error: 0.138619\n",
      "Epoch: 91, Train Loss: 0.161675, Train Error: 0.134890, Test Error: 0.138299\n",
      "Epoch: 92, Train Loss: 0.162050, Train Error: 0.135024, Test Error: 0.138419\n",
      "Epoch: 93, Train Loss: 0.161665, Train Error: 0.134904, Test Error: 0.137974\n",
      "Epoch: 94, Train Loss: 0.161459, Train Error: 0.134865, Test Error: 0.138078\n",
      "Epoch: 95, Train Loss: 0.160776, Train Error: 0.134308, Test Error: 0.138172\n",
      "Epoch: 96, Train Loss: 0.161565, Train Error: 0.134709, Test Error: 0.137752\n",
      "Epoch: 97, Train Loss: 0.162222, Train Error: 0.135015, Test Error: 0.138083\n",
      "Epoch: 98, Train Loss: 0.161268, Train Error: 0.134634, Test Error: 0.138132\n",
      "Epoch: 99, Train Loss: 0.161539, Train Error: 0.134646, Test Error: 0.138820\n",
      "Epoch: 100, Train Loss: 0.161471, Train Error: 0.134538, Test Error: 0.138168\n",
      "Epoch: 101, Train Loss: 0.161217, Train Error: 0.134329, Test Error: 0.138284\n",
      "Epoch: 102, Train Loss: 0.160941, Train Error: 0.134143, Test Error: 0.137765\n",
      "Epoch: 103, Train Loss: 0.161672, Train Error: 0.134515, Test Error: 0.137698\n",
      "Epoch: 104, Train Loss: 0.161027, Train Error: 0.134049, Test Error: 0.138273\n",
      "Epoch: 105, Train Loss: 0.160722, Train Error: 0.133924, Test Error: 0.137756\n",
      "Epoch: 106, Train Loss: 0.160314, Train Error: 0.133575, Test Error: 0.137123\n",
      "Epoch: 107, Train Loss: 0.160382, Train Error: 0.133608, Test Error: 0.137418\n",
      "Epoch: 108, Train Loss: 0.160646, Train Error: 0.133759, Test Error: 0.137594\n",
      "Epoch: 109, Train Loss: 0.160443, Train Error: 0.133676, Test Error: 0.137379\n",
      "Epoch: 110, Train Loss: 0.160503, Train Error: 0.133483, Test Error: 0.136951\n",
      "Epoch: 111, Train Loss: 0.159139, Train Error: 0.132738, Test Error: 0.137913\n",
      "Epoch: 112, Train Loss: 0.160058, Train Error: 0.133308, Test Error: 0.137488\n",
      "Epoch: 113, Train Loss: 0.159946, Train Error: 0.133227, Test Error: 0.137270\n",
      "Epoch: 114, Train Loss: 0.159477, Train Error: 0.132899, Test Error: 0.137021\n",
      "Epoch: 115, Train Loss: 0.158968, Train Error: 0.132364, Test Error: 0.137377\n",
      "Epoch: 116, Train Loss: 0.159344, Train Error: 0.132605, Test Error: 0.137312\n",
      "Epoch: 117, Train Loss: 0.159127, Train Error: 0.132530, Test Error: 0.136918\n",
      "Epoch: 118, Train Loss: 0.158859, Train Error: 0.132437, Test Error: 0.136440\n",
      "Epoch: 119, Train Loss: 0.159685, Train Error: 0.132762, Test Error: 0.136657\n",
      "Epoch: 120, Train Loss: 0.158824, Train Error: 0.132240, Test Error: 0.137122\n",
      "Epoch: 121, Train Loss: 0.159589, Train Error: 0.132715, Test Error: 0.137063\n",
      "Epoch: 122, Train Loss: 0.158868, Train Error: 0.132227, Test Error: 0.136863\n",
      "Epoch: 123, Train Loss: 0.159098, Train Error: 0.132410, Test Error: 0.136656\n",
      "Epoch: 124, Train Loss: 0.159752, Train Error: 0.132855, Test Error: 0.137235\n",
      "Epoch: 125, Train Loss: 0.159331, Train Error: 0.132471, Test Error: 0.137556\n",
      "Epoch: 126, Train Loss: 0.159676, Train Error: 0.132661, Test Error: 0.136654\n",
      "Epoch: 127, Train Loss: 0.157844, Train Error: 0.131672, Test Error: 0.136277\n",
      "Epoch: 128, Train Loss: 0.158861, Train Error: 0.132017, Test Error: 0.136571\n",
      "Epoch: 129, Train Loss: 0.158754, Train Error: 0.131941, Test Error: 0.136878\n",
      "Epoch: 130, Train Loss: 0.158936, Train Error: 0.132198, Test Error: 0.136605\n",
      "Epoch: 131, Train Loss: 0.158695, Train Error: 0.132205, Test Error: 0.136524\n",
      "Epoch: 132, Train Loss: 0.158742, Train Error: 0.132183, Test Error: 0.136452\n",
      "Epoch: 133, Train Loss: 0.158286, Train Error: 0.131746, Test Error: 0.136566\n",
      "Epoch: 134, Train Loss: 0.158894, Train Error: 0.132054, Test Error: 0.136275\n",
      "Epoch: 135, Train Loss: 0.158442, Train Error: 0.131546, Test Error: 0.135964\n",
      "Epoch: 136, Train Loss: 0.158618, Train Error: 0.131951, Test Error: 0.136890\n",
      "Epoch: 137, Train Loss: 0.157468, Train Error: 0.131406, Test Error: 0.136124\n",
      "Epoch: 138, Train Loss: 0.158219, Train Error: 0.131517, Test Error: 0.136827\n",
      "Epoch: 139, Train Loss: 0.157918, Train Error: 0.131638, Test Error: 0.135661\n",
      "Epoch: 140, Train Loss: 0.158091, Train Error: 0.131468, Test Error: 0.136461\n",
      "Epoch: 141, Train Loss: 0.157410, Train Error: 0.130991, Test Error: 0.135998\n",
      "Epoch: 142, Train Loss: 0.157723, Train Error: 0.131167, Test Error: 0.136384\n",
      "Epoch: 143, Train Loss: 0.157662, Train Error: 0.131148, Test Error: 0.135722\n",
      "Epoch: 144, Train Loss: 0.157534, Train Error: 0.130857, Test Error: 0.136524\n",
      "Epoch: 145, Train Loss: 0.157976, Train Error: 0.131302, Test Error: 0.136368\n",
      "Epoch: 146, Train Loss: 0.157829, Train Error: 0.131047, Test Error: 0.135696\n",
      "Epoch: 147, Train Loss: 0.158252, Train Error: 0.131565, Test Error: 0.136267\n",
      "Epoch: 148, Train Loss: 0.158165, Train Error: 0.131496, Test Error: 0.136829\n",
      "Epoch: 149, Train Loss: 0.157626, Train Error: 0.131147, Test Error: 0.135800\n",
      "Epoch: 150, Train Loss: 0.157841, Train Error: 0.131184, Test Error: 0.136339\n",
      "Epoch: 151, Train Loss: 0.157035, Train Error: 0.130706, Test Error: 0.135855\n",
      "Epoch: 152, Train Loss: 0.157106, Train Error: 0.130718, Test Error: 0.135800\n",
      "Epoch: 153, Train Loss: 0.156689, Train Error: 0.130526, Test Error: 0.135266\n",
      "Epoch: 154, Train Loss: 0.156951, Train Error: 0.130511, Test Error: 0.135978\n",
      "Epoch: 155, Train Loss: 0.157190, Train Error: 0.130592, Test Error: 0.136059\n",
      "Epoch: 156, Train Loss: 0.157526, Train Error: 0.131066, Test Error: 0.137776\n",
      "Epoch: 157, Train Loss: 0.157470, Train Error: 0.130750, Test Error: 0.135776\n",
      "Epoch: 158, Train Loss: 0.157225, Train Error: 0.130580, Test Error: 0.139013\n",
      "Epoch: 159, Train Loss: 0.157457, Train Error: 0.130822, Test Error: 0.135825\n",
      "Epoch: 160, Train Loss: 0.156836, Train Error: 0.130519, Test Error: 0.136432\n",
      "Epoch: 161, Train Loss: 0.156492, Train Error: 0.130065, Test Error: 0.135791\n",
      "Epoch: 162, Train Loss: 0.157292, Train Error: 0.130697, Test Error: 0.136727\n",
      "Epoch: 163, Train Loss: 0.157442, Train Error: 0.130572, Test Error: 0.135891\n",
      "Epoch: 164, Train Loss: 0.156395, Train Error: 0.130010, Test Error: 0.135727\n",
      "Epoch: 165, Train Loss: 0.157360, Train Error: 0.130667, Test Error: 0.136927\n",
      "Epoch: 166, Train Loss: 0.156195, Train Error: 0.129907, Test Error: 0.136601\n",
      "Epoch: 167, Train Loss: 0.156062, Train Error: 0.129918, Test Error: 0.136408\n",
      "Epoch: 168, Train Loss: 0.157011, Train Error: 0.130542, Test Error: 0.136141\n",
      "Epoch: 169, Train Loss: 0.156570, Train Error: 0.130195, Test Error: 0.137042\n",
      "Epoch: 170, Train Loss: 0.156153, Train Error: 0.129769, Test Error: 0.136682\n",
      "Epoch: 171, Train Loss: 0.156820, Train Error: 0.130200, Test Error: 0.136738\n",
      "Epoch: 172, Train Loss: 0.157567, Train Error: 0.130809, Test Error: 0.136796\n",
      "Epoch: 173, Train Loss: 0.157309, Train Error: 0.130571, Test Error: 0.136409\n",
      "Val Error: 0.135266\n"
     ]
    }
   ],
   "source": [
    "A_HR_train = pd.read_csv(\"../data/hr_train.csv\")\n",
    "\n",
    "pca = PCA(n_components=0.99, whiten=False)\n",
    "A_HR_train_pca = pca.fit_transform(A_HR_train)\n",
    "print(A_HR_train_pca.shape)\n",
    "\n",
    "gm = GaussianMixture(n_components=5, random_state=random_seed)\n",
    "A_HR_train_label = gm.fit_predict(A_HR_train_pca)\n",
    "unique, counts = np.unique(A_HR_train_label, return_counts=True)\n",
    "print(np.asarray((unique, counts)).T)\n",
    "\n",
    "X = np.load('A_LR_train_matrix.npy')\n",
    "y = np.load('A_HR_train_matrix.npy')\n",
    "\n",
    "n_sample = X.shape[0]\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X.reshape(n_sample, -1), \n",
    "    y.reshape(n_sample, -1), \n",
    "    test_size=0.20, \n",
    "    random_state=random_seed,\n",
    "    stratify=A_HR_train_label\n",
    ")\n",
    "\n",
    "X_train = X_train.reshape(-1, LR_size, LR_size)\n",
    "X_val = X_val.reshape(-1, LR_size, LR_size)\n",
    "y_train = y_train.reshape(-1, HR_size, HR_size)\n",
    "y_val = y_val.reshape(-1, HR_size, HR_size)\n",
    "\n",
    "print(\"Train size:\", len(X_train))\n",
    "print(\"Val size:\", len(X_val))\n",
    "\n",
    "netG = GSRNet(ks, args).to(device)\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=args.lr)\n",
    "args.early_stop_patient = 20\n",
    "# print(args.early_stop_patient)\n",
    "final_model = train(netG, optimizerG, X_train, y_train, args, X_val, y_val)\n",
    "\n",
    "# final_model = best_model_fold_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = best_model_fold_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(epochs=200, lr=5e-05, splits=3, lmbda=16, lr_dim=160, hr_dim=268, hidden_dim=268, padding=26, embedding_size=32, early_stop_patient=20)\n"
     ]
    }
   ],
   "source": [
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "MAE:  0.13350148288930702\n",
      "PCC:  0.6067668252251874\n",
      "Jensen-Shannon Distance:  0.29584277391954517\n",
      "Val\n",
      "MAE:  0.12906604591323537\n",
      "PCC:  0.6478599592850003\n",
      "Jensen-Shannon Distance:  0.2757063188368834\n"
     ]
    }
   ],
   "source": [
    "final_model.eval()\n",
    "pred_train_matrices = np.zeros(y_train.shape)\n",
    "pred_val_matrices = np.zeros(y_val.shape)\n",
    "with torch.no_grad():\n",
    "    for j, test_adj in enumerate(X_train):\n",
    "        pred_train_matrices[j] = final_model(torch.from_numpy(test_adj))[0].cpu()\n",
    "\n",
    "    print(\"Train\")\n",
    "    evaluate(pred_train_matrices, y_train)\n",
    "\n",
    "    for j, test_adj in enumerate(X_val):\n",
    "        pred_val_matrices[j] = final_model(torch.from_numpy(test_adj))[0].cpu()\n",
    "\n",
    "    print(\"Val\")\n",
    "    evaluate(pred_val_matrices, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_pred_list = []\n",
    "final_model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(A_LR_test_matrix.shape[0]):\n",
    "        output_pred = final_model(torch.Tensor(A_LR_test_matrix[i]))[0].cpu()\n",
    "        output_pred = MatrixVectorizer.vectorize(output_pred).tolist()\n",
    "        output_pred_list.append(output_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_pred_stack = np.stack(output_pred_list, axis=0)\n",
    "output_pred_1d = output_pred_stack.flatten()\n",
    "assert output_pred_1d.shape == (4007136, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.576910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.592497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.641155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.614392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.565211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4007131</th>\n",
       "      <td>4007132</td>\n",
       "      <td>0.061797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4007132</th>\n",
       "      <td>4007133</td>\n",
       "      <td>0.011763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4007133</th>\n",
       "      <td>4007134</td>\n",
       "      <td>0.262607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4007134</th>\n",
       "      <td>4007135</td>\n",
       "      <td>0.129944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4007135</th>\n",
       "      <td>4007136</td>\n",
       "      <td>0.178241</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4007136 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID  Predicted\n",
       "0              1   0.576910\n",
       "1              2   0.592497\n",
       "2              3   0.641155\n",
       "3              4   0.614392\n",
       "4              5   0.565211\n",
       "...          ...        ...\n",
       "4007131  4007132   0.061797\n",
       "4007132  4007133   0.011763\n",
       "4007133  4007134   0.262607\n",
       "4007134  4007135   0.129944\n",
       "4007135  4007136   0.178241\n",
       "\n",
       "[4007136 rows x 2 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"ID\": [i+1 for i in range(len(output_pred_1d))],\n",
    "    \"Predicted\": output_pred_1d.tolist()\n",
    "})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"gsr_gat_residual_node_edge_drop_50perecentdrop_fold0.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
